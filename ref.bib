@inproceedings{ MitraEtAl2011,
author = {V. Mitra and H. Nam and C. Y. Espy-Wilson and E. Saltzman and L. Goldstein},
title = {Gesture-based dynamic Bayesian network for noise robust speech recognition}, 
booktitle = icassp,
year = {2011}, 
pages = {5172–-5175},
}

@article{ KingEtAl2007,
author = {S. King and J. Frankel and K. Livescu and E. McDermott and K. Richmond and M. Wester}, 
title = {Speech production knowledge in automatic speech recognition},
journal = jasa,
volume = {121},
number = {2},
pages = {723-–742}, 
year = {2007},
}

@article{ CooperEtAl1952,
author = {Cooper and Delattre and Liberman and Borst and Gerstman},
year = {1952}, 
title = {Some experiments on the perception of synthetic speech sounds},
journal = jasa,
}

@article{ DelattreEtAl1955,
author = {Delattre and Liberman and Cooper},
year = {1955},
title = {Acoustic loci and translational cues for consonants},
journal = jasa,
}

@article{ BellEtAl1961,
author = {Bell and Fujisaki and Heinz and Stevens and House},
year = {1961},
title = {Reduction of speech spectra by analysis-by-synthesis technique},
journal = jasa,
}

french and steinberg (1947), "Factors governing the intelligibility of speech sounds" JASA

fletcher and galt (1950), "Perception of speech and its relation to telephony" JASA

Doesn't improve intelligibility: Hu and loizou 2007 "A comparitive inteligibility study of single-microphone noise reduction algorithms" JASA

maybe altering ibm: li, loizou (2008) "factors influencing intelligibility of ideal binary masked speech: implications for noise reduction" JASA

@inproceedings{ ZilanyAndBruce2007,
    abstract = {A fall-off in speech intelligibility at higher-than-normal presentation levels has been observed for listeners with and without hearing loss (Studebaker et al., 1999; Dubno et al., 2005; Molis and Summers, 2003; Shanks et al., 2002). Speech intelligibility predictors based on the acoustic signal properties, such as the articulation index and speech transmission index, cannot directly account for the effects of presentation level and hearing impairment. Recently, Elhilali et al. (2003) introduced the spectro-temporal modulation index ({STMI}), a speech intelligibility predictor based on a model of how the auditory cortex analyzes the joint spectro-temporal modulations present in speech. However, the auditory-periphery model used by Elhilali et al. is very simple and cannot describe many of the nonlinear, level-dependent properties of cochlear processing, nor the effect of hair cell impairment on this processing. In this study, we quantify the effects of speech presentation level and cochlear impairment on speech intelligibility using the {STMI} with a more physiologically-accurate model of the normal and impaired auditory periphery developed by Zilany and Bruce (2006). This model can accurately represent the auditory-nerve responses to a wide variety of stimuli across a range of characteristic frequencies and intensities spanning the dynamic range of hearing. In addition, outer and inner hair cell impairment can be incorporated. Compared to experimental word recognition scores, this model-based {STMI} can qualitatively predict the effect of presentation levels on speech intelligibility for both normal and impaired listeners in a wide variety of conditions},
    author = {Zilany, M. S. A. and Bruce, I. C.},
    booktitle = icne,
    citeulike-article-id = {2291403},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/CNE.2007.369714},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4227319},
    doi = {10.1109/CNE.2007.369714},
    institution = {McMaster Univ. Dept. of Electr. \& Comput. Eng.},
    isbn = {1-4244-0792-3},
    keywords = {intelligibility, intelligibilitymodel},
    month = may,
    pages = {481--485},
    posted-at = {2012-12-07 19:47:02},
    priority = {2},
    publisher = {IEEE},
    title = {Predictions of Speech Intelligibility with a Model of the Normal and Impaired Auditory-periphery},
    url = {http://dx.doi.org/10.1109/CNE.2007.369714},
    year = {2007}
}


@article{ElhilaliEtAl2003,
    abstract = {We present a biologically motivated method for assessing the intelligibility of speech recorded or transmitted under various types of distortions. The method employs an auditory model to analyze the effects of noise, reverberations, and other distortions on the joint spectro-temporal modulations present in speech, and on the ability of a channel to transmit these modulations. The effects are summarized by a spectro-temporal modulation index ({STMI}). The index is validated by comparing its predictions to those of the classical {STI} and to error rates reported by human subjects listening to speech contaminated with combined noise and reverberation. We further demonstrate that the {STMI} can handle difficult and nonlinear distortions such as phase-jitter and shifts, to which the {STI} is not sensitive. Nous pr\'{e}sentons une approche inspir\'{e}e par la biologie du syst\`{e}me auditif humain, qui pr\'{e}dit l'intelligibilit\'{e} d'enregistrements directes de paroles ou apr\`{e}s transmissions sous diff\'{e}rentes conditions de bruit propre, r\'{e}verb\'{e}rations, et autres d\'{e}formations. La m\'{e}thode est bas\'{e}e sur un mod\`{e}le auditif qui analyse les effets du bruit sur les modulations conjointes de temps et fr\'{e}quences, pr\'{e}sentes dans la parole. Par ailleurs, cette m\'{e}thode analyse la capacit\'{e} d'un canal \`{a} transmettre fid\`{e}lement ces modulations. Les effets sur les modulations sont convertis en un indice des modulations spectro-temporelles, appel\'{e} {STMI}. La validit\'{e} de cet indice est \'{e}tablie en comparant ses pr\'{e}dictions \`{a} celles du {STI} classique; ainsi qu'aux r\'{e}sultats exp\'{e}rimentaux des taux d'erreurs de sujets humains qui \'{e}coutent de la parole contamin\'{e}e par des combinaisons de bruit propre et de r\'{e}verb\'{e}ration. Nous d\'{e}montrons \'{e}galement que le {STMI} est capable de manipuler des conditions encore plus s\'{e}v\`{e}res, comme les d\'{e}formations non-lin\'{e}aires, tels les d\'{e}calages et autres instabilit\'{e}s des phases; conditions auxquelles le {STI} classique s'av\`{e}re \^{e}tre insensible.},
    author = {Elhilali, Mounya and Chi, Taishih and Shamma, Shihab A.},
    citeulike-article-id = {7752898},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0167-6393(02)00134-6},
    doi = {10.1016/S0167-6393(02)00134-6},
    issn = {01676393},
    journal = spcomm,
    keywords = {intelligibility, intelligibilitymodel, stmi},
    month = oct,
    number = {2-3},
    pages = {331--348},
    posted-at = {2012-12-07 19:48:29},
    priority = {2},
    title = {A spectro-temporal modulation index ({STMI}) for assessment of speech intelligibility},
    url = {http://dx.doi.org/10.1016/S0167-6393(02)00134-6},
    volume = {41},
    year = {2003}
}

@article{MaEtAl2009,
author = {Ma, J. and Hu, Y. and Loizou, P.},
year = 2009,
title = {Objective measures for predicting speech intelligibility in noisy conditions based on new band importance functions},
journal = jasa,
month = may,
volume = {125},
number = {5},
pages = {3387--3405},
}

@article{DauEtAl1996,
author = {T. Dau and D. P\"uschel and A. Kohlrausch},
year = {1996},
title = {A quantitative model of the `effective' signal processing in the auditory system. {I}. Model structure},
journal = jasa,
volume = {99}, 
number = {6}, 
pages = {3615--3622},
}

@inproceedings{ BoldtAndEllis2009,
author = {Jesper Boldt and Daniel Ellis},
year = {2009},
title = {A simple correlation-based model of interlligibility for nonlinear speech enhancement and separation},
booktitle = {EUSIPCO},
abstract = {Applying a binary mask to a pure noise signal can result in speech that is highly intelligible, despite the absence of any of the target speech signal. Therefore, to estimate the intelligibility benefit of highly nonlinear speech enhancement techniques, we contend that SNR is not useful; instead we propose a measure based on the similarity between the time-varying spectral envelopes of target speech and system output, as measured by correlation. As with previous correlation-based intelligibility measures, our system can broadly match subjective intelligibility for a range of enhanced signals. Our system, however, is notably simpler and we explain the practical motivation behind each stage. This measure, freely available as a small Matlab implementation, can provide a more meaningful evaluation measure for nonlinear speech enhancement systems, as well as providing a transparent objective function for the optimization of such systems.},
}

@article{ ChristiansenEtAl2010,
author = {C. Christiansen and M. Pedersen and T. Dau},
year = {2010},
title = {Prediction of speech intelligibility based on an auditory preprocessing model},
journal = spcomm,
volume = {52}, 
number = {7}, 
month = jul,
pages = {678-–692},
}

@inproceedings{TaalEtAl2009,
author = {Cees H. Taal and Richard C. Hendriks and Richard Heusdens and Jesper Jensen and Ulrich Kjems}, 
year = 2009,
title = {An evaluation of objective quality measures for speech intelligibility prediction},
booktitle = {Interspeech},
pages={1947--1950},
}

@article{TaalEtAl2011,
    abstract = {In the development process of noise-reduction algorithms, an objective machine-driven intelligibility measure which shows high correlation with speech intelligibility is of great interest. Besides reducing time and costs compared to real listening experiments, an objective intelligibility measure could also help provide answers on how to improve the intelligibility of noisy unprocessed speech. In this paper, a short-time objective intelligibility measure ({STOI}) is presented, which shows high correlation with the intelligibility of noisy and time-frequency weighted noisy speech (e.g., resulting from noise reduction) of three different listening experiments. In general, {STOI} showed better correlation with speech intelligibility compared to five other reference objective intelligibility models. In contrast to other conventional intelligibility models which tend to rely on global statistics across entire sentences, {STOI} is based on shorter time segments (386 ms). Experiments indeed show that it is beneficial to take segment lengths of this order into account. In addition, a free Matlab implementation is provided.},
    author = {Taal, C. H. and Hendriks, R. C. and Heusdens, R. and Jensen, J.},
    citeulike-article-id = {11846644},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TASL.2011.2114881},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5713237},
    doi = {10.1109/TASL.2011.2114881},
    institution = {Delft Univ. of Technol. Signal Inf. \& Process. Lab.},
    issn = {1558-7916},
    journal = taslp,
    keywords = {intelligibility, model, objectivemeasure},
    number = {7},
    pages = {2125--2136},
    posted-at = {2012-12-16 16:25:38},
    priority = {0},
    publisher = {IEEE},
    title = {An Algorithm for Intelligibility Prediction of Time-Frequency Weighted Noisy Speech},
    url = {http://dx.doi.org/10.1109/TASL.2011.2114881},
    volume = {19},
    year = {2011}
}


@article{GosselinAndSchyns2001,
    abstract = {Everyday, people flexibly perform different categorizations of common faces, objects and scenes. Intuition and scattered evidence suggest that these categorizations require the use of different visual information from the input. However, there is no unifying method, based on the categorization performance of subjects, that can isolate the information used. To this end, we developed Bubbles, a general technique that can assign the credit of human categorization performance to specific visual information. To illustrate the technique, we applied Bubbles on three categorization tasks (gender, expressive or not and identity) on the same set of faces, with human and ideal observers to compare the features they used.},
    author = {Gosselin, Fr\'{e}d\'{e}ric and Schyns, Philippe G.},
    citeulike-article-id = {8310593},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0042-6989(01)00097-9},
    doi = {10.1016/S0042-6989(01)00097-9},
    issn = {00426989},
    journal = {Vision Research},
    keywords = {psych, vision},
    month = aug,
    number = {17},
    pages = {2261--2271},
    posted-at = {2010-11-26 17:21:32},
    priority = {2},
    title = {Bubbles: a technique to reveal the use of information in recognition tasks},
    url = {http://dx.doi.org/10.1016/S0042-6989(01)00097-9},
    volume = {41},
    year = {2001}
}

@article{LaconteEtAl2005,
    abstract = {This paper treats support vector machine ({SVM}) classification applied to block design {fMRI}, extending our previous work with linear discriminant analysis [{LaConte}, S., Anderson, J., Muley, S., Ashe, J., Frutiger, S., Rehm, K., Hansen, {L.K}., Yacoub, E., Hu, X., Rottenberg, D., Strother S., 2003a. The evaluation of preprocessing choices in single-subject {BOLD} {fMRI} using {NPAIRS} performance metrics. {NeuroImage} 18, 10–27; Strother, {S.C}., Anderson, J., Hansen, {L.K}., Kjems, U., Kustra, R., Siditis, J., Frutiger, S., Muley, S., {LaConte}, S., Rottenberg, D. 2002. The quantitative evaluation of functional neuroimaging experiments: the {NPAIRS} data analysis framework. {NeuroImage} 15, 747–771]. We compare {SVM} to canonical variates analysis ({CVA}) by examining the relative sensitivity of each method to ten combinations of preprocessing choices consisting of spatial smoothing, temporal detrending, and motion correction. Important to the discussion are the issues of classification performance, model interpretation, and validation in the context of {fMRI}. As the {SVM} has many unique properties, we examine the interpretation of support vector models with respect to neuroimaging data. We propose four methods for extracting activation maps from {SVM} models, and we examine one of these in detail. For both {CVA} and {SVM}, we have classified individual time samples of whole brain data, with {TRs} of roughly 4 s, thirty slices, and nearly 30,000 brain voxels, with no averaging of scans or prior feature selection.},
    author = {Laconte, S. and Strother, S. and Cherkassky, V. and Anderson, J. and Hu, X.},
    citeulike-article-id = {1129129},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.neuroimage.2005.01.048},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/15907293},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=15907293},
    citeulike-linkout-3 = {http://www.sciencedirect.com/science/article/B6WNP-4FSNXRC-2/2/04a2d9ed95128616d27db07e2de22ceb},
    doi = {10.1016/j.neuroimage.2005.01.048},
    issn = {10538119},
    journal = {NeuroImage},
    keywords = {classification, fmri, svm},
    month = jun,
    number = {2},
    pages = {317--329},
    pmid = {15907293},
    posted-at = {2012-12-06 19:02:34},
    priority = {2},
    title = {Support vector machines for temporal classification of block design {fMRI} data},
    url = {http://dx.doi.org/10.1016/j.neuroimage.2005.01.048},
    volume = {26},
    year = {2005}
}

@article{ ApouxAndHealy2012,
author = {Apoux, F. and Healy, E.W.},
year = 2012,
title = {Use of a compound approach to derive auditory-filter-wide frequency-importance functions for vowels and consonants},
journal = jasa,
volume = {132},
pages = {1078--1087},
}

@article{ Cooke2006,
author = {M. P. Cooke},
year = {2006},
title = {A glimpsing model of speech perception in noise},
journal = jasa,
volume = {119},
pages = {1562--1573},
}

@article{SteenekenAndHoutgast1980,
    abstract = {A physical method for measuring the quality of speech‐transmission channels has been developed. Essentially, the method represents an extension of the Articulation Index ({AI}) concept, which was developed mainly to account for distortions in the frequency domain (noise, bandpass‐limiting). The underlying concept of the present approach, based on the Modulation Transfer Function ({MTF}) of a transmission channel, has been adapted to account for nonlinear distortions (peak clipping) as well as for distortions in the time domain (reverberation, echoes, {AGC}). The resulting index, the {Speech‐Transmission} Index ({STI}), has been correlated with subjective intelligibility scores obtained on 167 different transmission channels with a wide variety of disturbances. The relative predictive power of the {STI}, expressed in {PB}‐word score, appeared to be 5\%. This accuracy is comparable with results obtained from subjective measurements when about four talkers and four listeners are used. Expressed in terms of signal‐to‐noise ratio, the accuracy is about 1 {dB}. Pilot studies have been carried out to evaluate the use of the {STI} for testing digital speech‐transmission channels.},
    author = {Steeneken, H. J. M. and Houtgast, T.},
    citeulike-article-id = {11836808},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.384464},
    day = {01},
    doi = {10.1121/1.384464},
    keywords = {channel, intelligibility, modulation, reverb},
    month = jan,
    number = {1},
    pages = {318--326},
    posted-at = {2012-12-10 00:59:32},
    priority = {2},
    journal = jasa,
    title = {A physical method for measuring speech‐transmission quality},
    url = {http://dx.doi.org/10.1121/1.384464},
    volume = {67},
    year = {1980}
}

@article{GoldsworthyAndGreenberg2004,
    abstract = {The Speech Transmission Index ({STI}) is a physical metric that is well correlated with the intelligibility of speech degraded by additive noise and reverberation. The traditional {STI} uses modulated noise as a probe signal and is valid for assessing degradations that result from linear operations on the speech signal. Researchers have attempted to extend the {STI} to predict the intelligibility of nonlinearly processed speech by proposing variations that use speech as a probe signal. This work considers four previously proposed speech-based {STI} methods and four novel methods, studied under conditions of additive noise, reverberation, and two nonlinear operations (envelope thresholding and spectral subtraction). Analyzing intermediate metrics in the {STI} calculation reveals why some methods fail for nonlinear operations. Results indicate that none of the previously proposed methods is adequate for all of the conditions considered, while four proposed methods produce qualitatively reasonable results and warrant further study. The discussion considers the relevance of this work to predicting the intelligibility of cochlear-implant processed speech. {\copyright} 2004 Acoustical Society of America.},
    author = {Goldsworthy, Ray L. and Greenberg, Julie E.},
    citeulike-article-id = {11836815},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.1804628},
    day = {01},
    doi = {10.1121/1.1804628},
    keywords = {intelligibility, objectivemeasure},
    month = dec,
    number = {6},
    pages = {3679--3689},
    posted-at = {2012-12-10 01:06:11},
    priority = {2},
    journal = jasa,
    title = {Analysis of speech-based speech transmission index methods with implications for nonlinear operations},
    url = {http://dx.doi.org/10.1121/1.1804628},
    volume = {116},
    year = {2004}
}

@book{Stevens2000,
  title={Acoustic Phonetics},
  author={Stevens, K.N.},
  isbn={9780262692502},
  lccn={97046999},
  series={Current Studies in Linguistics Series},
  url={http://books.google.com/books?id=Gej94hCGrLMC},
  year={2000},
  publisher={{MIT} Press},
}

@book{Fant1970,
  title={Acoustic theory of speech production},
  author={Fant, G.},
  edition={Second},
  year={1970},
  publisher={Mouton \& Co N.~V.},
  location={The Hague},
}

@article{AstudilloEtAl2010,
    abstract = {In this paper, we show how uncertainty propagation, combined with observation uncertainty techniques, can be applied to a realistic implementation of robust distributed speech recognition ({DSR}) to improve recognition robustness furthermore, with little increase in computational complexity. Uncertainty propagation, or error propagation, techniques employ a probabilistic description of speech to reflect the information lost during speech enhancement or source separation in the time or frequency domain. This uncertain description is then propagated through the feature extraction process to the domain of features used in speech recognition. In this domain, the statistical information can be combined with the statistical parameters of the recognition model by employing observation uncertainty techniques. We show that the combination of a piecewise uncertainty propagation scheme with front-end uncertainty decoding or modified imputation improves the baseline of the advanced front-end ({AFE}), the state of the art algorithm of the European Telecommunications Standards Institute ({ETSI}), on the {AURORA5} database. We compare this method with other observation uncertainty techniques and show how the use of uncertainty propagation reduces the word error rates without the need for any kind of adaptation to noise using stereo data or iterative parameter estimation.},
    author = {Astudillo, R. F. and Kolossa, D. and Mandelartz, P. and Orglmeister, R.},
    citeulike-article-id = {11836825},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/JSTSP.2010.2057194},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5504821},
    doi = {10.1109/JSTSP.2010.2057194},
    institution = {Tech. Univ. Berlin Dept. of Electron. \& Med. Signal Process.},
    issn = {1932-4553},
    journal = {Selected Topics in Signal Processing, IEEE Journal of},
    keywords = {speechrecognition, uncertaintypropagation},
    month = oct,
    number = {5},
    pages = {824--833},
    posted-at = {2012-12-10 01:36:25},
    priority = {2},
    publisher = {IEEE},
    title = {An Uncertainty Propagation Approach to Robust {ASR} Using the {ETSI} Advanced {Front-End}},
    url = {http://dx.doi.org/10.1109/JSTSP.2010.2057194},
    volume = {4},
    year = {2010}
}

@article{Raj2004Reconstruction,
    abstract = {Speech recognition systems perform poorly in the presence of corrupting noise. Missing feature methods attempt to compensate for the noise by removing noise corrupted components of spectrographic representations of noisy speech and performing recognition with the remaining reliable components. Conventional classifier-compensation methods modify the recognition system to work with the incomplete representations so obtained. This constrains them to perform recognition using spectrographic features which are known to be less optimal than cepstra. In this paper we present two missing-feature algorithms that reconstruct complete spectrograms from incomplete noisy ones. Cepstral vectors can now be derived from the reconstructed spectrograms for recognition. The first algorithm uses {MAP} procedures to estimate corrupt components from their correlations with reliable components. The second algorithm clusters spectral vectors of clean speech. Corrupt components of noisy speech are estimated from the distribution of the cluster that the analysis frame is identified with. Experiments show that, although conventional classifier-compensation methods are superior when recognition is performed with spectrographic features, cepstra derived from the reconstructed spectrograms result in better recognition performance overall. The proposed methods are also less expensive computationally and do not require modification of the recognizer.},
    author = {Raj, Bhiksha and Seltzer, Michael L. and Stern, Richard M.},
    booktitle = {Special Issue on the Recognition and Organization of Real-World Sound},
    citeulike-article-id = {2216013},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.specom.2004.03.007},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6V1C-4D1YV53-1/2/1b4ea43751d95a4cf54057dc4c43c21f},
    doi = {10.1016/j.specom.2004.03.007},
    issn = {01676393},
    journal = spcomm,
    keywords = {missingdata, speechrecognition},
    month = sep,
    number = {4},
    pages = {275--296},
    posted-at = {2008-01-10 19:59:16},
    priority = {3},
    title = {Reconstruction of missing features for robust speech recognition},
    url = {http://dx.doi.org/10.1016/j.specom.2004.03.007},
    volume = {43},
    year = {2004}
}

@article{Cooke2001Robust,
    abstract = {Human speech perception is robust in the face of a wide variety of distortions, both experimentally applied and naturally occurring. In these conditions, state-of-the-art automatic speech recognition ({ASR}) technology fails. This paper describes an approach to robust {ASR} which acknowledges the fact that some spectro-temporal regions will be dominated by noise. For the purposes of recognition, these regions are treated as missing or unreliable. The primary advantage of this viewpoint is that it makes minimal assumptions about any noise background. Instead, reliable regions are identified, and subsequent decoding is based on this evidence. We introduce two approaches for dealing with unreliable evidence. The first – marginalisation – computes output probabilities on the basis of the reliable evidence only. The second – state-based data imputation – estimates values for the unreliable regions by conditioning on the reliable parts and the recognition hypothesis. A further source of information is the bounds on the energy of any constituent acoustic source in an additive mixture. This additional knowledge can be incorporated into the missing data framework. These approaches are applied to continuous-density hidden Markov model ({HMM})-based speech recognisers and evaluated on the {TIDigits} corpus for several noise conditions. Two criteria which use simple noise estimates are employed as a means of identifying reliable regions. The first treats regions which are negative after spectral subtraction as unreliable. The second uses the estimated noise spectrum to derive local signal-to-noise ratios, which are then thresholded to identify reliable data points. Both marginalisation and state-based data imputation produce a substantial performance advantage over spectral subtraction alone. The use of energy bounds leads to a further increase in performance for both approaches. While marginalisation outperforms data imputation, the latter technique allows the technique to act as a preprocessor for conventional recognisers, or in speech-enhancement applications.},
    author = {Cooke, Martin and Green, Phil and Josifovski, Ljubomir and Vizinho, Ascension},
    citeulike-article-id = {2247594},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0167-6393(00)00034-0},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6V1C-42RDMR2-3/2/12a5eb7447ef73220aec82d8425f4232},
    doi = {10.1016/S0167-6393(00)00034-0},
    issn = {01676393},
    journal = spcomm,
    keywords = {missingdata, reliability, separation, speechrecognition},
    month = jun,
    number = {3},
    pages = {267--285},
    posted-at = {2008-01-17 22:03:01},
    priority = {3},
    title = {Robust automatic speech recognition with missing and unreliable acoustic data},
    url = {http://dx.doi.org/10.1016/S0167-6393(00)00034-0},
    volume = {34},
    year = {2001}
}

@article{ Glass2003,
author = {J. Glass},
year = {2003},
title = {A probabilistic framework for segment-based speech recognition},
journal = {Computer Speech and Language}, 
volume = {17},
pages = {137-–152},
}

@inproceedings{ HasegawaJohnsonEtAl2005,
author = {M. Hasegawa-Johnson and J. Baker and S. Borys and K. Chen and E. Coogan and S. Greenberg and A. Juneja and K. Kirchhoff and K. Livescu and K. Sonmez and S. Mohan and J. Muller and and T. Wang},
title = {Landmark-based speech recognition},
booktitle = icassp,
year = {2005},
}

@article{ MorrisAndFoslerLussier2008,
author = {J. Morris and E. Fosler-Lussier},
title = {Conditional random fields for integrating local discriminative classifiers},
journal = taslp, 
volume = {16},
number = {3},
pages = {617-–628}, 
year = {2008},
}

@inproceedings{ StuekerEtAl2003,
author = {S. Stueker and F. Metze and T. Schultz and A. Waibel},
title = {Integrating multilingual articulatory features into speech recognition},
booktitle = {Proc. Eurospeech}, 
year = {2003},
}

@inproceedings{ GunawardanaEtAl2005,
author = {A. Gunawardana and M. Mahajan and A. Acero and J. C. Platt},
title = {Hidden conditional random fields for phone classification},
booktitle = {Proc. Interspeech}, 
location = {Lisbon, Portugal}, 
month = sep,
year = {2005},
}

@inproceedings{ LivescuEtAl2007,
author = {K. Livescu and O. Cetin and M. Hasegawa-Johnson and S. King and C. Bartels and N. Borges and A. Kantor and P. Lal and L. Yung and A. Bezman and S. Dawson-Haggerty and B. Woods and J. Frankel and M. Magimai-Doss and K. Saenko},
title = {Articulatory feature-based methods for acoustic and audio-visual speech recognition: Summary from the 2006 {JHU} Summer Workshop},
booktitle = icassp,
year = {2007},
}

@inproceedings{ GreenbergEtAl1996,
author = {S. Greenberg and J. Hollenback and D. Ellis},
title = {Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus},
booktitle = {ICSLP}, 
location = {Philadelphia}, 
month = oct,
pages={S32--35},
year = {1996},
}

@phdthesis{ FoslerLussier1999,
author = {J. E. Fosler-Lussier},
title = {Dynamic Pronunciation Models for Automatic Speech Recognition},
school = {U. C. Berkeley}, 
location = {Berkeley, CA}, 
year = {1999},
}

@article{LiEtAl2010,
    abstract = {Synthetic speech has been widely used in the study of speech cues. A serious disadvantage of this method is that it requires prior knowledge about the cues to be identified in order to synthesize the speech. Incomplete or inaccurate hypotheses about the cues often lead to speech sounds of low quality. In this research a psychoacoustic method, named three-dimensional deep search ({3DDS}), is developed to explore the perceptual cues of stop consonants from naturally produced speech. For a given sound, it measures the contribution of each subcomponent to perception by time truncating, highpass/lowpass filtering, or masking the speech with white noise. The {AI}-gram, a visualization tool that simulates the auditory peripheral processing, is used to predict the audible components of the speech sound. The results are generally in agreement with the classical studies that stops are characterized by a short duration burst followed by a F2 transition, suggesting the effectiveness of the {3DDS} method. However, it is also shown that /ba/ and /pa/ may have a wide band click as the dominant cue. F2 transition is not necessary for the perception of /ta/ and /ka/. Moreover, many stop consonants contain conflicting cues that are characteristic of competing sounds. The robustness of a consonant sound to noise is determined by the intensity of the dominant cue.},
    author = {Li, Feipeng and Menon, Anjali and Allen, Jont B.},
    citeulike-article-id = {11837629},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.3295689},
    day = {01},
    doi = {10.1121/1.3295689},
    keywords = {intelligibility, psychoacoustics},
    month = apr,
    number = {4},
    pages = {2599--2610},
    posted-at = {2012-12-10 15:10:46},
    priority = {0},
    journal = jasa,
    title = {A psychoacoustic method to find the perceptual cues of stop consonants in natural speech},
    url = {http://dx.doi.org/10.1121/1.3295689},
    volume = {127},
    year = {2010}
}

@incollection{ DusanAndRabiner2005,
author = {S. Dusan and L. R. Rabiner}, 
title = {Can automatic speech recognition learn more from human speech perception?},
booktitle = {Trends in Speech Technology}, 
editor = {C. Burileanu},
location = {Cluj Napoca, Romania},
publisher = {Romanian Academic Publisher},
year = {2005},
page = {21-–36},
}

@article{KimEtAl2009,
    abstract = {Traditional noise-suppression algorithms have been shown to improve speech quality, but not speech intelligibility. Motivated by prior intelligibility studies of speech synthesized using the ideal binary mask, an algorithm is proposed that decomposes the input signal into time-frequency ({T-F}) units and makes binary decisions, based on a Bayesian classifier, as to whether each {T-F} unit is dominated by the target or the masker. Speech corrupted at low signal-to-noise ratio ({SNR}) levels (−5 and 0 {dB}) using different types of maskers is synthesized by this algorithm and presented to normal-hearing listeners for identification. Results indicated substantial improvements in intelligibility (over 60\% points in −5 {dB} babble) over that attained by human listeners with unprocessed stimuli. The findings from this study suggest that algorithms that can estimate reliably the {SNR} in each {T-F} unit can improve speech intelligibility.},
    author = {Kim, Gibak and Lu, Yang and Hu, Yi and Loizou, Philipos C.},
    citeulike-article-id = {11837636},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.3184603},
    day = {01},
    doi = {10.1121/1.3184603},
    keywords = {idealbinarymask, intelligibility, masking, separation},
    month = sep,
    number = {3},
    pages = {1486--1494},
    posted-at = {2012-12-10 15:27:05},
    priority = {2},
    journal = jasa,
    title = {An algorithm that improves speech intelligibility in noise for normal-hearing listeners},
    url = {http://dx.doi.org/10.1121/1.3184603},
    volume = {126},
    year = {2009}
}

@article{Scharenborg2007,
    abstract = {The fields of human speech recognition ({HSR}) and automatic speech recognition ({ASR}) both investigate parts of the speech recognition process and have word recognition as their central issue. Although the research fields appear closely related, their aims and research methods are quite different. Despite these differences there is, however, lately a growing interest in possible cross-fertilisation. Researchers from both {ASR} and {HSR} are realising the potential benefit of looking at the research field on the other side of the 'gap'. In this paper, we provide an overview of past and present efforts to link human and automatic speech recognition research and present an overview of the literature describing the performance difference between machines and human listeners. The focus of the paper is on the mutual benefits to be derived from establishing closer collaborations and knowledge interchange between {ASR} and {HSR}. The paper ends with an argument for more and closer collaborations between researchers of {ASR} and {HSR} to further improve research in both fields.},
    address = {Amsterdam, The Netherlands, The Netherlands},
    author = {Scharenborg, Odette},
    citeulike-article-id = {4717415},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1243164},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.specom.2007.01.009},
    doi = {10.1016/j.specom.2007.01.009},
    issn = {01676393},
    journal = spcomm,
    keywords = {evaluation, review, speech, speechrecognition},
    month = may,
    number = {5},
    pages = {336--347},
    posted-at = {2009-08-19 20:24:23},
    priority = {0},
    publisher = {Elsevier Science Publishers B. V.},
    title = {Reaching over the gap: A review of efforts to link human and automatic speech recognition research},
    url = {http://dx.doi.org/10.1016/j.specom.2007.01.009},
    volume = {49},
    year = {2007}
}
@article{Lippmann1997Speech,
    abstract = {This paper reviews past work comparing modern speech recognition systems and humans to determine how far recent dramatic advances in technology have progressed towards the goal of human-like performance. Comparisons use six modern speech corpora with vocabularies ranging from 10 to more than 65,000 words and content ranging from read isolated words to spontaneous conversations. Error rates of machines are often more than an order of magnitude greater than those of humans for quiet, wideband, read speech. Machine performance degrades further below that of humans in noise, with channel variability, and for spontaneous speech. Humans can also recognize quiet, clearly spoken nonsense syllables and nonsense sentences with little high-level grammatical information. These comparisons suggest that the human-machine performance gap can be reduced by basic research on improving low-level acoustic-phonetic modeling, on improving robustness with noise and channel variability, and on more accurately modeling spontaneous speech.},
    author = {Lippmann, R.},
    citeulike-article-id = {4486972},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0167-6393(97)00021-6},
    doi = {10.1016/S0167-6393(97)00021-6},
    issn = {01676393},
    journal = spcomm,
    keywords = {6820, psych, speechrecognition, survey},
    month = jul,
    number = {1},
    pages = {1--15},
    posted-at = {2009-05-07 15:48:24},
    priority = {0},
    title = {Speech recognition by machines and humans},
    url = {http://dx.doi.org/10.1016/S0167-6393(97)00021-6},
    volume = {22},
    year = {1997}
}

@inproceedings{Moore2003,
    author = {Roger K. Moore},
    title = {A comparison of the data requirements of automatic speech recognition systems and human listeners},
    booktitle = {Proc. Eurospeech},
    location = {Geneva},
    year = {2003},
    pages = {2582--2584}
}

@inproceedings{ MeyerEtAl2006,
author = {Meyer, B. and Wesker, T. and Brand, T. and Mertins, A. and Kollmeier, B.},
year = 2006,
title = {A human-machine comparison in speech recognition based on a logatome corpus},
booktitle = {Proc.~Workshop on Speech Recognition and Intrinsic Variation}, 
location = {Toulouse},
}

@article{SrokaAndBraida2005,
    abstract = {Three traditional {ASR} parameterizations matched with Hidden Markov Models ({HMMs}) are compared to humans for speaker-dependent consonant recognition using nonsense syllables degraded by highpass filtering, lowpass filtering, or additive noise. Confusion matrices were determined by recognizing the syllables using different {ASR} front ends, including {Mel-Filter} Bank ({MFB}) energies, {Mel-Filtered} Cepstral Coefficients ({MFCCs}), and the Ensemble Interval Histogram ({EIH}). In general the {MFB} recognition accuracy was slightly higher than the {MFCC}, which was higher than the {EIH}. For syllables degraded by lowpass and highpass filtering, automated systems trained on the degraded condition recognized the consonants as well as humans. For syllables degraded by additive speech-shaped noise, none of the automated systems recognized consonants as well as humans. The greatest advantage displayed by humans was in determining the correct voiced/unvoiced classification of consonants in noise.},
    author = {Sroka, Jason J. and Braida, Louis D.},
    citeulike-article-id = {11837657},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.specom.2004.11.009},
    doi = {10.1016/j.specom.2004.11.009},
    issn = {01676393},
    journal = spcomm,
    keywords = {intelligibility, psychoacoustics, speechrecognition},
    month = apr,
    number = {4},
    pages = {401--423},
    posted-at = {2012-12-10 16:13:55},
    priority = {2},
    title = {Human and machine consonant recognition},
    url = {http://dx.doi.org/10.1016/j.specom.2004.11.009},
    volume = {45},
    year = {2005}
}

@article{JuergensAndBrand2009,
    abstract = {This study compares the phoneme recognition performance in speech-shaped noise of a microscopic model for speech recognition with the performance of normal-hearing listeners.  ``Microscopic'' is defined in terms of this model twofold. First, the speech recognition rate is predicted on a phoneme-by-phoneme basis. Second, microscopic modeling means that the signal waveforms to be recognized are processed by mimicking elementary parts of human's auditory processing. The model is based on an approach by 
Holube and Kollmeier [J. Acoust. Soc. Am. 100, 1703–1716 (1996)]
 and consists of a psychoacoustically and physiologically motivated preprocessing and a simple dynamic-time-warp speech recognizer. The model is evaluated while presenting nonsense speech in a closed-set paradigm. Averaged phoneme recognition rates, specific phoneme recognition rates, and phoneme confusions are analyzed. The influence of different perceptual distance measures and of the model's a-priori knowledge is investigated. The results show that human performance can be predicted by this model using an optimal detector, i.e., identical speech waveforms for both training of the recognizer and testing. The best model performance is yielded by distance measures which focus mainly on small perceptual distances and neglect outliers.},
    author = {J\"{u}rgens, Tim and Brand, Thomas},
    citeulike-article-id = {11837664},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.3224721},
    day = {01},
    doi = {10.1121/1.3224721},
    keywords = {intelligibility, intelligibilitymodel, speechrecognition},
    month = nov,
    number = {5},
    pages = {2635--2648},
    posted-at = {2012-12-10 16:30:20},
    priority = {2},
    journal = jasa,
    title = {Microscopic prediction of speech recognition for listeners with normal hearing in noise using an auditory model},
    url = {http://dx.doi.org/10.1121/1.3224721},
    volume = {126},
    year = {2009}
}

@article{FestenAndPlomp1990,
    abstract = {The speech‐reception threshold ({SRT}) for sentences presented in a fluctuating interfering background sound of 80 {dBA} {SPL} is measured for 20 normal‐hearing listeners and 20 listeners with sensorineural hearing impairment. The interfering sounds range from steady‐state noise, via modulated noise, to a single competing voice. Two voices are used, one male and one female, and the spectrum of the masker is shaped according to these voices. For both voices, the {SRT} is measured as well in noise spectrally shaped according to the target voice as shaped according to the other voice. The results show that, for normal‐hearing listeners, the {SRT} for sentences in modulated noise is 4–6 {dB} lower than for steady‐state noise; for sentences masked by a competing voice, this difference is 6–8 {dB}. For listeners with moderate sensorineural hearing loss, elevated thresholds are obtained without an appreciable effect of masker fluctuations. The implications of these results for estimating a hearing handicap in everyday conditions are discussed. By using the articulation index ({AI}), it is shown that hearing‐impaired individuals perform poorer than suggested by the loss of audibility for some parts of the speech signal. Finally, three mechanisms are discussed that contribute to the absence of unmasking by masker fluctuations in hearing‐impaired listeners. The low sensation level at which the impaired listeners receive the masker seems a major determinant. The second and third factors are: reduced temporal resolution and a reduction in comodulation masking release, respectively.},
    author = {Festen, Joost M. and Plomp, Reinier},
    citeulike-article-id = {11288790},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.400247},
    day = {01},
    doi = {10.1121/1.400247},
    keywords = {hearing, hearingimpaired, psychoacoustics, srt},
    month = oct,
    number = {4},
    pages = {1725--1736},
    posted-at = {2012-09-21 20:33:01},
    priority = {2},
    journal = jasa,
    title = {Effects of fluctuating noise and interfering speech on the speech‐reception threshold for impaired and normal hearing},
    url = {http://dx.doi.org/10.1121/1.400247},
    volume = {88},
    year = {1990}
}

@article{PhatakAndAllen2007,
    abstract = {This paper presents the results of a closed-set recognition task for 64 consonant-vowel sounds (16 C×4 V, spoken by 18 talkers) in speech-weighted noise (−22,−20,−16,−10,−2 [dB]) and in quiet. The confusion matrices were generated using responses of a homogeneous set of ten listeners and the confusions were analyzed using a graphical method. In speech-weighted noise the consonants separate into three sets: a low-scoring set C1 (/f/, /θ/, /v/, /ð/, /b/, /m/), a high-scoring set C2 (/t/, /s/, /z/, /ʃ/, /ʒ/) and set C3 (/n/, /p/, /g/, /k/, /d/) with intermediate scores. The perceptual consonant groups are C1: { /f/-/θ/, /b/-/v/-/ð/, /θ/-/ð/ }, C2: { /s/-/z/, /ʃ/-/ʒ/ }, and C3: /m/-/n/, while the perceptual vowel groups are /ɑ/-/{\ae}/ and /ε/-/ɪ/. The exponential articulation index (AI) model for consonant score works for 12 of the 16 consonants, using a refined expression of the AI. Finally, a comparison with past work shows that white noise masks the consonants more uniformly than speech-weighted noise, and shows that the AI, because it can account for the differences in noise spectra, is a better measure than the wideband signal-to-noise ratio for modeling and comparing the scores with different noise maskers.},
    author = {Phatak, Sandeep A. and Allen, Jont B.},
    citeulike-article-id = {11837671},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.2642397},
    day = {01},
    doi = {10.1121/1.2642397},
    keywords = {intelligibility, phonemes, psychoacoustics},
    month = apr,
    number = {4},
    pages = {2312--2326},
    posted-at = {2012-12-10 16:49:49},
    priority = {0},
    journal = jasa,
    title = {Consonant and vowel confusions in speech-weighted noise},
    url = {http://dx.doi.org/10.1121/1.2642397},
    volume = {121},
    year = {2007}
}

@inproceedings{ CareyAndQuang2005,
author = {Carey, M.J. and Quang, T.P.}, 
year = {2005},
title = {A speech similarity distance weighting for robust recognition},
booktitle = {Interspeech}, 
location = {Lisbon},
pages = {1257–-1260},
}

@article{KjemsEtAl2009,
    abstract = {Intelligibility of ideal binary masked noisy speech was measured on a group of normal hearing individuals across mixture signal to noise ratio ({SNR}) levels, masker types, and local criteria for forming the binary mask. The binary mask is computed from time-frequency decompositions of target and masker signals using two different schemes: an ideal binary mask computed by thresholding the local {SNR} within time-frequency units and a target binary mask computed by comparing the local target energy against the long-term average speech spectrum. By depicting intelligibility scores as a function of the difference between mixture {SNR} and local {SNR} threshold, alignment of the performance curves is obtained for a large range of mixture {SNR} levels. Large intelligibility benefits are obtained for both sparse and dense binary masks. When an ideal mask is dense with many ones, the effect of changing mixture {SNR} level while fixing the mask is significant, whereas for more sparse masks the effect is small or insignificant.},
    author = {Kjems, Ulrik and Boldt, Jesper B. and Pedersen, Michael S. and Lunner, Thomas and Wang, DeLiang},
    citeulike-article-id = {11837694},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.3179673},
    day = {01},
    doi = {10.1121/1.3179673},
    keywords = {idealbinarymask, intelligibility, masking, psychoacoustics},
    month = sep,
    number = {3},
    pages = {1415--1426},
    posted-at = {2012-12-10 17:12:06},
    priority = {0},
    journal = jasa,
    title = {Role of mask pattern in intelligibility of ideal binary-masked noisy speech},
    url = {http://dx.doi.org/10.1121/1.3179673},
    volume = {126},
    year = {2009}
}

@article{ShannonEtAl1995,
    abstract = {Nearly perfect speech recognition was observed under conditions of greatly reduced spectral information. Temporal envelopes of speech were extracted from broad frequency bands and were used to modulate noises of the same bandwidths. This manipulation preserved temporal envelope cues in each band but restricted the listener to severely degraded information on the distribution of spectral energy. The identification of consonants, vowels, and words in simple sentences improved markedly as the number of bands increased; high speech recognition performance was obtained with only three bands of modulated noise. Thus, the presentation of a dynamic temporal pattern in only a few broad spectral regions is sufficient for the recognition of speech.},
    author = {Robert V. Shannon and Zeng, Fan-Gang and Kamath, Vivek and Wygonski, John and Ekelid, Michael},
    citeulike-article-id = {2448206},
    citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.270.5234.303},
    citeulike-linkout-1 = {http://www.sciencemag.org/cgi/content/abstract/270/5234/303},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/7569981},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=7569981},
    day = {13},
    doi = {10.1126/science.270.5234.303},
    journal = {Science},
    keywords = {intelligibility, noise, psych},
    month = oct,
    number = {5234},
    pages = {303--304},
    pmid = {7569981},
    posted-at = {2009-08-01 17:18:55},
    priority = {2},
    title = {Speech Recognition with Primarily Temporal Cues},
    url = {http://dx.doi.org/10.1126/science.270.5234.303},
    volume = {270},
    year = {1995}
}

@article{HolubeAndKollmeier1996,
    abstract = {Sensorineural hearing‐impaired listeners suffer severely from deterioration in the processing and internal representation of acoustic signals. In order to understand this deterioration in detail, a numerical perception model was developed which is based on current functional models of the signal processing in the auditory system. To test this model, the individual's speech intelligibility in quiet and in noise was predicted. The primary input parameter of the model is the precisely measured audiogram of each listener. In a refined version of the model, additional input parameters are derived from predicting the individual's temporal forward masking and notched‐noise measurements with the same model assumptions. The predictions of the perception model were compared with those of the articulation index ({AI}) and the speech transmission index ({STI}). The accuracy of prediction with the perception model is in the same range as with the {AI} and the {STI}. The model does not require a calibration function and has the advantage of a greater flexibility in including different processing deficits associated with hearing impairment. However, it requires more time for computation. For the hearing‐impaired listeners examined so far the individually measured psychoacoustical parameters have only a secondary effect on the prediction as compared to the audiogram. Nevertheless, the underlying model is a first step toward a quantitative understanding of speech intelligibility and helps to distinguish between the influence of the ''attenuation'' and the ''distortion'' component of the hearing loss. {\copyright} 1996 Acoustical Society of America.},
    author = {Holube, Inga and Kollmeier, Birger},
    citeulike-article-id = {11837790},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.417354},
    day = {01},
    doi = {10.1121/1.417354},
    keywords = {dtw, intelligibilitymodel, psychoacoustics, speechrecognition},
    month = sep,
    number = {3},
    pages = {1703--1716},
    posted-at = {2012-12-10 19:38:07},
    priority = {2},
    journal = jasa,
    title = {Speech intelligibility prediction in hearing‐impaired listeners based on a psychoacoustically motivated perception model},
    url = {http://dx.doi.org/10.1121/1.417354},
    volume = {100},
    year = {1996}
}

@article{AinsworthAndMeyer1994,
    abstract = {Some of the effects of noise on the perception of plosive‐vowel syllables have been investigated. It was found that noise added to the syllables for the duration of the speech had a more deleterious effect on perception than noise of the same intensity played continuously. Physiological experiments have shown that the response thresholds of cochlear nerve fibers to tones are raised by continuous background noise but not by short bursts of noise. It is suggested that this may be responsible for the speech perception results. In order to investigate this, an auditory model was developed which incorporated response threshold shifts. This was interfaced to a hidden Markov model recognizer and tested with the same sounds that were employed in the human perception experiments. The recognition scores were greater with the threshold shifts than without them.},
    author = {Ainsworth, W. A. and Meyer, G. F.},
    citeulike-article-id = {11837789},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.410306},
    day = {01},
    doi = {10.1121/1.410306},
    keywords = {intelligibility, intelligibilitymodel, psychoacoustics, speechrecognition},
    month = aug,
    number = {2},
    pages = {687--694},
    posted-at = {2012-12-10 19:37:28},
    priority = {2},
    journal = jasa,
    title = {Recognition of plosive syllables in noise: Comparison of an auditory model with human performance},
    url = {http://dx.doi.org/10.1121/1.410306},
    volume = {96},
    year = {1994}
}

@article{Ghitza1993,
    abstract = {A long‐standing question that arises when studying a particular auditory model is how to evaluate its performance. More precisely, it is of interest to evaluate to what extent the model representation can describe the actual human internal representation. Here, this question is addressed in the context of speech perception. That is, given a speech representation based on the auditory system, to what extent can it preserve phonetic information that is perceptually relevant? To answer this question, a diagnostic system has been developed that simulates the psychophysical procedure used in the standard {Diagnostic‐Rhyme} Test ({DRT}). In the psychophysical procedure, the subject has all the cognitive information needed for the discrimination task, a priori. Hence, errors in discrimination are due mainly to inaccuracies in the auditory representation of the stimulus. In the simulation, the human observer is replaced by an array of recognizers, one for each pair of words in the {DRT} database. The recognizer used [Ghitza and Sondhi, J. Acoust. Soc. Am. Suppl. 1 87, S107 (1990)] was designed to keep errors due to the recognition procedure to a minimum, so that the overall detected errors are due mainly to inaccuracies in the front‐end representation. The system provides detailed diagnostics that show the error distributions among six phonetically distinctive features. Results are given for the behavior of two speech analysis methods, a representation based on the auditory system and one based on the Fourier power spectrum, in quiet and in a noisy environment. These results are compared with psychophysical results for the same database.},
    author = {Ghitza, Oded},
    citeulike-article-id = {11837787},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.406679},
    day = {01},
    doi = {10.1121/1.406679},
    keywords = {intelligibilitymodel, microscopic, psychoacoustics},
    month = apr,
    number = {4},
    pages = {2160--2171},
    posted-at = {2012-12-10 19:36:43},
    priority = {2},
    journal = jasa,
    title = {Adequacy of auditory models to predict human internal representation of speech sounds},
    url = {http://dx.doi.org/10.1121/1.406679},
    volume = {93},
    year = {1993}
}

@article{CookeEtAl2006,
    abstract = {An audio-visual corpus has been collected to support the use of common material in speech perception and automatic speech recognition studies. The corpus consists of high-quality audio and video recordings of 1000 sentences spoken by each of 34 talkers. Sentences are simple, syntactically identical phrases such as  ” place green at B 4 now.” Intelligibility tests using the audio signals suggest that the material is easily identifiable in quiet and low levels of stationary noise. The annotated corpus is available on the web for research use.},
    author = {Cooke, Martin and Barker, Jon and Cunningham, Stuart and Shao, Xu},
    citeulike-article-id = {11837785},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.2229005},
    day = {01},
    doi = {10.1121/1.2229005},
    keywords = {corpus, speechrecognition, video},
    month = nov,
    number = {5},
    pages = {2421--2424},
    posted-at = {2012-12-10 19:36:08},
    priority = {2},
    journal = jasa,
    title = {An audio-visual corpus for speech perception and automatic speech recognition},
    url = {http://dx.doi.org/10.1121/1.2229005},
    volume = {120},
    year = {2006}
}


@article{ Kochkin2012,
        author = {Sergei Kochkin},
        title = {{MarkeTrak VIII}: The Key Influencing Factors in Hearing Aid Purchase Intent},
        year = {2012},
        month = mar,
        journal = {Hearing Review},
}


@incollection{Wang2005,
    abstract = {In his famous treatise of computational vision, Marr (1982) makes a compelling argument for separating different levels of analysis in order to understand complex information processing. In particular, the computational theory level, concerned with the goal of computation and general processing strategy, must be separated from the algorithm level, or the separation of what from how. This chapter is an attempt at a computational-theory analysis of auditory scene analysis, where the main task is to understand the character of the {CASA} problem.
                        My analysis results in the proposal of the ideal binary mask as a main goal of {CASA}. This goal is consistent with characteristics of human auditory scene analysis. The goal is also consistent with more specific objectives such as enhancing {ASR} and speech intelligibility. The resulting evaluation metric has the properties of simplicity and generality, and is easy to apply when the premixing target is available. The goal of the ideal binary mask has led to effective for speech separation algorithms that attempt to explicitly estimate such masks.},
    address = {Boston},
    author = {Wang, DeLiang},
    booktitle = {Speech Separation by Humans and Machines},
    chapter = {12},
    citeulike-article-id = {4284733},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/0-387-22794-6\_12},
    citeulike-linkout-1 = {http://www.springerlink.com/content/p41103403v35338k},
    citeulike-linkout-2 = {http://link.springer.com/chapter/10.1007/0-387-22794-6\_12},
    doi = {10.1007/0-387-22794-6\_12},
    editor = {Divenyi, Pierre},
    isbn = {1-4020-8001-8},
    keywords = {idealbinarymask, separation},
    pages = {181--197},
    posted-at = {2012-12-10 21:55:51},
    priority = {0},
    publisher = {Springer US},
    title = {On Ideal Binary Mask As the Computational Goal of Auditory Scene Analysis},
    url = {http://dx.doi.org/10.1007/0-387-22794-6\_12},
    year = {2005}
}

@article{MeyerEtAl2011,
    abstract = {The aim of this study is to quantify the gap between the recognition performance of human listeners and an automatic speech recognition ({ASR}) system with special focus on intrinsic variations of speech, such as speaking rate and effort, altered pitch, and the presence of dialect and accent. Second, it is investigated if the most common {ASR} features contain all information required to recognize speech in noisy environments by using resynthesized {ASR} features in listening experiments. For the phoneme recognition task, the {ASR} system achieved the human performance level only when the signal-to-noise ratio ({SNR}) was increased by 15 {dB}, which is an estimate for the human–machine gap in terms of the {SNR}. The major part of this gap is attributed to the feature extraction stage, since human listeners achieve comparable recognition scores when the {SNR} difference between unaltered and resynthesized utterances is 10 {dB}. Intrinsic variabilities result in strong increases of error rates, both in human speech recognition ({HSR}) and {ASR} (with a relative increase of up to 120\%). An analysis of phoneme duration and recognition rates indicates that human listeners are better able to identify temporal cues than the machine at low {SNRs}, which suggests incorporating information about the temporal dynamics of speech into {ASR} systems.},
    author = {Meyer, Bernd T. and Brand, Thomas and Kollmeier, Birger},
    citeulike-article-id = {11837883},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.3514525},
    day = {01},
    doi = {10.1121/1.3514525},
    keywords = {intelligibility, psychoacoustics, speechrecognition},
    month = jan,
    number = {1},
    pages = {388--403},
    posted-at = {2012-12-10 22:03:15},
    priority = {2},
    journal = jasa,
    title = {Effect of speech-intrinsic variations on human and automatic recognition of spoken phonemes},
    url = {http://dx.doi.org/10.1121/1.3514525},
    volume = {129},
    year = {2011}
}

@article{Juneja2012,
    abstract = {The accuracy of automatic speech recognition ({ASR}) systems is generally evaluated using corpora of grammatically sound read speech or natural spontaneous speech. This prohibits an accurate estimation of the performance of the acoustic modeling part of {ASR} because the language modeling performance is inherently integrated in the overall performance metric. In this work, {ASR} and human speech recognition ({HSR}) accuracies are compared for null grammar sentences in different signal-to-noise ratios and vocabulary sizes—1000, 2000, 4000, and 8000. The results shed light on differences between {ASR} and {HSR} in relative significance of bottom-up word recognition and context awareness.},
    author = {Juneja, Amit},
    citeulike-article-id = {11837893},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.3684744},
    day = {01},
    doi = {10.1121/1.3684744},
    keywords = {intelligibility, psychoacoustics, speechrecognition},
    month = mar,
    number = {3},
    pages = {EL256--EL261},
    posted-at = {2012-12-10 22:19:40},
    priority = {2},
    journal = jasa,
    title = {A comparison of automatic and human speech recognition in null grammar},
    url = {http://dx.doi.org/10.1121/1.3684744},
    volume = {131},
    year = {2012}
}

@article{HowardJonesAndRosen1993,
    abstract = {The ability of listeners to ''glimpse'' acoustic cues during the quieter sections of an interrupted noise has primarily been studied using maskers with interruptions occurring simultaneously across the entire frequency range of the masker—broadband comodulated interruptions. Here, the possibility of uncomodulated glimpsing (the glimpsing of acoustic cues separated both in time and frequency) was investigated. To achieve this, speech reception thresholds for a set of intervocalic consonants were adaptively measured in {100‐Hz} to {10‐kHz} pink noise divided into a varying number of frequency bands of equal energy. In uncomodulated noise conditions, the odd and even numbered bands were switched on and off alternately at a rate of 10 Hz. The spectrograms of such noises (on log frequency scales), resemble portions of a checkerboard. Glimpsing in ''checkerboard'' noise was found with maskers divided into two and four bands, but not into eight bands or more. Further investigations showed that, in the two‐band case, this release from masking was indeed due to uncomodulated glimpsing, and not simply attributable to glimpsing in one of the modulated bands. In the four‐band case, the release from masking in checkerboard noise can be accounted for without recourse to uncomodulated glimpsing. Interestingly, conditions which allowed glimpsing resulted in greater intersubject variability. The implications of these results for quantitative analyses of masker fluctuations are discussed.},
    author = {Howard-Jones, Paul A. and Rosen, Stuart}, 
    citeulike-article-id = {11837902},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.405811},
    day = {01},
    doi = {10.1121/1.405811},
    keywords = {glimpsing, intelligibility, psychoacoustics},
    month = may,
    number = {5},
    pages = {2915--2922},
    posted-at = {2012-12-10 22:58:41},
    priority = {0},
    journal = jasa,
    title = {Uncomodulated glimpsing in ''checkerboard'' noise},
    url = {http://dx.doi.org/10.1121/1.405811},
    volume = {93},
    year = {1993}
}

@article{MillerAndNicely1955,
    abstract = {Sixteen English consonants were spoken over voice communication systems with frequency distortion and with random masking noise. The listeners were forced to guess at every sound and a count was made of all the different errors that resulted when one sound was confused with another. With noise or low‐pass filtering the confusions fall into consistent patterns, but with high‐pass filtering the errors are scattered quite randomly. An articulatory analysis of these 16 consonants provides a system of five articulatory features or  ” dimensions” that serve to characterize and distinguish the different phonemes: voicing, nasality, affrication, duration, and place of articulation. The data indicate that voicing and nasality are little affected and that place is severely affected by low‐pass and noisy systems. The indications are that the perception of any one of these five features is relatively independent of the perception of the others, so that it is as if five separate, simple channels were involved rather than a single complex channel.},
    author = {Miller, George A. and Nicely, Patricia E.},
    citeulike-article-id = {1608947},
    citeulike-linkout-0 = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=JASMAN000027000002000338000001\&idtype=cvips\&gifs=yes},
    citeulike-linkout-1 = {http://link.aip.org/link/?JAS/27/338},
    citeulike-linkout-2 = {http://dx.doi.org/10.1121/1.1907526},
    day = {01},
    doi = {10.1121/1.1907526},
    journal = {The Journal of the Acoustical Society of America},
    keywords = {confusion, intelligibility, phonemes},
    month = mar,
    number = {2},
    pages = {338--352},
    posted-at = {2012-12-10 23:01:04},
    priority = {0},
    publisher = {Acoustical Society of America},
    title = {An Analysis of Perceptual Confusions Among Some English Consonants},
    url = {http://dx.doi.org/10.1121/1.1907526},
    volume = {27},
    year = {1955}
}

@article{MillerAndLicklider1950,
    abstract = {This paper concerns the effects of interrupting speech waves—turning them on and off intermittently or masking them with intermittent noise—upon their intelligibility. The effects were studied with various rates of interruption and with the speech left undisturbed various percentages of the time. Tests were conducted (1) with speech turned on and off in quiet, (2) with continuous speech masked by interrupted white noise, and (3) with speech and noise interrupted alternately, the speech wave being turned on as the noise wave was turned off, and vice versa.(1) When the speech wave is turned on and off infrequently, the percentage of the message that is missed is approximately the same as the percentage of time the speech is off. When the interruptions are periodic and occur more often than 10,000 times per second, the interruptions do not interfere with the reception of the message. In the quiet it is easy to understand conversational speech so long as the interruptions occur more than 10 times per second.(2) When continuous speech waves are masked by noise that is interrupted more than 200 times per second, intelligibility is independent of the interruption frequency and of the percentage of time the noise is on, provided the ratio of average speech power to average noise power is held constant. Interrupted masking noise impairs intelligibility least if the frequency of interruption is about 15 per second.(3) When interrupted speech and interrupted noise alternate at frequencies below 10 alternations per second, the noise does not impair intelligibility. At higher frequencies of alternation the temporal spread of masking becomes {appreciable.The} general features of these results are approximately the same whether the interruptions occur periodically or at random.},
    author = {Miller, George A. and Licklider, J. C. R.},
    citeulike-article-id = {2291495},
    citeulike-linkout-0 = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=JASMAN000022000002000167000001\&idtype=cvips\&gifs=yes},
    citeulike-linkout-1 = {http://link.aip.org/link/?JAS/22/167},
    citeulike-linkout-2 = {http://dx.doi.org/10.1121/1.1906584},
    day = {01},
    doi = {10.1121/1.1906584},
    journal = {The Journal of the Acoustical Society of America},
    keywords = {intelligibility, psychoacoustics},
    month = mar,
    number = {2},
    pages = {167--173},
    posted-at = {2012-12-10 23:02:22},
    priority = {0},
    publisher = {Acoustical Society of America},
    title = {The Intelligibility of Interrupted Speech},
    url = {http://dx.doi.org/10.1121/1.1906584},
    volume = {22},
    year = {1950}
}

@book{htkbook,
    address = {Cambridge, UK},
    author = {Young, S. J. and Evermann, G. and Gales, M. J. F. and Hain, T. and Kershaw, D. and Moore, G. and Odell, J. and Ollason, D. and Povey, D. and Valtchev, V. and Woodland, P. C.},
    citeulike-article-id = {2746956},
    keywords = {gmm, hmm, speechrecognition},
    posted-at = {2010-04-30 14:34:02},
    priority = {2},
    publisher = {Cambridge University Engineering Department},
    title = {The {HTK} Book, version 3.4},
    year = {2006}
}

@inproceedings{HermanskyEtAl2000,
    abstract = {Hidden Markov model speech recognition systems typically use Gaussian mixture models to estimate the distributions of decorrelated acoustic feature vectors that correspond to individual subword units. By contrast, hybrid {connectionist-HMM} systems use discriminatively-trained neural networks to estimate the probability distribution among subword units given the acoustic observations. In this work we show a large improvement in word recognition performance by combining neural-net discriminative feature processing with Gaussian-mixture distribution modeling. By training the network to generate the subword probability posteriors, then using transformations of these estimates as the base features for a conventionally-trained Gaussian-mixture based system, we achieve relative error rate reductions of 35\% or more on the multicondition Aurora noisy continuous digits task.},
    author = {Hermansky, H. and Ellis, D. P. W. and Sharma, S.},
    citeulike-article-id = {7062915},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/ICASSP.2000.862024},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ICASSP.2000.862024},
    doi = {10.1109/ICASSP.2000.862024},
    isbn = {0-7803-6293-4},
    booktitle = icassp,
    keywords = {neuralnet, speechrecognition},
    pages = {1635--1638},
    posted-at = {2010-04-22 18:12:51},
    priority = {0},
    title = {Tandem connectionist feature extraction for conventional {HMM} systems},
    url = {http://dx.doi.org/10.1109/ICASSP.2000.862024},
    volume = {3},
    year = {2000}
}

@techreport{House1963,
  title={Psychoacoustic speech tests: A modified rhyme test},
  author={House, A.S. and Williams, C. and Hecker, M.H. and Kryter, K.D.},
  year={1963},
  institution={DTIC Document},
  abstract={A multiple choice, and evaluated. It easily scored speech test has been developed was found that the speech intelligibility scores obtained with this test remain consistent for a given communication system when tested nearly daily for a period of one month using enlisted personnel as test listeners.},
  number={ESD-TDR-63-403},
  month=jun,
}

@article{ ShannonEtAl2001,
author = {Shannon, R. V. and Galvin, K. K. and Baskent, D.},
year = 2001,
title = {Holes in hearing},
journal = {Journal of Research in Otolaryngology},
volume = {3},
pages = {185--199},
}

@article{ ApouxAndBacon2004,
author = {F. Apoux and S. Bacon},
year = {2004},
title = {Relative importance of temporal information in various frequency regions for consonant identification in quiet and in noise},
journal = jasa,
volume = {116},
pages = {1671--1680},
}

@article{ CalandruccioAndDoherty2007,
author = {Calandruccio, L. and Doherty, K.},
year = 2007,
title = {Spectral weighting strategies for sentences measured by a correlational method},
journal = jasa,
volume = 121,
pages = {3827-3836},
}

@article{ DohertyAndTurner1996,
author = {Doherty, K. A. and Turner, C. W.},
year = {1996},
title = {Use of the correlational method to estimate a listener's weighting function of speech},
journal = jasa,
volume = {100},
pages = {3768--3773},
}

@article{ KasturiEtAl2002,
author = {Kasturi, K. and Loizou, P. C. and Dorman, M. and Spahr, T.},
year = {2002},
title = {The intelligibility of speech witih 'holes' in the spectrum},
journal = jasa,
volume = {112},
pages = {1102--1111},
}

@article{ TurnerEtAl1998,
author = {C. W. Turner and B. J. Kwon and C. Tanaka and J. Knapp and J. L. Hubbartt and K. A. Doherty},
year = {1998},
title = {Frequency-weighting functions for broadband speech as estimated by a correlational method},
journal = jasa,
volume = {104},
pages = {1580--1585},
}

@article{KirchhoffEtAl2002,
    abstract = {The idea of using articulatory representations for automatic speech recognition ({ASR}) continues to attract much attention in the speech community. Representations which are grouped under the label  ” articulatory” include articulatory parameters derived by means of acoustic-articulatory transformations (inverse filtering), direct physical measurements or classification scores for pseudo-articulatory features. In this study, we revisit the use of features belonging to the third category. In particular, we concentrate on the potential benefits of pseudo-articulatory features in adverse acoustic environments and on their combination with standard acoustic features. Systems based on articulatory features only and combined acoustic-articulatory systems are tested on two different recognition tasks: telephone-speech continuous numbers recognition and conversational speech recognition. We show that articulatory feature ({AF}) systems are capable of achieving a superior performance at high noise levels and that the combination of acoustic and {AFs} consistently leads to a significant reduction of word error rate across all acoustic conditions.},
    author = {Kirchhoff, Katrin and Fink, Gernot A. and Sagerer, Gerhard},
    citeulike-article-id = {11838045},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0167-6393(01)00020-6},
    doi = {10.1016/S0167-6393(01)00020-6},
    issn = {01676393},
    journal = spcomm,
    keywords = {articulatory, speechrecognition},
    month = jul,
    number = {3-4},
    pages = {303--319},
    posted-at = {2012-12-10 23:35:55},
    priority = {2},
    title = {Combining acoustic and articulatory feature information for robust speech recognition},
    url = {http://dx.doi.org/10.1016/S0167-6393(01)00020-6},
    volume = {37},
    year = {2002}
}

@techreport{ SIIStandard,
author = {{ANSI}},
title = {Methods for calculating the speech intelligibility index},
institution = {American national standards institute},
number = {S3.5-1997},
year = {1997},
city = {New York},
}

@techreport{ AIStandard,
author = {{ANSI}},
title = {Methods for Calculation of the Articulation Index},
institution = {American national standards institute},
number = {S3.5-1969},
year = 1969,
city = {New York},
}

@techreport{ G160,
   institution = {International Telecommunication Union Telecommunication Standardization Sector},
   organization = {{ITU-T}},
   label = {ITU-T G.160,},
   title = {Voice enhancement devices},
   number = {G.160},
   year = {2012},
   month = jun,
}

@techreport{ P835,
  institution = {International Telecommunication Union Telecommunication Standardization Sector},
  organization = {{ITU-T}},
  label = {ITU-T P.835},
  title = {Subjective test methodology for evaluating speech communication systems that include noise suppression algorithm},
  number = {P.835},
  year = {2003},
  month = nov,
}

@techreport{ MUSHRA,
  institution = {International Telecommunication Union Radiocommunication Standardization Sector},
  organization = {{ITU-R}},
  label = {ITU-R BS-1534-3,},
  title = {Method for the subjective assessment of intermediate quality level of audio systems},
  number = {BS.1534-3},
  year = {2015},
  month = oct,
}

@book{ Levelt1989,
title = {Speaking: from intention to articulation},
author = {Willem J. M. Levelt},
year = {1989},
publisher = {MIT Press},
location = {Cambridge, MA},
}

@book{Flanagan1972,
  title={Speech analysis: Synthesis and perception},
  author={Flanagan, J.L.},
  year={1972},
  publisher={Springer-Verlag}
}

@article{Bronkhorst2000,
    abstract = {It is in most cases relatively easy to understand one talker even if other persons are talking at the same time. Over the last decades, this so-called cocktail party phenomenon has been the subject of a considerable number of studies. In the present paper, an overview is given of part of this research, specifically those studies dealing with the intelligibility of speech presented against a background of competing speech. In the first section of the review, the properties of speech are considered that are relevant when it acts as an interfering sound: the long-term average frequency spectrum, and the modulation spectrum. In the middle sections, speech intelligibility data are reviewed for monaural and binaural listening and with one or multiple interfering voices (or speech-like sounds). It appears from these data that speech intelligibility depends in a complex manner on the properties of the interfering signal(s), the number of signals, the spatial configuration of the sources, and the acoustic environment. This dependency can be predicted in part by existing models. For the prediction of effects of voice similarity and interferer modulations, however, no suitable models are available. The fifth section of the review is devoted to research on effects of hearing impairment and the use of hearing aids. These studies demonstrate that speech intelligibility in conditions involving interfering speech is significantly poorer for the average hearing-impaired listener than for the normal hearing. Hearing aids can only be effective when they yield both selective amplification and an increase of the signal-to-noise ratio at the ears (e.g. by using directional microphones). The review ends by pointing out areas of interest for future research.},
    author = {Bronkhorst, Adelbert W.},
    citeulike-article-id = {3057217},
    citeulike-linkout-0 = {http://www.ingentaconnect.com/content/dav/aaua/2000/00000086/00000001/art00013},
    citeulike-linkout-1 = {http://www.ingentaconnect.com/content/dav/aaua/2000/00000086/00000001/art00013},
    issn = {1610-1928},
    journal = {Acta Acustica united with Acustica},
    keywords = {psych, separation, speech},
    month = jan,
    pages = {117--128},
    posted-at = {2008-07-29 17:40:25},
    priority = {2},
    publisher = {S. Hirzel Verlag},
    title = {The Cocktail Party Phenomenon: A Review of Research on Speech Intelligibility in {Multiple-Talker} Conditions},
    url = {http://www.ingentaconnect.com/content/dav/aaua/2000/00000086/00000001/art00013},
    year = {2000}
}

@book{Moore2007,
  title={Cochlear hearing loss: physiological, psychological and technical issues},
  author={Moore, B.},
  year={2007},
  edition={Second},
  publisher={Wiley-Interscience},
}

@techreport{Settles2009,
Author = {Burr Settles},
Institution = {University of Wisconsin--Madison},
Number = {1648},
Title = {Active Learning Literature Survey},
Type = {Computer Sciences Technical Report},
Year = {2009},
}

@phdthesis{ Lim2010,
author = {B. P. Lim}, 
title = {Computational differences between whispered and non-whispered speech},
school = {University of Illinois at Urbana-Champaign},
year = {2010},
}

@incollection{ArakiEtAl2012,
    abstract = {This paper summarizes the audio part of the 2011 community-based Signal Separation Evaluation Campaign ({SiSEC2011}). Four speech and music datasets were contributed, including datasets recorded in noisy or dynamic environments and a subset of the {SiSEC2010} datasets. The participants addressed one or more tasks out of four source separation tasks, and the results for each task were evaluated using different objective performance criteria. We provide an overview of the audio datasets, tasks and criteria. We also report the results achieved with the submitted systems, and discuss organization strategies for future campaigns.},
    author = {Araki, Shoko and Nesta, Francesco and Vincent, Emmanuel and Koldovsk\'{y}, Zbyn\v{e}k and Nolte, Guido and Ziehe, Andreas and Benichoux, Alexis},
    booktitle = {Latent Variable Analysis and Signal Separation},
    citeulike-article-id = {11840104},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-28551-6\_51},
    citeulike-linkout-1 = {http://link.springer.com/chapter/10.1007/978-3-642-28551-6\_51},
    doi = {10.1007/978-3-642-28551-6\_51},
    editor = {Theis, Fabian and Cichocki, Andrzej and Yeredor, Arie and Zibulevsky, Michael},
    keywords = {evaluation, separation},
    pages = {414--422},
    posted-at = {2012-12-12 01:01:41},
    priority = {2},
    publisher = {Springer Berlin Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {The 2011 Signal Separation Evaluation Campaign ({SiSEC2011}): - Audio Source Separation -},
    url = {http://dx.doi.org/10.1007/978-3-642-28551-6\_51},
    volume = {7191},
    year = {2012}
}

@article{ CoxAndSavoy2003,
author = {Cox, D. D. and Savoy, R. L.},
title = {Functional magnetic resonance imaging ({fMRI}) `brain reading': detecting and classifying distributed patterns of {fMRI} activity in human visual cortex},
journal = {Neuroimage},
volume = {19}, 
pages = {261–-270},
year = {2003},
}

@article{Haynes2006,
  title={Decoding mental states from brain activity in humans},
  author={Haynes, J.D. and Rees, G.},
  journal={Nature Reviews Neuroscience},
  volume={7},
  number={7},
  pages={523--534},
  year={2006},
  publisher={Nature Publishing Group},
}

@article{ HealyEtAl2013,
  author = {E. W. Healy and S. E. Yoho and F. Apoux},
  title = {Band-importance for sentences and words re-examined},
  journal = jasa,
  year = {2013},
  volume = {133},
  number = {1},
  note = {In press},
}

@inproceedings{BahlEtAl1986,
    abstract = {A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation.},
    author = {Bahl, L. and Brown, P. and de Souza, P. and Mercer, R.},
    booktitle = icassp,
    citeulike-article-id = {3857857},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICASSP.1986.1169179},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1169179},
    doi = {10.1109/ICASSP.1986.1169179},
    institution = {IBM Thomas J. Watson Research Center},
    keywords = {discriminative, speechrecognition},
    month = apr,
    pages = {49--52},
    posted-at = {2012-12-12 20:46:19},
    priority = {2},
    publisher = {IEEE},
    title = {Maximum mutual information estimation of hidden Markov model parameters for speech recognition},
    url = {http://dx.doi.org/10.1109/ICASSP.1986.1169179},
    volume = {11},
    year = {1986}
}

@article{WoodlandAndPovey2002,
    abstract = {This paper describes, and evaluates on a large scale, the lattice based framework for discriminative training of large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models ({HMMs}). This paper concentrates on the maximum mutual information estimation ({MMIE}) criterion which has been used to train {HMM} systems for conversational telephone speech transcription using up to 265 hours of training data. These experiments represent the largest-scale application of discriminative training techniques for speech recognition of which the authors are aware. Details are given of the {MMIE} lattice-based implementation used with the extended {Baum-Welch} algorithm, which makes training of such large systems computationally feasible. Techniques for improving generalization using acoustic scaling and weakened language models are discussed. The overall technique has allowed the estimation of triphone and quinphone {HMM} parameters which has led to significant reductions in word error rate for the transcription of conversational telephone speech relative to our best systems trained using maximum likelihood estimation ({MLE}). This is in contrast to some previous studies, which have concluded that there is little benefit in using discriminative training for the most difficult large vocabulary speech recognition tasks. The lattice {MMIE}-based discriminative training scheme is also shown to out-perform the frame discrimination technique. Various properties of the lattice-based {MMIE} training scheme are investigated including comparisons of different lattice processing strategies (full search and exact-match) and the effect of lattice size on performance. Furthermore a scheme based on the linear interpolation of the {MMIE} and {MLE} objective functions is shown to reduce the danger of over-training. It is shown that {HMMs} trained with {MMIE} benefit as much as {MLE}-trained {HMMs} from applying model adaptation using maximum likelihood linear regression ({MLLR}). This has allowed the straightforward integration of {MMIE}-trained {HMMs} into complex multi-pass systems for transcription of conversational telephone speech and has contributed to our {MMIE}-trained systems giving the lowest word error rates in both the 2000 and 2001 {NIST} Hub5 evaluations.},
    author = {Woodland, P. C. and Povey, D.},
    citeulike-article-id = {2924153},
    citeulike-linkout-0 = {http://dx.doi.org/10.1006/csla.2001.0182},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6WCW-458NBGK-3/1/e6dfd7024f705146e21e78ce31d90811},
    doi = {10.1006/csla.2001.0182},
    issn = {08852308},
    journal = {Computer Speech \& Language},
    keywords = {discriminative, speechrecognition},
    month = jan,
    number = {1},
    pages = {25--47},
    posted-at = {2012-12-12 20:47:32},
    priority = {2},
    title = {Large scale discriminative training of hidden Markov models for speech recognition},
    url = {http://dx.doi.org/10.1006/csla.2001.0182},
    volume = {16},
    year = {2002}
}

@article{Hermansky1990,
    abstract = {A new technique for the analysis of speech, the perceptual linear predictive ({PLP}) technique, is presented and examined. This technique uses three concepts from the psychophysics of hearing to derive an estimate of the auditory spectrum: (1) the critical‐band spectral resolution, (2) the equal‐loudness curve, and (3) the intensity‐loudness power law. The auditory spectrum is then approximated by an autoregressive all‐pole model. A 5th‐order all‐pole model is effective in suppressing speaker‐dependent details of the auditory spectrum. In comparison with conventional linear predictive ({LP}) analysis, {PLP} analysis is more consistent with human hearing. The effective second formant F2′ and the {3.5‐Bark} spectral‐peak integration theories of vowel perception are well accounted for. {PLP} analysis is computationally efficient and yields a low‐dimensional representation of speech. These properties are found to be useful in speaker‐independent automatic‐speech recognition.},
    author = {Hermansky, Hynek},
    citeulike-article-id = {11841335},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.399423},
    day = {01},
    doi = {10.1121/1.399423},
    keywords = {features, speechrecognition},
    month = apr,
    number = {4},
    pages = {1738--1752},
    posted-at = {2012-12-12 20:49:35},
    priority = {2},
    journal = jasa,
    title = {Perceptual linear predictive ({PLP}) analysis of speech},
    url = {http://dx.doi.org/10.1121/1.399423},
    volume = {87},
    year = {1990}
}

@inproceedings{EideAndGish1996,
    abstract = {Differences in vocal tract size among individual speakers contribute to the variability of speech waveforms. The first-order effect of a difference in vocal tract length is a scaling of the frequency axis; a female speaker, for example, exhibits formants roughly 20\% higher than the formants of from a male speaker, with the differences most severe in open vocal tract configurations. We describe a parametric method of normalisation which counteracts the effect of varied vocal tract length. The method is shown to be effective across a wide range of recognition systems and paradigms, but is particularly helpful in the case of a small amount of training data},
    author = {Eide, E. and Gish, H.},
    booktitle = icassp,
    citeulike-article-id = {11841339},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICASSP.1996.541103},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=541103},
    doi = {10.1109/ICASSP.1996.541103},
    institution = {BBN Syst. \& Technol. Corp., Cambridge, MA},
    isbn = {0-7803-3192-3},
    issn = {1520-6149},
    keywords = {normalization, speechrecognition, vocaltract},
    month = may,
    pages = {346--348},
    posted-at = {2012-12-12 21:02:44},
    priority = {2},
    publisher = {IEEE},
    title = {A parametric approach to vocal tract length normalization},
    url = {http://dx.doi.org/10.1109/ICASSP.1996.541103},
    volume = {1},
    year = {1996}
}

@incollection{FarnetaniAndRecasens1997,
  title={Coarticulation and connected speech processes},
  author={Farnetani, E. and Recasens, D.},
  booktitle={The handbook of phonetic sciences},
  editor={W. J. Hardcastle and J. Laver and F. E. Gibbon},
  pages={316--352},
  publisher={Blackwell Oxford},
  chapter={9},
  edition={Second},
  year={1997},
}

 @article{GlasbergAndMoore1990,
    abstract = {A well established method for estimating the shape of the auditory filter is based on the measurement of the threshold of a sinusoidal signal in a notched-noise masker, as a function of notch width. To measure the asymmetry of the filter, the notch has to be placed both symmetrically and asymmetrically about the signal frequency. In previous work several simplifying assumptions and approximations were made in deriving auditory filter shapes from the data. In this paper we describe modifications to the fitting procedure which allow more accurate derivations. These include: 1) taking into account changes in filter bandwidth with centre frequency when allowing for the effects of off-frequency listening; 2) correcting for the non-flat frequency response of the earphone; 3) correcting for the transmission characteristics of the outer and middle ear; 4) limiting the amount by which the centre frequency of the filter can shift in order to maximise the signal-to-masker ratio. In many cases, these modifications result in only small changes to the derived filter shape. However, at very high and very low centre frequencies and for hearing-impaired subjects the differences can be substantial. It is also shown that filter shapes derived from data where the notch is always placed symmetrically about the signal frequency can be seriously in error when the underlying filter is markedly asymmetric. New formulae are suggested describing the variation of the auditory filter with frequency and level. The implications of the results for the calculation of excitation patterns are discussed and a modified procedure is proposed. The appendix lists {FORTRAN} computer programs for deriving auditory filter shapes from notched-noise data and for calculating excitation patterns. The first program can readily be modified so as to derive auditory filter shapes from data obtained with other types of maskers, such as rippled noise.},
    author = {Glasberg, Brian R. and Moore, Brian C. J.},
    citeulike-article-id = {9600995},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/0378-5955(90)90170-T},
    day = {01},
    doi = {10.1016/0378-5955(90)90170-T},
    issn = {03785955},
    journal = {Hearing Research},
    keywords = {criticalbands, erb, psychoacoustics},
    month = aug,
    number = {1-2},
    pages = {103--138},
    posted-at = {2012-12-14 16:01:00},
    priority = {2},
    title = {Derivation of auditory filter shapes from notched-noise data},
    url = {http://dx.doi.org/10.1016/0378-5955(90)90170-T},
    volume = {47},
    year = {1990}
}

@article{SarampalisEtAl2009,
    abstract = {{PurposeThis} work is aimed at addressing a seeming contradiction related to the use of noise-reduction ({NR}) algorithms in hearing aids. The problem is that although some listeners claim a subjective improvement from {NR}, it has not been shown to improve speech intelligibility, often even making it worse.  {MethodTo} address this, the hypothesis tested here is that the positive effects of {NR} might be to reduce cognitive effort directed toward speech reception, making it available for other tasks. Normal-hearing individuals participated in 2 dual-task experiments, in which 1 task was to report sentences or words in noise set to various signal-to-noise ratios. Secondary tasks involved either holding words in short-term memory or responding in a complex visual reaction-time task.  {ResultsAt} low values of signal-to-noise ratio, although {NR} had no positive effect on speech reception thresholds, it led to better performance on the word-memory task and quicker responses in visual reaction times.  {ConclusionsResults} from both dual tasks support the hypothesis that {NR} reduces listening effort and frees up cognitive resources for other tasks. Future hearing aid research should incorporate objective measurements of cognitive benefits.},
    author = {Sarampalis, Anastasios and Kalluri, Sridhar and Edwards, Brent and Hafter, Ervin},
    citeulike-article-id = {11845174},
    citeulike-linkout-0 = {http://dx.doi.org/10.1044/1092-4388(2009/08-0111)},
    citeulike-linkout-1 = {http://jslhr.asha.org/cgi/content/abstract/52/5/1230},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/19380604},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=19380604},
    day = {1},
    doi = {10.1044/1092-4388(2009/08-0111)},
    journal = {J Speech Lang Hear Res},
    keywords = {hearingaids, hearingimpaired, listeningeffort, psychoacoustics},
    month = oct,
    number = {5},
    pages = {1230--1240},
    pmid = {19380604},
    posted-at = {2012-12-14 16:21:22},
    priority = {2},
    title = {Objective Measures of Listening Effort: Effects of Background Noise and Noise Reduction},
    url = {http://dx.doi.org/10.1044/1092-4388(2009/08-0111)},
    volume = {52},
    year = {2009}
}

@inproceedings{HirschAndPierce2000,
    abstract = {This paper describes a database designed to evaluate the performance of speech recognition algorithms in noisy conditions. The database may either be used for the evaluation of front-end feature extraction algorithms using a defined {HMM} recognition back-end or complete recognition systems. The source speech for this database is the {TIdigits}, consisting of connected digits task spoken by American English talkers (downsampled to 8 {kHz}). A selection of 8 different real-world noises have been added to the speech over a range of signal to noise ratios and special care has been taken to control the filtering of both the speech and noise.

The framework was prepared as a contribution to the {ETSI} {STQ}-{AURORA} {DSR} Working Group [1]. Aurora is developing standards for Distributed Speech Recognition ({DSR}) where the speech analysis is done in the telecommunication terminal and the recognition at a central location in the telecom network. The framework is currently being used to evaluate alternative proposals for front-end feature extraction. The database has been made publicly available through {ELRA} so that other speech researchers can evaluate and compare the performance of noise robust algorithms.

Recognition results are presented for the first standard {DSR} feature extraction scheme that is based on a cepstral analysis.},
    author = {Hirsch, Hans-Gunter and Pearce, David},
    booktitle = {ASR-2000},
    citeulike-article-id = {11845460},
    citeulike-linkout-0 = {http://www.isca-speech.org/archive\_open/asr2000/asr0\_181.html},
    keywords = {data, noise},
    location = {Paris},
    month = sep,
    pages = {181--188},
    posted-at = {2012-12-15 02:18:38},
    priority = {2},
    title = {The {AURORA} Experimental Framework For The Performance Evaluation of Speech Recognition Systems Under Noisy Conditions},
    url = {http://www.isca-speech.org/archive\_open/asr2000/asr0\_181.html},
    year = {2000}
}

@article{LivescuEtAl2012,
    abstract = {Modern automatic speech recognition systems handle large vocabularies of words, making it infeasible to collect enough repetitions of each word to train individual word models. Instead, large-vocabulary recognizers represent each word in terms of subword units. Typically the subword unit is the phone, a basic speech sound such as a single consonant or vowel. Each word is then represented as a sequence, or several alternative sequences, of phones specified in a pronunciation dictionary. Other choices of subword units have been studied as well. The choice of subword units, and the way in which the recognizer represents words in terms of combinations of those units, is the problem of subword modeling. Different subword models may be preferable in different settings, such as high-variability conversational speech, high-noise conditions, low-resource settings, or multilingual speech recognition. This article reviews past, present, and emerging approaches to subword modeling. To make clean comparisons between many approaches, the review uses the unifying language of graphical models.},
    author = {Livescu, K. and Fosler-Lussier, E. and Metze, F.},
    citeulike-article-id = {11845470},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/MSP.2012.2210952},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6296531},
    doi = {10.1109/MSP.2012.2210952},
    issn = {1053-5888},
    journal = {Signal Processing Magazine, IEEE},
    keywords = {articulatory, speechrecognition, survey},
    month = nov,
    number = {6},
    pages = {44--57},
    posted-at = {2012-12-15 03:20:22},
    priority = {0},
    publisher = {IEEE},
    title = {Subword Modeling for Automatic Speech Recognition: Past, Present, and Emerging Approaches},
    url = {http://dx.doi.org/10.1109/MSP.2012.2210952},
    volume = {29},
    year = {2012}
}

@article{ RifkinEtAl2003,
author = {Rifkin, R. and Yeo, G. and and Poggio, T.},
title = {Regularized least squares classification},
journal = {Advances in Learning Theory: Methods, Model and Applications NATO Science Series III: Computer and Systems Sciences},
volume = {190},
pages = {131–-153}, 
year = {2003},
}

@article{ OzerovEtAl2012,
author = {A. Ozerov and E. Vincent and F. Bimbot}, 
title = {A general flexible framework for the handling of prior information in audio source separation}, 
journal = taslp,
volume = {20},
number = {4},
pages = {1118--1133},
year = {2012},
}

@inproceedings{WoodruffAndWang2012,
    abstract = {We propose an approach to binaural speech segregation in reverberation based on pitch and azimuth cues. These cues are integrated within a statistical tracking framework to estimate up to two concurrent pitch frequencies and three concurrent azimuth angles. The tracking framework implicitly estimates binary time-frequency masks by solving a data association problem, thereby performing speech segregation. Experimental results show that the proposed approach compares favorably to existing two-microphone systems in spite of less prior information. The benefit of the proposed approach is most pronounced in conditions with substantial reverberation or for closely spaced sources.},
    author = {Woodruff, J. and Wang, DeLiang},
    booktitle = icassp,
    citeulike-article-id = {11846622},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICASSP.2012.6287862},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6287862},
    doi = {10.1109/ICASSP.2012.6287862},
    institution = {Ohio State Univ. Dept. of Comput. Sci. \& Eng.},
    isbn = {978-1-4673-0045-2},
    issn = {1520-6149},
    keywords = {binaural, monaural, separation},
    month = mar,
    pages = {241--244},
    posted-at = {2012-12-16 15:08:15},
    priority = {2},
    publisher = {IEEE},
    title = {Binaural speech segregation based on pitch and azimuth tracking},
    url = {http://dx.doi.org/10.1109/ICASSP.2012.6287862},
    year = {2012}
}

@inproceedings{Jourjine2000Blind,
	abstract = {We present a novel method for blind separation of any number of sources using only two mixtures. The method applies when sources are (W-)disjoint orthogonal, that is, when the supports of the (windowed) Fourier transform of any two signals in the mixture are disjoint sets. We show that, for anechoic mixtures of attenuated and delayed sources, the method allows one to estimate the mixing parameters by clustering ratios of the time-frequency representations of the mixtures. The estimates of the mixing parameters are then used to partition the time-frequency representation of one mixture to recover the original sources. The technique is valid even in the case when the number of sources is larger than the number of mixtures. The general results are verified on both speech and wireless signals},
	author = {Jourjine, Alexander  and Rickard, Scott  and Yilmaz, \"Ozg\"ur},
	booktitle = icassp,
	citeulike-article-id = {2215910},
	doi = {10.1109/ICASSP.2000.861162},
	journal = {},
	keywords = {bib-taslp08, bib-waspaa09, separation, underdetermined},
	pages = {2985--2988},
	posted-at = {2008-01-10 19:30:22},
	priority = {2},
	title = {Blind separation of disjoint orthogonal signals: demixing N sources from 2 mixtures},
	url = {http://dx.doi.org/10.1109/ICASSP.2000.861162},
	volume = {5},
	year = {2000}
}

@inproceedings{mouba06,
	author = {Mouba, Joan  and Marchand, Sylvain },
	booktitle = dafx,
	citeulike-article-id = {2212547},
	keywords = {bib-taslp08, bib-waspaa07, bibtex-import},
	pages = {233--238},
	posted-at = {2008-01-10 00:20:52},
	priority = {0},
	title = {A source localization/separation/respatialization system based on unsupervised classification of interaural cues},
	url = {http://www.dafx.ca/proceedings/papers/p\_233.pdf},
	year = {2006}
}

@inproceedings{Sawada2007TwoStage,
	abstract = {This paper proposes a two-stage method for the blind separation of convolutively mixed sources. We employ time-frequency masking, which can be applied even to an underdetermined case where the number of sensors is insufficient for the number of sources. In the first stage of the method, frequency bin-wise mixtures are classified based on Gaussian mixture model fitting. In the second stage, the permutation ambiguities of the bin-wise classified signals are aligned by clustering the posterior probability sequences calculated in the first stage. Experimental results for separating four speeches with three microphones under reverberant conditions show the superiority of the proposed method over existing methods based on time-difference-of-arrival estimations or signal envelope clustering.},
	author = {Sawada, Hiroshi   and Araki, Shoko   and Makino, Shoji  },
	booktitle = waspaa,
	citeulike-article-id = {2215900},
	doi = {10.1109/ASPAA.2007.4393012},
	journal = waspaa,
	keywords = {bib-taslp08, ica, separation, underdetermined},
	pages = {139--142},
	posted-at = {2008-01-10 19:26:52},
	priority = {0},
	title = {A Two-Stage Frequency-Domain Blind Source Separation Method for Underdetermined Convolutive Mixtures},
	url = {http://dx.doi.org/10.1109/ASPAA.2007.4393012},
	year = {2007}
}

@article{Vincent2006Performance,
    abstract = {In this paper, we discuss the evaluation of blind audio source separation ({BASS}) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several {BASS} problems with various difficulty levels},
    author = {Vincent, E. and Gribonval, R. and Fevotte, C.},
    booktitle = {{IEEE} Transactions on Audio, Speech, and Language Processing},
    citeulike-article-id = {3885691},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TSA.2005.858005},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1643671},
    doi = {10.1109/TSA.2005.858005},
    institution = {Electron. Eng. Dept., Queen Mary Univ. of London},
    issn = {1558-7916},
    journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
    keywords = {bib-taslp08, bss, evaluation, separation},
    month = jul,
    number = {4},
    pages = {1462--1469},
    posted-at = {2009-01-14 15:58:07},
    priority = {2},
    publisher = {IEEE},
    title = {Performance measurement in blind audio source separation},
    url = {http://dx.doi.org/10.1109/TSA.2005.858005},
    volume = {14},
    year = {2006}
}

@article{Li2009Optimality,
	abstract = {The concept of ideal binary time–frequency masks has received attention recently in monaural and binaural sound separation. Although often assumed, the optimality of ideal binary masks in terms of signal-to-noise ratio has not been rigorously addressed. In this paper we give a formal treatment on this issue and clarify the conditions for ideal binary masks to be optimal. We also experimentally compare the performance of ideal binary masks to that of ideal ratio masks on a speech mixture database and a music database. The results show that ideal binary masks are close in performance to ideal ratio masks which are closely related to the Wiener filter, the theoretically optimal linear filter.},
	author = {Li, Y.  and Wang, D. },
	citeulike-article-id = {4254157},
	doi = {10.1016/j.specom.2008.09.001},
	issn = {01676393},
	journal = spcomm,
	keywords = {bib-waspaa09, evaluation, oracle, separation, weinerfilter},
	month = mar,
	number = {3},
	pages = {230--239},
	posted-at = {2009-04-01 18:02:13},
	priority = {0},
	title = {On the optimality of ideal binary time-frequency masks},
	url = {http://dx.doi.org/10.1016/j.specom.2008.09.001},
	volume = {51},
	year = {2009}
}

@article{KimAndLoizou2010,
    abstract = {While most speech enhancement algorithms improve speech quality, they may not improve speech intelligibility in noise. This paper focuses on the development of an algorithm that can be optimized for a specific acoustic environment and improve speech intelligibility. The proposed method decomposes the input signal into time-frequency ({T-F}) units and makes binary decisions, based on a Bayesian classifier, as to whether each {T-F} unit is dominated by the target signal or the noise masker. Target-dominated {T-F} units are retained while masker-dominated {T-F} units are discarded. The Bayesian classifier is trained for each acoustic environment using an incremental approach that continuously updates the model parameters as more data become available. Listening experiments were conducted to assess the intelligibility of speech synthesized using the incrementally adapted models as a function of the number of training sentences. Results indicated substantial improvements in intelligibility (over 60\% in babble at -5 {dB} {SNR}) with as few as ten training sentences in babble and at least 80 sentences in other noisy conditions.},
    author = {Kim, Gibak and Loizou, P. C.},
    citeulike-article-id = {7154136},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TASL.2010.2041116},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5398891},
    doi = {10.1109/TASL.2010.2041116},
    institution = {Univ. of Texas at Dallas Dept. of Electr. Eng.},
    issn = {1558-7916},
    journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
    keywords = {evaluation, intelligibility, separation},
    month = nov,
    number = {8},
    pages = {2080--2090},
    posted-at = {2012-12-16 15:42:05},
    priority = {2},
    publisher = {IEEE},
    title = {Improving Speech Intelligibility in Noise Using {Environment-Optimized} Algorithms},
    url = {http://dx.doi.org/10.1109/TASL.2010.2041116},
    volume = {18},
    year = {2010},
}

@article{RegnierAndAllen2008,
    abstract = {This study focuses on correlating speech confusion patterns, defined as consonant-vowel confusion as a function of the speech-to-noise ratio, and a model acoustic feature ({AF}) representation called the {AI} gram, defined as the articulation index density in the spectrotemporal domain. By collecting many responses from many talkers and listeners, the {AF} and psychophysical feature (event) is shown to be correlated via the {AI}-gram model and the confusion matrices at the utterance level, thereby explaining the listener confusion. Consonant /t/ is used as an example to identify its primary robust-to-noise feature, and a precise correlation of the acoustic information with the listeners' confusions is used to label the event. The main spectrotemporal cue defining the /t/ event is an across-frequency temporal coincidence, wherein frequency spread and robustness vary across utterances, while the event remains invariant. The cross-frequency timing event is shown to be the key perceptual feature for consonants in a vowel following context. Coincidences are found to form the basic element of the auditory object. Neural circuits used for coincidence in binaural processing for localization across ears are proposed to be used within one ear across channels. It is further concluded that the event is based on the audibility of the /t/ burst rather than on any superthreshold property.},
    author = {R\'{e}gnier, Marion S. and Allen, Jont B.},
    citeulike-article-id = {11846638},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.2897915},
    day = {01},
    doi = {10.1121/1.2897915},
    keywords = {acousticphonetics, intelligibility},
    month = may,
    number = {5},
    pages = {2801--2814},
    posted-at = {2012-12-16 15:47:38},
    priority = {2},
    journal = jasa,
    title = {A method to identify noise-robust perceptual features: Application for consonant /t/},
    url = {http://dx.doi.org/10.1121/1.2897915},
    volume = {123},
    year = {2008}
}

@article{AllenAndLi2009,
    abstract = {Speech sounds are encoded by time-varying spectral patterns called acoustic cues. The processing and detection of these acoustic cues lead to events defined as the psychological correlates of the acoustic cues. Due to the similarity between the acoustic cues, speech sounds form natural confusion groups. When the feature of the sound within a group is masked by noise, one event can turn into another. A systematic psychoacoustic "{3-D} method" has been developed to explore the perceptual cues of stop consonants from naturally produced speech sounds. For each sound, our {3-D} method measures the contribution of each subcomponent by time-truncating, high-pass/low-pass filtering, and masking with noise. The Al-gram, a visualization tool that simulates the auditory peripheral processing, is used to predict the audible components of the speech sound. The results are that the plosive consonants are defined by a short duration bursts characterized by their center frequency, as well as the delay to the onset of voicing. Fricatives are characterized by the duration and bandwidth of a noise-like feature. Pilot studies of hearing-impaired ({HI}) speech perception indicate that cochlear dead regions have a considerable impact on consonant identification. An {HI} listener may have problems understanding speech simply because he/she cannot hear certain sounds, since the events are missing due to either the hearing loss, or the masking effect introduced by the noise.},
    author = {Allen, J. and Li, Feipeng},
    citeulike-article-id = {11846641},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/MSP.2009.932564},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5174500},
    doi = {10.1109/MSP.2009.932564},
    institution = {Univ. of Illinois Electr. Eng.},
    issn = {1053-5888},
    journal = {Signal Processing Magazine, IEEE},
    keywords = {acousticphonetics, intelligibility},
    month = jul,
    number = {4},
    pages = {73--77},
    posted-at = {2012-12-16 15:50:32},
    priority = {2},
    publisher = {IEEE},
    title = {Speech perception and cochlear signal processing [Life Sciences]},
    url = {http://dx.doi.org/10.1109/MSP.2009.932564},
    volume = {26},
    year = {2009}
}


@INPROCEEDINGS{VincentEtAl2008,
    author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
     title = {Extracting and Composing Robust Features with Denoising Autoencoders},
      year = {2008},
     pages = {1096--1103},
 booktitle = icml,
  abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization
for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In
this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for
RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This
approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.}
}


@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}


@inproceedings{dahl2013improving,
  title={Improving Deep Neural Networks for LVCSR using Rectified Linear Units and Dropout},
  author={Dahl, George E and Sainath, Tara N and Hinton, Geoffrey E},
  booktitle=icassp,
  year={2013}
}

@article{HintonEtAl2012,
    author = {Hinton, G. and Deng, Li and Yu, Dong and Dahl, G. E. and Mohamed, A. and Jaitly, N. and Senior, A. and Vanhoucke, V. and Nguyen, P. and Sainath, T. N. and Kingsbury, B.},
    citeulike-article-id = {12721718},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/msp.2012.2205597},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6296526},
    doi = {10.1109/msp.2012.2205597},
    issn = {1053-5888},
    journal = {Signal Processing Magazine, IEEE},
    keywords = {deeplearning, hmm, hybrid, neuralnet},
    month = nov,
    number = {6},
    pages = {82--97},
    posted-at = {2013-10-15 04:09:15},
    priority = {0},
    publisher = {IEEE},
    title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
    url = {http://dx.doi.org/10.1109/msp.2012.2205597},
    volume = {29},
    year = {2012}
}

@inproceedings{DroppoEtAl2002,
    author = {Droppo, J. and Acero, A. and Deng, Li},
    booktitle = icassp,
    month = may,
    pages = {57--60},
    publisher = {IEEE},
    title = {Uncertainty decoding with {SPLICE} for noise robust speech recognition},
    volume = {1},
    year = {2002}
}

@article{ShannonEtAl1999,
    author = {Shannon, Robert V. and Jensvold, Angela and Padilla, Monica and Robert, Mark E. and Wang, Xiaosong},
    journal = jasa,
    number = {6},
    pages = {L71+},
    title = {Consonant recordings for speech testing},
    url = {http://dx.doi.org/10.1121/1.428150},
    volume = {106},
    year = {1999}
}
  
@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle=icml,
  pages={41--48},
  year={2009},
}

@article{Brungart2006Isolating,
    abstract = {When a target speech signal is obscured by an interfering speech wave form, comprehension of the target message depends both on the successful detection of the energy from the target speech wave form and on the successful extraction and recognition of the spectro-temporal energy pattern of the target out of a background of acoustically similar masker sounds. This study attempted to isolate the effects that energetic masking, defined as the loss of detectable target information due to the spectral overlap of the target and masking signals, has on multitalker speech perception. This was achieved through the use of ideal time-frequency binary masks that retained those spectro-temporal regions of the acoustic mixture that were dominated by the target speech but eliminated those regions that were dominated by the interfering speech. The results suggest that energetic masking plays a relatively small role in the overall masking that occurs when speech is masked by interfering speech but a much more significant role when speech is masked by interfering noise.},
    author = {Brungart, Douglas S. and Chang, Peter S. and Simpson, Brian D. and Wang, Deliang},
    citeulike-article-id = {2972975},
    citeulike-linkout-0 = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=JASMAN000120000006004007000001\&idtype=cvips\&gifs=yes},
    citeulike-linkout-1 = {http://link.aip.org/link/?JAS/120/4007},
    citeulike-linkout-2 = {http://dx.doi.org/10.1121/1.2363929},
    day = {01},
    doi = {10.1121/1.2363929},
    issn = {0001-4966},
    journal = jasa,
    keywords = {informationalmasking, intelligibility, masking, psych, speech},
    month = dec,
    number = {6},
    pages = {4007--4018},
    posted-at = {2008-07-08 17:19:39},
    priority = {3},
    publisher = {ASA},
    title = {Isolating the energetic component of speech-on-speech masking with ideal time-frequency segregation},
    url = {http://dx.doi.org/10.1121/1.2363929},
    volume = {120},
    year = {2006}
}

@article{LiAndLoizou2007,
    abstract = {The idea that listeners are able to  ” glimpse” the target speech in the presence of competing noise has been supported by many studies, and is based on the assumption that listeners are able to glimpse pieces of the target speech occurring at different times and somehow patch them together to hear out the target speech. The factors influencing glimpsing in noise are not well understood and are examined in the present study. Specifically, the effects of the frequency location, spectral width, and duration of the glimpses are examined. Stimuli were constructed using an ideal time-frequency ( T - F ) masking technique that ensures that the target is stronger than the masker in certain T - F regions of the mixture, thereby rendering certain regions easier to glimpse than others. Sentences were synthesized using this technique with glimpse information placed in several frequency regions while varying the glimpse window duration and total duration of glimpsing. Results indicated that the frequency location and total duration of the glimpses had a significant effect on speech recognition, with the highest performance obtained when the listeners were able to glimpse information in the F 1 ∕ F 2 frequency region ( 0 – 3 {kHz} ) for at least 60\% of the utterance.},
    author = {Li, Ning and Loizou, Philipos C.},
    citeulike-article-id = {12907261},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.2749454},
    day = {01},
    doi = {10.1121/1.2749454},
    issn = {0001-4966},
    journal = jasa,
    keywords = {glimpsing, speech},
    month = aug,
    number = {2},
    pages = {1165--1172},
    posted-at = {2014-01-12 03:19:42},
    priority = {4},
    title = {Factors influencing glimpsing of speech in noise},
    url = {http://dx.doi.org/10.1121/1.2749454},
    volume = {122},
    year = {2007}
}

@article{ApouxAndHealy2009,
    abstract = {The number of auditory filter outputs required to identify phonemes was estimated in two experiments. Stimuli were divided into 30 contiguous equivalent rectangular bandwidths ({ERB} N ) spanning 80–7563 Hz. Normal-hearing listeners were presented with limited numbers of bands having frequency locations determined randomly from trial to trial to provide a general view, i.e., irrespective of specific band location, of the number of {1-ERB} N -wide speech bands needed to identify phonemes. The first experiment demonstrated that 20 such bands are required to accurately identify vowels, and 16 are required to identify consonants. In the second experiment, speech-shaped noise or time-reversed speech was introduced to the non-speech bands at various signal-to-noise ratios. Considerably elevated noise levels were necessary to substantially affect phoneme recognition, confirming a high degree of channel independence in the auditory system. The independence observed between auditory filter outputs supports current views of speech recognition in noise in which listeners extract and combine pieces of information randomly distributed both in time and frequency. These findings also suggest that the ability to partition incoming sounds into a large number of narrow bands, an ability often lost in cases of hearing impairment or cochlear implantation, is critical for speech recognition in noise.},
    author = {Apoux, Fr\'{e}d\'{e}ric and Healy, Eric W.},
    citeulike-article-id = {5314500},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.heares.2009.06.005},
    day = {16},
    doi = {10.1016/j.heares.2009.06.005},
    issn = {03785955},
    journal = {Hearing Research},
    keywords = {glimpsing, intelligibility, speech},
    month = sep,
    number = {1-2},
    pages = {99--108},
    posted-at = {2014-01-12 03:28:37},
    priority = {2},
    title = {On the number of auditory filter outputs needed to understand speech: Further evidence for auditory channel independence},
    url = {http://dx.doi.org/10.1016/j.heares.2009.06.005},
    volume = {255},
    year = {2009}
}

@inproceedings{VincentEtAl2013,
	Abstract = {Distant-microphone automatic speech recognition (ASR) remains a challenging goal in everyday environments involving multiple background sources and reverberation. This paper is intended to be a reference on the 2nd 'CHiME' Challenge, an initiative designed to analyze and evaluate the performance of ASR systems in a real-world domestic environment. Two separate tracks have been proposed: a small-vocabulary task with small speaker movements and a medium-vocabulary task without speaker movements. We discuss the rationale for the challenge and provide a detailed description of the datasets, tasks and baseline performance results for each track.},
	Author = {Vincent, Emmanuel and Barker, Jon and Watanabe, Shinji and Le Roux, Jonathan and Nesta, Francesco and Matassoni, Marco},
	Booktitle = icassp,
	Month = May,
	Pages = {126-130},
	Title = {The second `{CHiME}' Speech Separation and Recognition Challenge: Datasets, tasks and baselines},
	Year = {2013}}

@article{SmiljanicAndBradlow2009,
    abstract = {This article provides an overview of the research concerning the nature of the distinct, listener-oriented speaking style called 'clear speech' and its effect on intelligibility for various listener populations. We review major findings that identify talker, listener and signal characteristics that contribute to the characteristically high intelligibility of clear speech. Understanding the interplay of these factors sheds light on the interaction between higher level cognitive and lower-level sensory and perceptual factors that affect language processing. Clear speech research is, thus, relevant for both its theoretical insights and practical applications. Throughout the review, we highlight open questions and promising future directions.},
    author = {Smiljani\'{c}, Rajka and Bradlow, Ann R.},
    citeulike-article-id = {6581248},
    citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1749-818x.2008.00112.x},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/20046964},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=20046964},
    day = {1},
    doi = {10.1111/j.1749-818x.2008.00112.x},
    issn = {1749-818X},
    journal = {Language and Linguistics Compass},
    keywords = {clearspeech, phonetics, speech},
    month = jan,
    number = {1},
    pages = {236--264},
    pmid = {20046964},
    posted-at = {2014-01-12 03:54:26},
    priority = {2},
    publisher = {Blackwell Publishing Ltd},
    title = {Speaking and Hearing Clearly: Talker and Listener Factors in Speaking Style Changes},
    url = {http://dx.doi.org/10.1111/j.1749-818x.2008.00112.x},
    volume = {3},
    year = {2009}
}
@article{Junqua1993,
    abstract = {Automatic speech recognitionexperiments show that, depending on the task performed and how speech variability is modeled, automatic speech recognizers are more or less sensitive to the Lombard reflex. To gain an understanding about the Lombard effect with the prospect of improving performance of automatic speech recognizers, (1) an analysis was made of the acoustic‐phonetic changes occurring in Lombard speech, and (2) the influence of the Lombard effect on speech perception was studied. Both acoustic and perceptual analyses suggest that the influence of the Lombard effect on male and female speakers is different. The analyses also bring to light that, even if some tendencies across speakers can be observed consistently, the Lombard reflex is highly variable from speaker to speaker. Based on the results of the acoustic and perceptual studies, some ways of dealing with Lombard speech variability in automatic speech recognition are also discussed.},
    author = {Junqua, Jean‐Claude},
    citeulike-article-id = {10393317},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.405631},
    day = {01},
    doi = {10.1121/1.405631},
    issn = {0001-4966},
    journal = jasa,
    keywords = {clearspeech, lombard, phonetics, speech, speechproduction},
    month = jan,
    number = {1},
    pages = {510--524},
    posted-at = {2014-01-12 04:03:20},
    priority = {2},
    publisher = {\$abstract.copyright\_name.value},
    title = {The Lombard reflex and its role on human listeners and automatic speech recognizers},
    url = {http://dx.doi.org/10.1121/1.405631},
    volume = {93},
    year = {1993}
}

@article{RogersEtAl2010,
    abstract = {The ability of native and non-native speakers to enhance intelligibility of target vowels by speaking clearly was compared across three talker groups: monolingual English speakers and native Spanish speakers with either an earlier or a later age of immersion in an English-speaking environment. Talkers produced the target syllables  ” bead, bid, bayed, bed, bad” and  ” bod” in 'conversational' and clear speech styles. The stimuli were presented to native English-speaking listeners in multi-talker babble with signal-to-noise ratios of −8 {dB} for the monolingual and early learners and −4 {dB} for the later learners. The monolinguals and early learners of English showed a similar average clear speech benefit, and the early learners showed equal or greater intelligibility than monolinguals for most target vowels. The {4-dB} difference in signal-to-noise ratio yielded approximately equal average intelligibility for the monolinguals and later learners. The average clear speech benefit was smallest for the later learners, and a significant clear speechdecrement was obtained for the target syllable  ” bid.” These results suggest that later learners of English as a second language may be less able than monolinguals to accommodate listeners in noisy environments, due to a reduced ability to improve intelligibility by speaking more clearly.},
    author = {Rogers, Catherine L. and DeMasi, Teresa M. and Krause, Jean C.},
    citeulike-article-id = {12907285},
    citeulike-linkout-0 = {http://dx.doi.org/10.1121/1.3436523},
    day = {16},
    doi = {10.1121/1.3436523},
    issn = {0001-4966},
    journal = jasa,
    keywords = {clearspeech, speech, speechproduction},
    month = jul,
    number = {1},
    pages = {410--423},
    posted-at = {2014-01-12 04:21:49},
    priority = {2},
    title = {Conversational and clear speech intelligibility of /{bVd}/ syllables produced by native and non-native English speakersa)},
    url = {http://dx.doi.org/10.1121/1.3436523},
    volume = {128},
    year = {2010}
}

@incollection{Beckman1997,
    abstract = {Building accurate computational models of the prosody of spontaneous speech is a daunting enterprise because speech produced without a carefully devised written script does not readily allow the explicit control and repeated observation that read  ” lab speech” corpora are designed to provide. The prosody of spontaneous speech is affected profoundly by the social and rhetorical context of the recording, and these contextual factors can themselves vary widely in ways beyond our current understanding and control, so that there are many types of spontaneous speech which differ substantially not just from lab speech but also from each other. This paper motivates the study of spontaneous speech by describing several important aspects of prosody and its function that cannot be studied fully in lab speech, either because the relevant phenomena do not occur at all in lab speech or occur in a limited range of types. It then lists and characterizes some kinds of spontaneous speech that have been successfully recorded and analysed by scientists working on some of these aspects of prosody or on related discourse phenomena.},
    author = {Beckman, MaryE},
    booktitle = {Computing Prosody},
    citeulike-article-id = {12907286},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-1-4612-2258-3\_2},
    citeulike-linkout-1 = {http://link.springer.com/chapter/10.1007/978-1-4612-2258-3\_2},
    doi = {10.1007/978-1-4612-2258-3\_2},
    editor = {Sagisaka, Yoshinori and Campbell, Nick and Higuchi, Norio},
    keywords = {prosody, speech, speechproduction},
    pages = {7--26},
    posted-at = {2014-01-12 04:29:11},
    priority = {2},
    publisher = {Springer US},
    title = {A Typology of Spontaneous Speech},
    url = {http://dx.doi.org/10.1007/978-1-4612-2258-3\_2},
    year = {1997}
}

@article{PanAndYang2010,
    abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
    author = {Pan, Sinno J. and Yang, Qiang},
    journal = {IEEE Transactions on Knowledge and Data Engineering},
    month = oct,
    number = {10},
    pages = {1345--1359},
    title = {A Survey on Transfer Learning},
    volume = {22},
    year = {2010}
}

@article{AlcantaraEtAl2003,
    abstract = {We evaluated the effectiveness of a noise reduction system implemented in a commercial digital multichannel compression hearing aid. Eight experienced hearing aid wearers with moderate sensorineural hearing loss were fitted bilaterally according to the manufacturer's fitting guidelines. After a 3-month period of regular use of two programs, one with and one without the noise reduction system, speech recognition thresholds ({SRTs}) were measured in four types of background noise, including steady noise, and noises with spectral and/or temporal dips. {SRTs} were very similar with and without the noise reduction system; in both cases, {SRTs} were markedly lower than for unaided listening. {SRTs} were lower for the noises with dips than for the steady noise, especially for the aided conditions, indicating that amplification can help to 'listen in the dips'. Ratings of sound quality and listening comfort in the aided conditions were uniformly high and very similar with and without the noise reduction system. Sumario: Evaluamos la efectividad del sistema de reducci\'{o}n de ruido en un auxiliar auditivo digital comercial con compresi\'{o}n multicanal. Se adaptaron auxi-liares auditivos bilateralmente a ocho usuarios experimentados con hipoacusias sensorineurales moderadas de acuerdo con las guias de adaptaci\'{o}n del fabricante. Despu\'{e}s de un periodo de 3 meses de uso regular de dos programas, uno con y uno sin el sistema de reducci\'{o}n de ruido, se midieron los umbrales de reconocimiento del lenguaje ({SRT}) con cuatro tipos de ruido de fondo, incluyendo ruido estacionario y ruidos con componentes espectrales y/o temporales. Los {SRT} fueron muy similares con y sin el sistema de reducci\'{o}n de ruido. En ambos casos, los {SRT} fueron marca-damente m\'{a}s bajos que en condiciones de escucha sin el auxiliar. Los {SRT} fueron m\'{a}s bajos con los ruidos que tenian componentes, en comparaci\'{o}n con el ruido estacionario, especialmente en condiciones de uso del auxiliar, lo que indic\'{o} que la amplificaci\'{o}n puede ayudar a escuchar en presencia de esos componentes. Los resultados en cuanto a calidad de sonido y de audici\'{o}n agradablc en condiciones de uso del auxiliar auditivo, fueron uniformemente elevados y muy similares con y sin el sistema de reducci\'{o}n de ruido.},
    author = {Alc\'{a}ntara, Jos\'{e} I. and Moore, Brian C. J. and K\"{u}hnel, Volker and Launer, Stefan},
    booktitle = {International Journal of Audiology},
    citeulike-article-id = {11288858},
    citeulike-linkout-0 = {http://dx.doi.org/10.3109/14992020309056083},
    citeulike-linkout-1 = {http://www.informahealthcare.com/doi/abs/10.3109/14992020309056083},
    day = {1},
    doi = {10.3109/14992020309056083},
    journal = {Int J Audiol},
    keywords = {hearingaids, hearingimpaired, psych, psychoacoustics},
    month = jan,
    number = {1},
    pages = {34--42},
    posted-at = {2012-09-21 22:25:57},
    priority = {2},
    publisher = {Informa Allied Health},
    title = {Evaluation of the noise reduction system in a commercial digital hearing aid: Evaluaci\'{o}n del sistema de reducci\'{o}n de ruido en un auxiliar auditivo digital comercial},
    url = {http://dx.doi.org/10.3109/14992020309056083},
    volume = {42},
    year = {2003}
}

@inproceedings{LiaoEtAl2013,
    author = {Liao, Hank and McDermott, Erik and Senior, Andrew},
    booktitle = {Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on},
    citeulike-article-id = {12908501},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/asru.2013.6707758},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6707758},
    doi = {10.1109/asru.2013.6707758},
    institution = {Google Inc},
    keywords = {asr, deeplearning, youtube},
    month = dec,
    pages = {368--373},
    posted-at = {2014-01-13 20:13:50},
    priority = {0},
    publisher = {IEEE},
    title = {Large scale deep neural network acoustic modeling with semi-supervised training data for {YouTube} video transcription},
    url = {http://dx.doi.org/10.1109/asru.2013.6707758},
    year = {2013}
}

@inproceedings{livescu2003hidden,
  title={Hidden feature models for speech recognition using dynamic Bayesian networks.},
  author={Livescu, Karen and Glass, James R and Bilmes, Jeff},
  booktitle=interspeech,
  year={2003},
  organization={Citeseer}
}

@article{Stevens2002,
    abstract = {This article describes a model in which the acoustic speech signal is processed to yield a discrete representation of the speech stream in terms of a sequence of segments, each of which is described by a set (or bundle) of binary distinctive features. These distinctive features specify the phonemic contrasts that are used in the language, such that a change in the value of a feature can potentially generate a new word. This model is a part of a more general model that derives a word sequence from this feature representation, the words being represented in a lexicon by sequences of feature bundles. The processing of the signal proceeds in three steps: (1) Detection of peaks, valleys, and discontinuities in particular frequency ranges of the signal leads to identification of acoustic landmarks. The type of landmark provides evidence for a subset of distinctive features called articulator-free features (e.g., [vowel], [consonant], [continuant]). (2) Acoustic parameters are derived from the signal near the landmarks to provide evidence for the actions of particular articulators, and acoustic cues are extracted by sampling selected attributes of these parameters in these regions. The selection of cues that are extracted depends on the type of landmark and on the environment in which it occurs. (3) The cues obtained in step (2) are combined, taking context into account, to provide estimates of \&quot;articulator-bound\&quot; features associated with each landmark (e.g., [lips], [high], [nasal]). These articulator-bound features, combined with the articulator-free features in (1), constitute the sequence of feature bundles that forms the output of the model. Examples of cues that are used, and justification for this selection, are given, as well as examples of the process of inferring the underlying features for a segment when there is variability in the signal due to enhancement gestures (recruited by a speaker to make a contrast more salient) or due to overlap of gestures from neighboring segments. \&copy;2002 Acoustical Society of America.},
    author = {Stevens, Kenneth N.},
    citeulike-article-id = {2747903},
    citeulike-linkout-0 = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=JASMAN000111000004001872000001\&idtype=cvips\&gifs=yes},
    citeulike-linkout-1 = {http://link.aip.org/link/?JAS/111/1872},
    citeulike-linkout-2 = {http://dx.doi.org/10.1121/1.1458026},
    doi = {10.1121/1.1458026},
    issn = {00014966},
    journal = jasa,
    keywords = {acousticphonetics, speech},
    number = {4},
    pages = {1872--1891},
    posted-at = {2013-12-13 16:08:42},
    priority = {4},
    publisher = {ASA},
    title = {Toward a model for lexical access based on acoustic landmarks and distinctive features},
    url = {http://dx.doi.org/10.1121/1.1458026},
    volume = {111},
    year = {2002}
}

@article{picheny1986speaking,
  title={Speaking clearly for the hard of hearing II: Acoustic characteristics of clear and conversational speech},
  author={Picheny, Michael A and Durlach, Nathaniel I and Braida, Louis D},
  journal={Journal of Speech, Language and Hearing Research},
  volume={29},
  number={4},
  pages={434},
  year={1986},
  publisher={ASHA}
}

@article{gay1978effect,
  title={Effect of speaking rate on vowel formant movements},
  author={Gay, Thomas},
  journal=jasa,
  volume={63},
  pages={223},
  year={1978}
}

@inproceedings{neti1997word,
  title={Word-based confidence measures as a guide for stack search in speech recognition},
  author={Neti, Chalapathy V and Roukos, Salim and Eide, E},
  booktitle={Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on},
  volume={2},
  pages={883--886},
  year={1997},
  organization={IEEE}
}

@Inproceedings {HuangEtAl2014,
abstract     = {<p>We propose a multi-accent deep neural network acoustic model with an
                accent-specific top layer and shared bottom hidden layers. The accent-specific
                top layer is used to model the distinct accent specific patterns. The shared
                bottom hidden layers allow maximum knowledge sharing between the native and the
                accent models. This design is particularly attractive when considering deploying
                such a system to a live speech service due to its computational efficiency. We
                applied the KL-divergence (KLD) regularized model adaptation to train the
                accent-specific top layer. On the mobile short message dictation task (SMD), with
                1K, 10K, and 100K British or Indian accent adaptation utterances, the proposed
                approach achieves 18.1{\%}, 26.0{\%}, and 28.5{\%} or 16.1{\%}, 25.4{\%}, and 30.6{\%} word error
                rate reduction (WERR) for the British and the Indian accent respectively against
                a baseline cross entropy (CE) model trained from 400 hour data. On the 100K
                utterance accent adaptation setup, comparable performance gain can be obtained
                against a baseline CE model trained with 2000 hour data. We observe smaller yet
                significant WER reduction on a baseline model trained using the MMI
                sequence-level criterion.</p>},
author       = {Yan Huang and Dong Yu and Chaojun Liu and Yifan Gong},
booktitle    = {Interspeech 2014},
month        = {September},
title        = {Multi-Accent Deep Neural Network Acoustic Model with Accent-Specific Top Layer
                Using the KLD-Regularized Model Adaptation},
url          = {http://research.microsoft.com/apps/pubs/default.aspx?id=230139},
year         = {2014},
}

 @phdthesis{Chase1997,
   author = "Lin Chase",
   title = "Error-Responsive Feedback Mechanisms for Speech Recognizers",
   booktitle = "",
   school = "Robotics Institute, Carnegie Mellon University",
   month = "April",
   year = "1997",
   number= "CMU-RI-TR-97-18",
   address= "Pittsburgh, PA",
}

@article{TheunissenEtAl2000,
abstract = {The stimulus-response function of many visual and auditory neurons has been described by a spatial-temporal receptive field (STRF), a linear model that for mathematical reasons has until recently been estimated with the reverse correlation method, using simple stimulus ensembles such as white noise. Such stimuli, however, often do not effectively activate high-level sensory neurons, which may be optimized to analyze natural sounds and images. We show that it is possible to overcome the simple-stimulus limitation and then use this approach to calculate the STRFs of avian auditory forebrain neurons from an ensemble of birdsongs. We find that in many cases the STRFs derived using natural sounds are strikingly different from the STRFs that we obtained using an ensemble of random tone pips. When we compare these two models by assessing their predictions of neural response to the actual data, we find that the STRFs obtained from natural sounds are superior. Our results show that the STRF model is an incomplete description of response properties of nonlinear auditory neurons, but that linear receptive fields are still useful models for understanding higher level sensory processing, as long as the STRFs are estimated from the responses to relevant complex stimuli.
},
author = {Theunissen, Frederic E. and Sen, Kamal and Doupe, Allison J.},
journal = {J. Neurosci.},
keywords = {birdSong,reverseCorrelation,strf},
mendeley-tags = {birdSong,reverseCorrelation,strf},
month = mar,
number = {6},
pages = {2315--2331},
title = {Spectral-Temporal Receptive Fields of Nonlinear Auditory Neurons Obtained Using Natural Sounds},
url = {http://www.jneurosci.org/content/20/6/2315.abstract},
volume = {20},
year = {2000}
}

@article{DeBoerAndDeJongh1978,
abstract = {This paper presents a description of the interrelation between two major properties of the responses recordable from auditory nerve fibers:f r e q u e n c y s e l e c t i v i t y and p a r t i a l s y n c h r o n y between stimulus and response. In the course of this work the influence of nonlinearity on the cochlear encoding process can be assessed. The theory of the r e v e r s e‐c o r r e l a t i o n t e c h n i q u e is derived in a most general way. It is based on a model in which a filter—assumed to be linear—is followed by a stochastic pulse generator—the probability of producing an output pulse being an instantaneous but nonlinear function of its input signal. Insofar as such a model represents stimulus transformations in a primary auditory neuron, the technique can be applied to the responses recorded from an auditory nerve fiber. Several illustrative examples of experimental reverse‐correlation functions−abbreviated: r e v c o r f u n c t i o n s—are presented and discussed. These functions have the general character of impulse responses of sharp bandpass filters. They show very little phase modulation. For noise stimuli of up to 70 dB per third octave the revcor functions are almost invariant. Above that level some (but not all) of the revcor functions show a loss of frequency selectivity. If a nerve fiber can be contacted for a sufficiently long time, it is possible to compare the response with that of a model filter, in which the revcor function of that fiber is substituted as its impulse response. The output signal of the model filter is shown to be a very good predictor of the firing probability of the fiber under study. This property is demonstrated for noise as well as for tone stimuli. There is surprisingly little evidence of nonlinear filtering in these results. This so‐called simulation method can also be applied when the stimulus is switched on and off. The results show, apart from effects due to filtering, clear manifestations of fast adaptation. Again, the filtering appears to be independent of the latter effect. It is concluded that for wide‐band noise and single‐tone signals the firing probability is predominantly controlled by a linearly filtered version of the acoustical stimulus; this constitutes the principle of s p e c i f i c c o d i n g. The conspicuous absence of nonlinear effects in the results can partly be explained in terms of the response properties of a class of networks in which sharp filtering occurs after the generation of nonlinear distortion products. It can then be predicted that this property will hold only for wide‐band and tonal stimuli. That our results show so little evidence of cochlear distortion appears to be a property of signal transformations and is not due to linearization tendencies of the experimental method.},
author = {de Boer, E.},
doi = {10.1121/1.381704},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
keywords = {auditory,neural,reverse\_correlation,strf},
mendeley-tags = {auditory,neural,reverse\_correlation,strf},
month = jan,
number = {1},
pages = {115},
publisher = {Acoustical Society of America},
title = {{On cochlear encoding: Potentialities and limitations of the reverse-correlation technique}},
url = {http://scitation.aip.org/content/asa/journal/jasa/63/1/10.1121/1.381704},
volume = {63},
year = {1978}
}

@ARTICLE{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, R.},
  journal={Machine Learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997}
}

@inproceedings{tur2006multitask,
  title={Multitask learning for spoken language understanding},
  author={Tur, G.},
  booktitle=icassp,
  pages={585--588},
  year={2006}
}

@INPROCEEDINGS{Seltzer13Multitask,
  author = {Seltzer, M. and Droppo, J.},
  title = {Multi-task learning in deep neural networks for improved phoneme recognition},
  booktitle = icassp,
  year = {2013},
  pages = {6965--6969}
}

@article{NarayananAndWang2014,
abstract = {Recently, supervised classification has been shown to work well for the task of speech separation. We perform an in-depth evaluation of such techniques as a front-end for noise-robust automatic speech recognition (ASR). The proposed separation front-end consists of two stages. The first stage removes additive noise via time-frequency masking. The second stage addresses channel mismatch and the distortions introduced by the first stage; a non-linear function is learned that maps the masked spectral features to their clean counterpart. Results show that the proposed front-end substantially improves ASR performance when the acoustic models are trained in clean conditions. We also propose a diagonal feature discriminant linear regression (dFDLR) adaptation that can be performed on a per-utterance basis for ASR systems employing deep neural networks and HMM. Results show that dFDLR consistently improves performance in all test conditions. Surprisingly, the best average results are obtained when dFDLR is applied to models trained using noisy log-Mel spectral features from the multi-condition training set. With no channel mismatch, the best results are obtained when the proposed speech separation front-end is used along with multi-condition training using log-Mel features followed by dFDLR adaptation. Both these results are among the best on the Aurora-4 dataset.},
author = {Narayanan, Arun and Wang, DeLiang},
doi = {10.1109/TASLP.2014.2305833},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
keywords = {ASR systems,Acoustics,Adaptation models,Aurora-4,Aurora-4 dataset,Feature extraction,HMM,Noise,Speech,Speech processing,Training,acoustic models,additive noise removal,asr,channel mismatch,dFDLR adaptation,deep neural networks,diagonal feature discriminant linear regression ad,distortion,distortions,feature mapping,hidden Markov models,irm,masked spectral features,masking,multicondition training set,neural nets,noise robust speech recognition,noise\_robust,noisy log-Mel spectral features,nonlinear function,nonlinear functions,regression analysis,robust ASR,signal classification,signal denoising,speech recognition,speech separation front-end,supervised classification,time-frequency masking},
mendeley-tags = {asr,irm,masking,noise\_robust},
month = apr,
number = {4},
pages = {826--835},
shorttitle = {Audio, Speech, and Language Processing, IEEE/ACM T},
title = {{Investigation of Speech Separation as a Front-End for Noise Robust Speech Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6739096},
volume = {22},
year = {2014}
}

@phdthesis{Ellis1996,
author = {Ellis, Daniel P. W.},
keywords = {casa,separation,source{\_}separation},
mendeley-tags = {casa,separation,source{\_}separation},
month = {jan},
school = {Massachusetts Institute of Technology},
title = {Prediction-driven computational auditory scene analysis},
url = {http://dl.acm.org/citation.cfm?id=924513},
year = {1996}
}

@phdthesis{Liao2007,
	Author = {H. Liao},
	Owner = {arun},
	School = {University of Cambridge},
	Timestamp = {2011.03.03},
	Title = {Uncertainty Decoding for Noise Robust Speech Recognition},
	Year = {2007}}


@article{DengEtAl2005,
	Author = {Li Deng and Jasha Droppo and Alex Acero},
	Date-Modified = {2010-09-15 10:23:04 -0400},
	Journal = tsap,
	Pages = {412-421},
	Title = {Dynamic compensation of {HMM} variances using the feature enhancement uncertainty computed from a parametric model of speech distortion},
	Volume = {13},
	Year = {2005}}

@inproceedings{SeltzerEtAl2013,
	Author = {M. Seltzer and D. Yu and Y. Wang},
	Booktitle = icassp,
	Date-Added = {2013-11-11 04:50:43 +0000},
	Date-Modified = {2013-11-11 04:50:43 +0000},
	Title = {An Investigation Of Deep Neural Networks For Noise Robust Speech Recognition},
	Year = {2013}}

@article{HartmannEtal2013,
	Author = {W. Hartmann and A. Narayanan and E. Fosler-Lussier and D. L. Wang},
	Date-Added = {2013-11-11 04:57:13 +0000},
	Date-Modified = {2013-11-11 04:57:13 +0000},
	Journal = taslp,
	Number = {10},
	Pages = {1993-2005},
	Title = {A Direct Masking Approach to Robust {ASR}},
	Volume = {21},
	Year = {2013}}

@inproceedings{YoshiokaEtAl2014,
author = {Yoshioka, Takuya and Chen, Xie and Gales, Mark J. F.},
booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2014.6854660},
isbn = {978-1-4799-2893-4},
keywords = {DNN-based meeting transcription systems,Environmental robustness,Hidden Markov models,Reverberation,Silicon,Speech,Speech processing,Vectors,acoustic model structure,acoustic signal processing,asr,automatic speech recognition systems,classical acoustic model,deep neural network,deep neural networks,dereverb,dereverberation processing,environmental distortion,farfield,feature vectors,front-end techniques,meeting,meeting transcription,microphones,neural nets,recognition performance,reverberation,single distant microphone,single-microphone dereverberation,speaker recognition,speaker-adaptive DNN-based systems,speaker-independent DNN-based systems},
language = {English},
mendeley-tags = {asr,dereverb,farfield,meeting},
month = may,
pages = {5527--5531},
publisher = {IEEE},
title = {{Impact of single-microphone dereverberation on DNN-based meeting transcription systems}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6854660},
year = {2014}
}

@techreport{MorganEtAl2013,
author = {Morgan, Nelson and Wegmann, Steven and Cohen, Jordan},
month = mar,
title = {What's Wrong With Automatic Speech Recognition ({ASR}) and How Can We Fix It?},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord\&metadataPrefix=html\&identifier=ADA590075},
year = {2013},
number = {AFRL-RH-WP-TR-2013-0061},
organization = {Air Force Research Laboratory},
}

@inproceedings{EideEtAl1995,
author = {Eide, E. and Gish, H. and Jeanrenaud, P. and Mielke, A.},
booktitle = icassp,
pages = {221--224},
title = {Understanding and improving speech recognition performance through the use of diagnostic tools},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=479404},
volume = {1},
year = 1995,
}

@inproceedings{GillickEtAl2012,
author = {Gillick, Dan and Wegmann, Steven and Gillick, Larry},
booktitle = icassp,
doi = {10.1109/ICASSP.2012.6288979},
isbn = {978-1-4673-0046-9},
issn = {1520-6149},
language = {English},
mendeley-tags = {asr,diagnostic,discriminative},
month = mar,
pages = {4745--4748},
publisher = {IEEE},
title = {Discriminative training for speech recognition is compensating for statistical dependence in the {HMM} framework},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6288979},
year = {2012}
}
@inproceedings{ParthasarathiEtAl2013,
author = {Parthasarathi, Sree Hari Krishnan and Chang, Shuo-Yiin and Cohen, Jordan and Morgan, Nelson and Wegmann, Steven},
booktitle = icassp,
doi = {10.1109/ICASSP.2013.6638970},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
language = {English},
mendeley-tags = {asr,diagnostics,reverb},
month = may,
pages = {6758--6762},
publisher = {IEEE},
title = {{The blame game in meeting room ASR: An analysis of feature versus model errors in noisy and mismatched conditions}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6638970},
year = {2013}
}
@inproceedings{GillickEtAl2011,
author = {Gillick, Dan and Gillick, Larry and Wegmann, Steven},
booktitle = asru,
doi = {10.1109/ASRU.2011.6163908},
isbn = {978-1-4673-0367-5},
language = {English},
mendeley-tags = {asr,diagnostics,switchboard},
month = dec,
pages = {71--76},
publisher = {IEEE},
title = {Don't multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6163908},
year = {2011}
}

@article{EcksteinAndAhumada2002,
author = {Eckstein, Miguel P and Ahumada, Albert J},
doi = {10:1167/2.1.i},
issn = {1534-7362},
journal = {Journal of vision},
keywords = {Classification,Humans,Models, Statistical,Visual Perception,Visual Perception: physiology,classification\_image,preface,strategy},
mendeley-tags = {classification\_image,preface,strategy},
month = jan,
number = {1},
pages = {1x},
pmid = {12678601},
title = {Classification images: a tool to analyze visual strategies.},
url = {http://www.journalofvision.org/content/2/1/i.short},
volume = {2},
year = {2002}
}

@incollection{ZeilerAndFergus2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Computer Vision – ECCV 2014},
doi = {10.1007/978-3-319-10590-1},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10589-5},
keywords = {convolutional\_networks,diagnostics,vision,visualization},
mendeley-tags = {convolutional\_networks,diagnostics,vision,visualization},
pages = {818--833},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {Visualizing and Understanding Convolutional Networks},
url = {http://link.springer.com/10.1007/978-3-319-10590-1},
volume = {8689},
year = {2014}
}

@InProceedings{SimonyanEtAl2014,
  author       = "Simonyan, K. and Vedaldi, A. and Zisserman, A.",
  title        = "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
  booktitle    = "Workshop at International Conference on Learning Representations",
  year         = "2014",
}

@inproceedings{CollobertAndWeston2008,
address = {New York, New York, USA},
author = {Collobert, Ronan and Weston, Jason},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390177},
isbn = {9781605582054},
keywords = {multitask,nlp},
mendeley-tags = {multitask,nlp},
month = jul,
pages = {160--167},
publisher = {ACM Press},
title = {A unified architecture for natural language processing},
url = {http://dl.acm.org/citation.cfm?id=1390156.1390177},
year = {2008}
}

@article{Jiang2005,
abstract = {In speech recognition, confidence measures (CM) are used to evaluate reliability of recognition results. A good confidence measure can largely benefit speech recognition systems in many practical applications. In this survey, I summarize most research works related to confidence measures which have been done during the past 10–12 years. I will present all these approaches as three major categories, namely CM as a combination of predictor features, CM as a posterior probability, and CM as utterance verification. Then, I also introduce some recent advances in the area. Moreover, I will discuss capabilities and limitations of the current CM techniques and generally comment on today’s CM approaches. Based on the discussion, I will conclude the paper with some clues for future works.},
author = {Jiang, Hui},
doi = {10.1016/j.specom.2004.12.004},
issn = {01676393},
journal = {Speech Communication},
keywords = {Automatic speech recognition (ASR),Bayes factors,Confidence measures (CM),Likelihood ratio testing (LRT),Utterance verification,Word posterior probability,asr,confidence,review},
mendeley-tags = {asr,confidence,review},
month = apr,
number = {4},
pages = {455--470},
title = {Confidence measures for speech recognition: A survey},
url = {http://www.sciencedirect.com/science/article/pii/S0167639305000051},
volume = {45},
year = {2005}
}

@book{BourlardAndMorgan1994,
abstract = {Connectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state of the art continuous speech recognition systems based on hidden Markov models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e. HMM emission probability estimation and feature extraction.  The book describes a successful five-year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical systems.  Using standard databases and comparison with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods.  Connectionist Speech Recognition is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. The book is also suitable as a text for advanced courses on neural networks or speech processing.},
author = {Bourlard, Herv\'{e} A. and Morgan, Nelson},
isbn = {0792393961},
keywords = {asr,hybrid\_mlp\_asr},
mendeley-tags = {asr,hybrid\_mlp\_asr},
pages = {312},
publisher = {Springer Science \& Business Media},
title = {Connectionist Speech Recognition: A Hybrid Approach},
url = {http://books.google.com/books/about/Connectionist\_Speech\_Recognition.html?id=7vm\_8bPHCEUC\&pgis=1},
year = {1994}
}

@book{Everitt1992,
abstract = {Much of the data collected in medicine and the social sciences is categorical, for example, sex, marital status, blood group, whether a smoker or not and so on, rather than interval-scaled. Frequently the researcher collecting such data is interested in the relationships or associations between pairs, or between a set of such categorical variables; often the data is displayed in the form of a contingency table for example, smoker versus non-smoker against death from lung cancer or death from some other cause. This text gives a comprehensive account of the analysis of such tables, written at a level suitable for the applied researcher. The first edition of "The Analysis of Contingency Tables" arose from Professor A.E. Maxwell's earlier text, "Analysing Qualitative Data". In this new edition, more material is included that those methods which have developed over the last decade or so, for example, logistic regression models for tables with ordered categories and for response variables with more than two categories. A brief account is given of the increasingly important technique, correspondence analysis. The methods of analysis described in this book should be relevant to research workers and graduate students dealing with data from surveys, particularly in the area of psychiatry, social sciences and psychology.},
author = {Everitt, Brian S.},
isbn = {0412398508},
keywords = {mcnemar,statistics},
mendeley-tags = {mcnemar,statistics},
pages = {168},
publisher = {CRC Press},
title = {The Analysis of Contingency Tables},
edition = {Second},
url = {http://books.google.com/books?hl=en\&lr=\&id=aSe1LbYz3v0C\&pgis=1},
year = {1992}
}

@phdthesis{Gemmeke2011,
author = {Gemmeke, Jort Florent},
isbn = {9090258787},
keywords = {asr,noise\_robust,nonparametric},
mendeley-tags = {asr,noise\_robust,nonparametric},
pages = {161},
publisher = {UB Nijmegen},
school = {Radboud Universiteit Nijmegen},
title = {Noise Robust {ASR}: Missing Data Techniques and Beyond},
type = {PhD},
url = {http://books.google.com/books/about/Noise\_Robust\_ASR.html?id=HmCykQEACAAJ\&pgis=1},
year = {2011}
}

@article{SainathEtAl2012,
author = {Sainath, Tara and Ramabhadran, Bhuvana and Nahamoo, David and Kanevsky, Dimitri and Compernolle, Dirk and Demuynck, Kris and Gemmeke, Jort and Bellegarda, Jerome and Sundaram, Shiva},
doi = {10.1109/MSP.2012.2208663},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {Acoustics,Automatic speech recognition,Computational modeling,Data models,Hidden Markov models,Learning systems,Machine learning,Speech recognition,Uncertainty,accurate inference,asr,data generation,exemplar based processing,nonparametric,random fluctuations,review,signal classification,speech production,speech recognition,survey,vocal tract variations},
language = {English},
mendeley-tags = {asr,nonparametric,review,survey},
month = nov,
number = {6},
pages = {98--113},
publisher = {IEEE},
title = {Exemplar-Based Processing for Speech Recognition: An Overview},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6296529},
volume = {29},
year = {2012}
}

@inproceedings{GodfreyEtAl1992,
author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
booktitle = icassp,
doi = {10.1109/ICASSP.1992.225858},
isbn = {0-7803-0532-9},
issn = {1520-6149},
language = {English},
mendeley-tags = {asr,corpus,dataset,switchboard},
pages = {517--520},
publisher = {IEEE},
title = {{SWITCHBOARD}: telephone speech corpus for research and development},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=225858},
volume = {1},
year = {1992}
}

@inproceedings{greenberg1996insights,
  title={Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus},
  author={Greenberg, Steven and Hollenback, Joy and Ellis, Dan},
  booktitle={International Conference on Spoken Language Processing},
  pages={S32--35},
  year={1996},
  organization={Citeseer}
}

@article{HochreiterAndSchmidhuber1997,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {lstm,neural\_net,rnn},
language = {English},
mendeley-tags = {lstm,neural\_net,rnn},
month = nov,
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {Long Short-Term Memory},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6795963},
volume = {9},
year = {1997}
}

@article{ZinielEtAl2014,
abstract = {For the problem of binary linear classification and feature selection, we propose algorithmic approaches to classifier design based on the generalized approximate message passing (GAMP) algorithm, recently proposed in the context of compressive sensing. We are particularly motivated by problems where the number of features greatly exceeds the number of training examples, but where only a few features suffice for accurate classification. We show that sum-product GAMP can be used to (approximately) minimize the classification error rate and max-sum GAMP can be used to minimize a wide variety of regularized loss functions. Furthermore, we describe an expectation-maximization (EM)-based scheme to learn the associated model parameters online, as an alternative to cross-validation, and we show that GAMP's state-evolution framework can be used to accurately predict the misclassification rate. Finally, we present a detailed numerical study to confirm the accuracy, speed, and flexibility afforded by our GAMP-based approaches to binary linear classification and feature selection.},
archivePrefix = {arXiv},
arxivId = {1401.0872},
author = {Ziniel, Justin and Schniter, Philip and Sederberg, Per},
eprint = {1401.0872},
file = {:home/mim/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ziniel, Schniter, Sederberg - 2014 - Binary Linear Classification and Feature Selection via Generalized Approximate Message Passing.pdf:pdf},
keywords = {feature\_selection,fmri,gamp,sparse,sparse\_coding},
mendeley-tags = {feature\_selection,fmri,gamp,sparse,sparse\_coding},
month = jan,
title = {Binary Linear Classification and Feature Selection via Generalized Approximate Message Passing},
url = {http://arxiv.org/abs/1401.0872},
year = {2014}
}

@inproceedings{Rangan2011,
author = {Rangan, Sundeep},
booktitle = {2011 IEEE International Symposium on Information Theory Proceedings},
doi = {10.1109/ISIT.2011.6033942},
isbn = {978-1-4577-0596-0},
issn = {2157-8095},
language = {English},
mendeley-tags = {gamp,sparse,sparse\_coding},
month = jul,
pages = {2168--2172},
publisher = {IEEE},
title = {Generalized approximate message passing for estimation with random linear mixing},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6033942},
year = {2011}
}

@incollection{CarlettaEtAl2006,
abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
address = {Berlin, Heidelberg},
author = {Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and Lisowska, Agnes and McCowan, Iain and Post, Wilfried and Reidsma, Dennis and Wellner, Pierre},
booktitle = {Machine Learning for Multimodal Interaction},
doi = {10.1007/11677482},
editor = {Renals, Steve and Bengio, Samy},
isbn = {978-3-540-32549-9},
keywords = {ami,asr,corpus,meeting},
mendeley-tags = {ami,asr,corpus,meeting},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {The {AMI} Meeting Corpus: A Pre-announcement},
url = {http://www.springerlink.com/index/10.1007/11677482},
volume = {3869},
year = {2006}
}

@article{HealyEtAl2013b,
abstract = {Despite considerable effort, monaural (single-microphone) algorithms capable of increasing the intelligibility of speech in noise have remained elusive. Successful development of such an algorithm is especially important for hearing-impaired (HI) listeners, given their particular difficulty in noisy backgrounds. In the current study, an algorithm based on binary masking was developed to separate speech from noise. Unlike the ideal binary mask, which requires prior knowledge of the premixed signals, the masks used to segregate speech from noise in the current study were estimated by training the algorithm on speech not used during testing. Sentences were mixed with speech-shaped noise and with babble at various signal-to-noise ratios (SNRs). Testing using normal-hearing and HI listeners indicated that intelligibility increased following processing in all conditions. These increases were larger for HI listeners, for the modulated background, and for the least-favorable SNRs. They were also often substantial, allowing several HI listeners to improve intelligibility from scores near zero to values above 70\%.},
author = {Healy, Eric W and Yoho, Sarah E and Wang, Yuxuan and Wang, DeLiang},
doi = {10.1121/1.4820893},
file = {::},
issn = {1520-8524},
journal = {The Journal of the Acoustical Society of America},
keywords = {Acoustic Stimulation,Adult,Aged,Algorithms,Audiometry, Pure-Tone,Audiometry, Speech,Auditory Threshold,Case-Control Studies,Correction of Hearing Impairment,Correction of Hearing Impairment: instrumentation,Equipment Design,Female,Hearing Aids,Hearing Loss, Sensorineural,Hearing Loss, Sensorineural: diagnosis,Hearing Loss, Sensorineural: psychology,Hearing Loss, Sensorineural: rehabilitation,Humans,Male,Middle Aged,Noise,Noise: adverse effects,Perceptual Masking,Persons With Hearing Impairments,Persons With Hearing Impairments: psychology,Persons With Hearing Impairments: rehabilitation,Recognition (Psychology),Signal Processing, Computer-Assisted,Sound Spectrography,Speech Intelligibility,Speech Perception,binary\_mask,dnn,hearing\_impaired,intelligibility},
mendeley-tags = {binary\_mask,dnn,hearing\_impaired,intelligibility},
month = oct,
number = {4},
pages = {3029--38},
pmid = {24116438},
publisher = {Acoustical Society of America},
title = {{An algorithm to improve speech recognition in noise for hearing-impaired listeners.}},
url = {http://scitation.aip.org/content/asa/journal/jasa/134/4/10.1121/1.4820893},
volume = {134},
year = {2013}
}

@article{SaonAndChien2012,
abstract = {Over the past decade or so, several advances have been made to the design of modern large vocabulary continuous speech recognition (LVCSR) systems to the point where their application has broadened from early speaker dependent dictation systems to speaker-independent automatic broadcast news transcription and indexing, lectures and meetings transcription, conversational telephone speech transcription, open-domain voice search, medical and legal speech recognition, and call center applications, to name a few. The commercial success of these systems is an impressive testimony to how far research in LVCSR has come, and the aim of this article is to describe some of the technological underpinnings of modern systems. It must be said, however, that, despite the commercial success and widespread adoption, the problem of large-vocabulary speech recognition is far from being solved: background noise, channel distortions, foreign accents, casual and disfluent speech, or unexpected topic change can cause automated systems to make egregious recognition errors. This is because current LVCSR systems are not robust to mismatched training and test conditions and cannot handle context as well as human listeners despite being trained on thousands of hours of speech and billions of words of text.},
author = {Saon, George and Chien, Jen-Tzung},
doi = {10.1109/MSP.2012.2197156},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {Acoustics,Adaptation models,Automatic speech recognition,Hidden Markov models,Speech recognition,Vocabularies,asr,background noise,call center application,casual speech,channel distortion,conversational telephone speech transcription,disfluent speech,foreign accent,large-vocabulary continuous speech recognition sys,lectures transcription,legal speech recognition,lvcsr,medical speech recognition,meetings transcription,open-domain voice search,review,speaker dependent dictation system,speaker recognition,speaker-independent automatic broadcast news index,speaker-independent automatic broadcast news trans,survey,vocabulary},
mendeley-tags = {asr,lvcsr,review,survey},
month = nov,
number = {6},
pages = {18--33},
shorttitle = {Signal Processing Magazine, IEEE},
title = {{Large-Vocabulary Continuous Speech Recognition Systems: A Look at Some Recent Advances}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6296522},
volume = {29},
year = {2012}
}

@article{LibermanEtAl1967,
abstract = {Man could not perceive speech well if each phoneme were cued by a unit sound. In fact, many phonemes are encoded so that a single acoustic cue carries information in parallel about successive phonemic segments. This reduces the rate at which discrete sounds must be perceived, but at the price of a complex relation between cue and phoneme: cues vary greatly with context, and there are, in these cases, no commutable acoustic segments of phonemic size. Phoneme perception therefore requires a special decoder. A possible model supposes that the encoding occurs below the level of the (invariant) neuromotor commands to the articulatory muscles. The decoder may then identify phonemes by referring the incoming speech sounds to those commands.},
author = {Liberman, A. M. and Cooper, F. S. and Shankweiler, D. P. and Studdert-Kennedy, M.},
journal = {Psychological Review},
keywords = {haskins,linguistics,phoneme,psych,speech,synthesis},
mendeley-tags = {haskins,linguistics,phoneme,psych,speech,synthesis},
number = {6},
pages = {431--461},
title = {Perception of the speech code.},
url = {http://psycnet.apa.orgjournals/rev/74/6/431},
volume = {74},
year = {1967}
}

@article{Akeroyd2008,
abstract = {This paper summarizes twenty studies, published since 1989, that have measured experimentally the relationship between speech recognition in noise and some aspect of cognition, using statistical techniques such as correlation or factor analysis. The results demonstrate that there is a link, but it is secondary to the predictive effects of hearing loss, and it is somewhat mixed across study. No one cognitive test always gave a significant result, but measures of working memory (especially reading span) were mostly effective, whereas measures of general ability, such as IQ, were mostly ineffective. Some of the studies included aided listening, and two reported the benefits from aided listening: again mixed results were found, and in some circumstances cognition was a useful predictor of hearing-aid benefit.},
author = {Akeroyd, Michael A},
doi = {10.1080/14992020802301142},
issn = {1708-8186},
journal = {International journal of audiology},
keywords = {Adult,Aged,Cognition,Correction of Hearing Impairment,Hearing Aids,Hearing Disorders,Hearing Disorders: physiopathology,Hearing Disorders: psychology,Hearing Disorders: therapy,Humans,Memory,Middle Aged,Perceptual Masking,Persons With Hearing Impairments,Persons With Hearing Impairments: psychology,Persons With Hearing Impairments: rehabilitation,Psychoacoustics,Reading,Speech Perception,Young Adult,congitive\_load,individual\_differences,meta,metastudy,speech\_cognition},
language = {en},
mendeley-tags = {congitive\_load,individual\_differences,meta,metastudy,speech\_cognition},
month = nov,
pages = {S53--71},
pmid = {19012113},
publisher = {Informa UK Ltd  UK},
title = {{Are individual differences in speech reception related to individual differences in cognitive ability? A survey of twenty experimental studies with normal and hearing-impaired adults.}},
url = {http://informahealthcare.com/doi/full/10.1080/14992020802301142},
volume = {47 Suppl 2},
year = {2008}
}

@article{AhumadaAndLovell1971,
abstract = {Short bursts of computer‐generated Gaussian noise were rated by observers for the presence or absence of a 500‐Hz signal tone burst. A multiple regression analysis found for each observer the linear combination of the energies in narrow bands around the tone frequency that best predicts his total ratings. The estimates of the regression coefficients provide graphs of the “frequency responses” of the observers. Most of the reliable variance in the total ratings was accounted for by the regression analysis in terms of energy in narrow bands. Differences among observers are explained in terms of differential weighting by observers of features labeled “tone presence,” “pitch,” and “loudness.”},
author = {Ahumada, Al and Lovell, John},
doi = {10.1121/1.1912577},
isbn = {doi:10.1121/1.1912577},
issn = {00014966},
journal = jasa,
keywords = {correlation,correlational\_method,psych,reverse\_correlation,tone\_in\_noise},
mendeley-tags = {correlation,correlational\_method,psych,reverse\_correlation,tone\_in\_noise},
month = aug,
number = {6B},
pages = {1751},
publisher = {Acoustical Society of America},
title = {{Stimulus Features in Signal Detection}},
url = {http://scitation.aip.org/content/asa/journal/jasa/49/6B/10.1121/1.1912577},
volume = {49},
year = {1971}
}

@article{JonesAndPalmer1987,
author = {Jones, J. P. and Palmer, L. A.},
journal = {J Neurophysiol},
keywords = {receptive\_field,reverse\_correlation,vision},
mendeley-tags = {receptive\_field,reverse\_correlation,vision},
month = dec,
number = {6},
pages = {1187--1211},
title = {{The two-dimensional spatial structure of simple receptive fields in cat striate cortex}},
url = {http://jn.physiology.org/content/58/6/1187.abstract?ijkey=449d8517daf235ee9df765a00610e4f383de233d\&keytype2=tf\_ipsecsha},
volume = {58},
year = {1987}
}

@inproceedings{Ahumada1996,
abstract = {Letting external noise rather than internal noise limit discrimination performance allows information to be extracted about the observer's stimulus classification rule. A perceptual classification image is the correlation over trials between the noise amplitude at a spatial location and the observer's responses. If, for example, the observer followed the rule of the ideal observer, the response correlation image would be an estimate of the ideal observer filter, the difference between the two unmasked images being discriminated. Perceptual classification images were estimated for a Vernier discrimination task. The display screen had 48 pixels deg-1 horizontally and vertically. The no-offset image had a dark horizontal line of 4 pixels, a 1 pixel space, and 4 more dark pixels. Classification images were based on 1600 discrimination trials with the line contrast adjusted to keep the error rate near 25\%. In the offset image, the second line was one pixel higher. Unlike the ideal observer filter (a horizontal dipole), the observer perceptual classification images are strongly oriented. Fourier transforms of the classification images had a peak amplitude near 1 cycle deg-1 and an orientation near 25 deg. The spatial spread is much more than image blur predicts, and probably indicates the spatial position uncertainty in the task.},
author = {Ahumada, Albert J},
booktitle = {Perception ECVP abstract},
doi = {10.1068/v96l0501},
keywords = {classification\_image,correlational,image,vision},
language = {EN},
mendeley-tags = {classification\_image,correlational,image,vision},
publisher = {Pion Ltd.},
title = {{Perceptual classification images from Vernier acuity masked by noise}},
url = {http://www.perceptionweb.com/abstract.cgi?id=v96l0501},
volume = {25},
year = {1996}
}

@article{NeriAndLevi2006,
abstract = {This brief review article brings together a series of related experiments in psychophysics and physiology that show striking similarities between measurements in human observers and in single neurons. We consider seven pairs of primary research articles, each pair consisting of one paper in physiology and one in psychophysics, and we highlight common features between receptive and perceptive fields obtained using reverse correlation. We conclude by discussing how to assess the validity of perceptive fields as predictors of human responses, and by deriving a novel expression for the maximum trial-by-trial predictability attainable by any model for any psychophysical task.},
author = {Neri, Peter and Levi, Dennis M},
doi = {10.1016/j.visres.2006.02.002},
file = {::},
issn = {0042-6989},
journal = {Vision research},
month = aug,
number = {16},
pages = {2465--74},
pmid = {16542700},
title = {Receptive versus perceptive fields from the reverse-correlation viewpoint.},
url = {http://www.sciencedirect.com/science/article/pii/S0042698906000733},
volume = {46},
year = {2006}
}

@article{RingachAndShapley2004,
abstract = {This article presents a review of reverse correlation in neurophysiology. We discuss the basis of reverse correlation in linear transducers and in spiking neurons. The application of reverse correlation to measure the receptive fields of visual neurons using white noise and m-sequences, and classical findings about spatial and color processing in the cortex resulting from such measurements, are emphasized. Finally, we describe new developments in reverse correlation, including “sub-space” and categorical reverse-correlation. Recent results obtained by applying such methods in the orientation, spatial-frequency and Fourier domains have revealed the importance of cortical inhibition in the establishment of sharp tuning selectivity in single neurons.},
author = {Ringach, D and Shapley, R},
doi = {10.1016/j.cogsci.2003.11.003},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Impulse response,Kernels,M-sequences,Simple cells,Sub-space reverse correlation,Triggered-correlation,neuro,physiology,reverse\_correlation},
mendeley-tags = {neuro,physiology,reverse\_correlation},
month = mar,
number = {2},
pages = {147--166},
title = {Reverse correlation in neurophysiology},
url = {http://www.sciencedirect.com/science/article/pii/S0364021303001174},
volume = {28},
year = {2004}
}

@article{VarnetEtAl2013,
author = {Varnet, L\'{e}o and Knoblauch, Kenneth and Meunier, Fanny and Hoen, Michel},
doi = {doi: 10.3389/fnhum.2013.00865},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {importance,tfif},
title = {{Using auditory classification images for the identification of fine acoustic cues used in speech perception}},
url = {citeulike-article-id:13217478 http://dx.doi.org/10.3389/fnhum.2013.00865},
volume = {7},
year = {2013}
}

@inproceedings{ HanEtAl2015,
author = {Kun Han and Yanzhang He and Eric Fosler-Lussier and DeLiang Wang},
title = {Deep Neural Network Based Spectral Mapping For Robust Speech Recognition},
booktitle = icassp,
year = {2015},
note = {Submitted},
}

@article{Watkins2005,
abstract = {Listeners were asked to identify modified recordings of the words "sir" and "stir," which were spoken by an adult male British-English speaker. Steps along a continuum between the words were obtained by a pointwise interpolation of their temporal-envelopes. These test words were embedded in a longer "context" utterance, and played with different amounts of reverberation. Increasing only the test-word's reverberation shifts the listener's category boundary so that more "sir"-identifications are made. This effect reduces when the context's reverberation is also increased, indicating perceptual compensation that is informed by the context. Experiment 1 finds that compensation is more prominent in rapid speech, that it varies between rooms, that it is more prominent when the test-word's reverberation is high, and that it increases with the context's reverberation. Further experiments show that compensation persists when the room is switched between the context and the test word, when presentation is monaural, and when the context is reversed. However, compensation reduces when the context's reverberation pattern is reversed, as well as when noise-versions of the context are used. "Tails" that reverberation introduces at the ends of sounds and at spectral transitions may inform the compensation mechanism about the amount of reflected sound in the signal.},
author = {Watkins, Anthony J},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {Adaptation, Physiological,Adult,Environment,Humans,Male,Noise,Perceptual Distortion,Speech Acoustics,Speech Perception,Speech Perception: physiology,priming,psych,reverb},
mendeley-tags = {priming,psych,reverb},
month = jul,
number = {1},
pages = {249--62},
pmid = {16119347},
title = {Perceptual compensation for effects of reverberation in speech identification.},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16119347},
volume = {118},
year = {2005}
}

@article{LuceAndPisoni1998,
abstract = {OBJECTIVE: A fundamental problem in the study of human spoken word recognition concerns the structural relations among the sound patterns of words in memory and the effects these relations have on spoken word recognition. In the present investigation, computational and experimental methods were employed to address a number of fundamental issues related to the representation and structural organization of spoken words in the mental lexicon and to lay the groundwork for a model of spoken word recognition.

DESIGN: Using a computerized lexicon consisting of transcriptions of 20,000 words, similarity neighborhoods for each of the transcriptions were computed. Among the variables of interest in the computation of the similarity neighborhoods were: 1) the number of words occurring in a neighborhood, 2) the degree of phonetic similarity among the words, and 3) the frequencies of occurrence of the words in the language. The effects of these variables on auditory word recognition were examined in a series of behavioral experiments employing three experimental paradigms: perceptual identification of words in noise, auditory lexical decision, and auditory word naming.

RESULTS: The results of each of these experiments demonstrated that the number and nature of words in a similarity neighborhood affect the speed and accuracy of word recognition. A neighborhood probability rule was developed that adequately predicted identification performance. This rule, based on Luce's (1959) choice rule, combines stimulus word intelligibility, neighborhood confusability, and frequency into a single expression. Based on this rule, a model of auditory word recognition, the neighborhood activation model, was proposed. This model describes the effects of similarity neighborhood structure on the process of discriminating among the acoustic-phonetic representations of words in memory. The results of these experiments have important implications for current conceptions of auditory word recognition in normal and hearing impaired populations of children and adults.},
author = {Luce, P A and Pisoni, D B},
issn = {0196-0202},
journal = {Ear and hearing},
keywords = {Adult,Decision Making,Humans,Language,Memory,Memory: physiology,Models, Psychological,Phonetics,Reaction Time,Speech Perception,Speech Perception: physiology,Vocabulary,memory,speech,word\_neighborhood},
mendeley-tags = {memory,speech,word\_neighborhood},
month = feb,
number = {1},
pages = {1--36},
pmid = {9504270},
title = {Recognizing spoken words: the neighborhood activation model.},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3467695\&tool=pmcentrez\&rendertype=abstract},
volume = {19},
year = {1998}
}

@inproceedings{ JyothiAndLivescu2014,
author = {Preethi Jyothi and Karen Livescu},
title = {Revisiting word neighborhoods for speech recognition},
booktitle = {ACL MORPHFSM Workshop},
year = 2014},
}

@article{ZilanyEtAl2009,
abstract = {There is growing evidence that the dynamics of biological systems that appear to be exponential over short time courses are in some cases better described over the long-term by power-law dynamics. A model of rate adaptation at the synapse between inner hair cells and auditory-nerve (AN) fibers that includes both exponential and power-law dynamics is presented here. Exponentially adapting components with rapid and short-term time constants, which are mainly responsible for shaping onset responses, are followed by two parallel paths with power-law adaptation that provide slowly and rapidly adapting responses. The slowly adapting power-law component significantly improves predictions of the recovery of the AN response after stimulus offset. The faster power-law adaptation is necessary to account for the "additivity" of rate in response to stimuli with amplitude increments. The proposed model is capable of accurately predicting several sets of AN data, including amplitude-modulation transfer functions, long-term adaptation, forward masking, and adaptation to increments and decrements in the amplitude of an ongoing stimulus.},
author = {Zilany, Muhammad S A and Bruce, Ian C and Nelson, Paul C and Carney, Laurel H},
doi = {10.1121/1.3238250},
file = {::},
issn = {1520-8524},
journal = {The Journal of the Acoustical Society of America},
keywords = {Acoustic Stimulation,Adaptation, Physiological,Adaptation, Physiological: physiology,Animals,Cochlear Nerve,Cochlear Nerve: physiology,Hair Cells, Auditory, Inner,Hair Cells, Auditory, Inner: physiology,Humans,Models, Neurological,Noise,Perceptual Masking,Perceptual Masking: physiology,Psychoacoustics,Synapses,Synapses: physiology,haircell\_model,impairment,model,neuro,neurogram,representation},
mendeley-tags = {haircell\_model,impairment,model,neuro,neurogram,representation},
month = nov,
number = {5},
pages = {2390--412},
pmid = {19894822},
publisher = {Acoustical Society of America},
title = {A phenomenological model of the synapse between the inner hair cell and auditory nerve: long-term adaptation with power-law dynamics.},
url = {http://scitation.aip.org/content/asa/journal/jasa/126/5/10.1121/1.3238250},
volume = {126},
year = {2009}
}

@inproceedings{Kingsbury2009,
abstract = {Acoustic models used in hidden Markov model/neural-network (HMM/NN) speech recognition systems are usually trained with a frame-based cross-entropy error criterion. In contrast, Gaussian mixture HMM systems are discriminatively trained using sequence-based criteria, such as minimum phone error or maximum mutual information, that are more directly related to speech recognition accuracy. This paper demonstrates that neural-network acoustic models can be trained with sequence classification criteria using exactly the same lattice-based methods that have been developed for Gaussian mixture HMMs, and that using a sequence classification criterion in training leads to considerably better performance. A neural network acoustic model with 153K weights trained on 50 hours of broadcast news has a word error rate of 34.0\% on the rt04 English broadcast news test set. When this model is trained with the state-level minimum Bayes risk criterion, the rt04 word error rate is 27.7\%.},
author = {Kingsbury, Brian},
booktitle = icassp,
doi = {10.1109/ICASSP.2009.4960445},
isbn = {978-1-4244-2353-8},
issn = {1520-6149},
keywords = {Acoustic testing,Backpropagation algorithms,Broadcasting,Error analysis,Gaussian mixture,Hidden Markov models,Lattices,Maximum likelihood estimation,Mutual information,Neural networks,Speech recognition,asr,discriminative training,hidden Markov model,hidden Markov models,lattice-based optimization,neural nets,neural networks,neural-network acoustic modeling,sequence classification criteria,sequence\_discriminative,speech recognition,word error rate},
mendeley-tags = {asr,sequence\_discriminative},
month = apr,
pages = {3761--3764},
publisher = {IEEE},
shorttitle = {Acoustics, Speech and Signal Processing, 2009. ICA},
title = {Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4960445},
year = {2009}
}

@article{Gales1998,
abstract = {This paper examines the application of linear transformations for speaker and environmental adaptation in an HMM-based speech recognition system. In particular, transformations that are trained in a maximum likelihood sense on adaptation data are investigated. Only model-based linear transforms are considered, since, for linear transforms, they subsume the appropriate feature–space transforms. The paper compares the two possible forms of model-based transforms: (i) unconstrained, where any combination of mean and variance transform may be used, and (ii) constrained, which requires the variance transform to have the same form as the mean transform. Re-estimation formulae for all appropriate cases of transform are given. This includes a new and efficient full variance transform and the extension of the constrained model–space transform from the simple diagonal case to the full or block–diagonal case. The constrained and unconstrained transforms are evaluated in terms of computational cost, recognition time efficiency, and use for speaker adaptive training. The recognition performance of the two model–space transforms on a large vocabulary speech recognition task using incremental adaptation is investigated. In addition, initial experiments using the constrained model–space transform for speaker adaptive training are detailed.},
author = {Gales, M.J.F.},
doi = {10.1006/csla.1998.0043},
issn = {08852308},
journal = {Computer Speech \& Language},
keywords = {adaptation,asr,noise,robustness},
mendeley-tags = {adaptation,asr,noise,robustness},
month = apr,
number = {2},
pages = {75--98},
title = {{Maximum likelihood linear transformations for HMM-based speech recognition}},
url = {http://www.sciencedirect.com/science/article/pii/S0885230898900432},
volume = {12},
year = {1998}
}

@article{VargaAndSteeneken1993,
abstract = {The NOISEX-92 experiment and database is described and discussed. NOISEX-92 specifies a carefully controlled experiment on artificially noisy speech data, examining performance for a limited digit recognition task but with a relatively wide range of noises and signal-to-noise ratios. Example recognition results are given. Es werden die Datenbank und die Tests NOISEX-92 beschrieben und kommentiert. NOISEX-92 spezifiziert Tests \"{u}ber k\"{u}nstliche L\"{a}rmdaten, wobei die Leistungen im Rahmen der Erjennung von Zahlen und ein relativ breiter Rauschabstand bewertet werden. Es werden Beispiele der Ergebnisse f\"{u}r Erkennungen beschrieben. La base de donn\'{e}es et les tests NOISEX-92 sont d\'{e}crits et comment\'{e}s. NOISEX-92 sp\'{e}cifie des tests sur des donn\'{e}es bruit\'{e}es artificiellement en \'{e}valuant les performances dans le cadre de la reconnaissance de chiffres et une gamme relativement large de rapports signal sur bruit. Des examples de scores de reconnaissances sont pr\'{e}sent\'{e}s.},
author = {Varga, Andrew and Steeneken, Herman J.M.},
doi = {10.1016/0167-6393(93)90095-3},
file = {::},
issn = {01676393},
journal = {Speech Communication},
keywords = {Speech recognizer assessment,additive noise,asr,calibrated data base,corpus,dataset,noise},
mendeley-tags = {asr,corpus,dataset,noise},
month = jul,
number = {3},
pages = {247--251},
title = {Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems},
url = {http://www.sciencedirect.com/science/article/pii/0167639393900953},
volume = {12},
year = {1993}
}

@inproceedings{ThiemannEtAl2013,
abstract = {Multi-microphone arrays allow for the use of spatial filtering techniques that can greatly improve noise reduction and source separation. However, for speech and audio data, work on noise reduction or separation has focused primarily on one- or two-channel systems. Because of this, databases of multichannel environmental noise are not widely available. DEMAND (Diverse Environments Multi-channel Acoustic Noise Database) addresses this problem by providing a set of 16-channel noise files recorded in a variety of indoor and outdoor settings. The data was recorded using a planar microphone array consisting of four staggered rows, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. DEMAND is freely available under a Creative Commons license to encourage research into algorithms beyond the stereo setup.},
author = {Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
booktitle = {Proceedings of Meetings on Acoustics},
doi = {10.1121/1.4799597},
issn = {1939-800X},
keywords = {corpus,dataset,micarray,noise},
mendeley-tags = {corpus,dataset,micarray,noise},
month = jun,
number = {1},
pages = {035081--035081},
publisher = {Acoustical Society of America},
title = {The Diverse Environments Multi-channel Acoustic Noise Database ({DEMAND}): A database of multichannel environmental noise recordings},
url = {http://scitation.aip.org/content/asa/journal/poma/19/1/10.1121/1.4799597},
volume = {19},
year = {2013}
}

@inproceedings{sak2014long,
  title={Long short-term memory recurrent neural network architectures for large scale acoustic modeling},
  author={Sak, Hasim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
  booktitle={Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH)},
  year={2014}
}

@inproceedings{SeungEtAl1992,
address = {New York, New York, USA},
author = {Seung, H. S. and Opper, M. and Sompolinsky, H.},
booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
doi = {10.1145/130385.130417},
file = {::},
isbn = {089791497X},
keywords = {active\_learning,ensemble\_learning},
mendeley-tags = {active\_learning,ensemble\_learning},
month = jul,
pages = {287--294},
publisher = {ACM Press},
title = {Query by committee},
url = {http://dl.acm.org/citation.cfm?id=130385.130417},
year = {1992}
}

@inproceedings{povey2011kaldi,
  title={The Kaldi speech recognition toolkit},
  author={Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Qian, Yanmin and Schwarz, Petr and others},
  year = {2011},
  booktitle = {Proceedings of automatic speech recognition and understanding},
  pages = {1--4},
}

@techreport{ SmithAndPage2015,
  author = {Aaron Smith and Dana Page},
  year = {2015},
  title = {The Smartphone Difference},
  organization = {Pew Research Center},
  month = apr,
  note = {Available at: http://www.pewinternet.org/2015/04/01/us-smartphone-use-in-2015/},
  }

@article{ChenEtAl2006,
abstract = {The problem of noise reduction has attracted a considerable amount of research attention over the past several decades. Among the numerous techniques that were developed, the optimal Wiener filter can be considered as one of the most fundamental noise reduction approaches, which has been delineated in different forms and adopted in various applications. Although it is not a secret that the Wiener filter may cause some detrimental effects to the speech signal (appreciable or even significant degradation in quality or intelligibility), few efforts have been reported to show the inherent relationship between noise reduction and speech distortion. By defining a speech-distortion index to measure the degree to which the speech signal is deformed and two noise-reduction factors to quantify the amount of noise being attenuated, this paper studies the quantitative performance behavior of the Wiener filter in the context of noise reduction. We show that in the single-channel case the a posteriori signal-to-noise ratio (SNR) (defined after the Wiener filter) is greater than or equal to the a priori SNR (defined before the Wiener filter), indicating that the Wiener filter is always able to achieve noise reduction. However, the amount of noise reduction is in general proportional to the amount of speech degradation. This may seem discouraging as we always expect an algorithm to have maximal noise reduction without much speech distortion. Fortunately, we show that speech distortion can be better managed in three different ways. If we have some a priori knowledge (such as the linear prediction coefficients) of the clean speech signal, this a priori knowledge can be exploited to achieve noise reduction while maintaining a low level of speech distortion. When no a priori knowledge is available, we can still achieve a better control of noise reduction and speech distortion by properly manipulating the Wiener filter, resulting in a suboptimal Wiener filter. In case that we have multi-
      -
      ple microphone sensors, the multiple observations of the speech signal can be used to reduce noise with less or even no speech distortion},
author = {Chen, Jingdong and Benesty, J and Huang, Yiteng and Doclo, S},
doi = {doi: 10.1109/tsa.2005.860851},
issn = {1558-7916},
journal = taslp,
keywords = {separation,speech,wienerfilter},
month = jul,
number = {4},
pages = {1218--1234},
title = {New insights into the noise reduction Wiener filter},
url = {citeulike-article-id:2973405 http://dx.doi.org/10.1109/tsa.2005.860851},
volume = {14},
year = {2006}
}

@article{ Hecht2014,
  author = {Jeff Hecht},
  title = {Why Mobile Voice Quality Still Stinks---and How to Fix It},
  year = 2014,
  month = sep,
  journal = {{IEEE} Spectrum},
  }

@inproceedings{ LevitEtAl2015,
  author = {Levit, M. and Stolcke, A. and Subba, R. and Parthasarathy, S. and Chang, S. and Xie, S. and Anastasakos, T. and Dumoulin, B.},
  year = {2015},
  title = {Personalization of Word-Phrase-Entity Language Models},
  booktitle = interspeech,
}

@inproceedings{Rosenberg2010,
author = {Rosenberg, Andrew},
booktitle = interspeech,
keywords = {classification,prosody},
mendeley-tags = {classification,prosody},
title = {{AuToBI} – A tool for automatic {ToBI} annotation},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.463.4856},
year = {2010}
}

@article{DeCheveigneAndKawahara2002,
abstract = {An algorithm is presented for the estimation of the fundamental frequency (F 0 ) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.},
author = {de Cheveigné, Alain and Kawahara, Hideki},
doi = {10.1121/1.1458024},
issn = {00014966},
journal = jasa,
keywords = {f0,fundamental,pitch},
mendeley-tags = {f0,fundamental,pitch},
month = {apr},
number = {4},
pages = {1917},
publisher = {Acoustical Society of America},
title = {{YIN}, a fundamental frequency estimator for speech and music},
url = {http://scitation.aip.org/content/asa/journal/jasa/111/4/10.1121/1.1458024},
volume = {111},
year = {2002}
}

@inproceedings{CharpentierAndStella1986,
abstract = {A new method is presented for text-to-speech synthesis using diphones. The diphone database consists of the diphone waveforms labeled with pitch-marks indicating the pitch-periods. At synthesis time, the diphone waveforms are processed through a new analysis-synthesis system, providing an independent control of all prosodic parameters, while retaining a good degree of naturalness. This system is based on a representation of the speech signal by its short-time Fourier transform (STFT) at a pitch-synchronous sampling rate. The synthesis part of the system works by overlap-adding the modified short-term signals and it ensures a smooth concatenation of the diphone waveforms. The synthetic speech obtained by this method sounds more natural than with the conventional LPC method.},
author = {Charpentier, F. and Stella, M.},
booktitle = icassp,
doi = {10.1109/ICASSP.1986.1168657},
keywords = {Control system synthesis,Databases,Filter bank,Linear predictive coding,Signal analysis,Signal synthesis,Speech analysis,Speech coding,Speech synthesis,Telecommunication control,concatenative,psola,synthesis},
mendeley-tags = {concatenative,psola,synthesis},
pages = {2015--2018},
publisher = {Institute of Electrical and Electronics Engineers},
shorttitle = {Acoustics, Speech, and Signal Processing, IEEE Int},
title = {Diphone synthesis using an overlap-add technique for speech waveforms concatenation},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1168657},
volume = {11},
year = {1986}
}

@inproceedings{HuangEtAl2013,
address = {New York, New York, USA},
author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
booktitle = cikm,
doi = {10.1145/2505515.2505665},
isbn = {9781450322638},
keywords = {clickthrough data,deep learning,deep{\_}learning,dssn,semantic model,similarity,vector{\_}space,web search},
mendeley-tags = {deep{\_}learning,dssn,similarity,vector{\_}space},
month = {oct},
pages = {2333--2338},
publisher = {ACM Press},
title = {Learning deep structured semantic models for web search using clickthrough data},
url = {http://dl.acm.org/citation.cfm?id=2505515.2505665},
year = {2013}
}

@inproceedings{ChopraEtAl2005,
abstract = {We present a method for training a similarity metric from  data. The method can be used for recognition or verification  applications where the number of categories is very large  and not known during training, and where the number of  training samples for a single category is very small. The  idea is to learn a function that maps input patterns into a  target space such that the L₁ norm in the target space   approximates the "semantic" distance in the input space. The  method is applied to a face verification task. The learning  process minimizes a discriminative loss function that drives  the similarity metric to be small for pairs of faces from the  same person, and large for pairs from different persons. The  mapping from raw to the target space is a convolutional network  whose architecture is designed for robustness to geometric  distortions. The system is tested on the Purdue/AR  face database which has a very high degree of variability in  the pose, lighting, expression, position, and artificial occlusions  such as dark glasses and obscuring scarves.},
address = {Los Alamitos, CA, USA},
author = {Chopra, S and Hadsell, R and LeCun, Y},
booktitle = cvpr,
doi = {doi: 10.1109/cvpr.2005.202},
isbn = {0-7695-2372-2},
keywords = {metriclearning,neuralnet,siamesenetwork},
month = {jun},
pages = {539--546 vol. 1},
publisher = {IEEE},
title = {Learning a similarity metric discriminatively, with application to face verification},
url = {citeulike-article-id:937619 http://dx.doi.org/10.1109/cvpr.2005.202},
volume = {1},
year = {2005}
}


@article{LyonEtAl2010,
    abstract = {To create systems that understand the sounds that humans are exposed to in everyday life, we need to represent sounds with features that can discriminate among many different sound classes. Here, we use a sound-ranking framework to quantitatively evaluate such representations in a large-scale task. We have adapted a machine-vision method, the passive-aggressive model for image retrieval ({PAMIR}), which efficiently learns a linear mapping from a very large sparse feature space to a large query-term space. Using this approach, we compare different auditory front ends and different ways of extracting sparse features from high-dimensional auditory images. We tested auditory models that use an adaptive pole?zero filter cascade ({PZFC}) auditory filter bank and sparse-code feature extraction from stabilized auditory images with multiple vector quantizers. In addition to auditory image models, we compare a family of more conventional mel-frequency cepstral coefficient ({MFCC}) front ends. The experimental results show a significant advantage for the auditory models over vector-quantized {MFCCs}. When thousands of sound files with a query vocabulary of thousands of words were ranked, the best precision at top-1 was 73\% and the average precision was 35\%, reflecting a 18\% improvement over the best competing {MFCC} front end.},
    author = {Lyon, Richard F. and Rehn, Martin and Bengio, Samy and Walters, Thomas C. and Chechik, Gal},
    citeulike-article-id = {9566015},
    citeulike-linkout-0 = {http://dx.doi.org/10.1162/neco\_a\_00011},
    citeulike-linkout-1 = {http://www.mitpressjournals.org/doi/abs/10.1162/NECO\_a\_00011},
    day = {22},
    doi = {10.1162/neco\_a\_00011},
    journal = nc,
    keywords = {environmental, pamir, ranking, retrieval, sparse},
    month = jun,
    number = {9},
    pages = {2390--2416},
    posted-at = {2013-04-10 18:33:30},
    priority = {2},
    publisher = {MIT Press},
    title = {Sound Retrieval and Ranking Using Sparse Auditory Representations},
    url = {http://dx.doi.org/10.1162/neco\_a\_00011},
    volume = {22},
    year = {2010}
}

@inproceedings{BarkerEtAl2015,
author = {Barker, Jon and Marxer, Ricard and Vincent, Emmanuel and Watanabe, Shinji},
title = {The third `CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines},
booktitle=asru,
year={2015},
note={to appear}
}

@inproceedings{BraunschweilerEtAl2010,
abstract = {Large quantities of audio data with associated text such as audiobooks are nowadays available. These data are a ttractive for a range of research areas as they include featu res that go beyond the level of single sentences. The proposed approach allows high quality transcriptions and associated a lignments of this form of data to be automatically generated. It combines information from lightly supervised recognition and the original text to yield the final transcription. The scheme is fully automatic and has been successfully applied t o a number of audiobooks. Performance measurements show low word/sentence error rates as well as high sentence boundary accuracy},
author = {Braunschweiler, Norbert and Gales, Mark JF and Buchholz, Sabine},
booktitle = {INTERSPEECH},
keywords = {asr,audiobooks,forced{\_}alignment},
mendeley-tags = {forced{\_}alignment,asr,audiobooks},
pages = {2222--2225},
title = {{Lightly supervised recognition for automatic alignment of large coherent speech recordings}},
url = {https://www.researchgate.net/profile/Norbert{\_}Braunschweiler/publication/221492025{\_}Lightly{\_}supervised{\_}recognition{\_}for{\_}automatic{\_}alignment{\_}of{\_}large{\_}coherent{\_}speech{\_}recordings/links/09e4151138d384e97f000000.pdf},
year = {2010}
}

@inproceedings{ErdoganEtAl2015,
author = {Erdogan, Hakan and Hershey, John R. and Watanabe, Shinji and {Le Roux}, Jonathan},
booktitle = icassp,
doi = {10.1109/ICASSP.2015.7178061},
isbn = {978-1-4673-6997-8},
keywords = {ASR,LSTM,Linear programming,Noise measurement,Signal to noise ratio,Speech,Speech enhancement,Speech recognition,Training,bidirectional recurrent networks,deep networks,deep recurrent neural networks,dnn,loss{\_}function,masking,nonstationary interference,phase-sensitive objective function,phase{\_}sensitive,recognition-boosted speech separation,recurrent neural nets,rnn,signal reconstruction,signal-approximation based objective function,speech enhancement,speech recognition,speech separation},
language = {English},
mendeley-tags = {dnn,loss{\_}function,masking,phase{\_}sensitive,rnn},
month = {apr},
pages = {708--712},
publisher = {IEEE},
title = {Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7178061},
year = {2015}
}

@article{WangAndWang2013,
author = {Wang, Yuxuan and Wang, DeLiang},
doi = {doi: 10.1109/tasl.2013.2250961},
issn = {1558-7916},
journal = taslp,
keywords = {deeplearning,dnn,masking,separation},
month = {jul},
number = {7},
pages = {1381--1390},
publisher = {IEEE},
title = {Towards Scaling Up Classification-Based Speech Separation},
url = {citeulike-article-id:13055846 http://dx.doi.org/10.1109/tasl.2013.2250961},
volume = {21},
year = {2013}
}

@article{MujaAndLowe2014,
abstract = {For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.},
author = {Muja, Marius and Lowe, David G.},
doi = {10.1109/TPAMI.2014.2321376},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Approximation algorithms,Approximation methods,Clustering algorithms,Computer vision,FLANN,Machine learning algorithms,Nearest neighbor search,Partitioning algorithms,Vegetation,algorithm configuration,approximate,approximate search,approximate{\_}nearest{\_}neighbors,automated configuration procedure,big data,computer vision,distributed nearest neighbor matching framework,fast library for approximate nearest neighbors,high dimensional data,high dimensional feature matching,high dimensional vectors,image matching,learning (artificial intelligence),machine learning problems,multiple hierarchical clustering trees,nearest{\_}neighbors,open source library,priority search k-means tree,randomized k-d forest algorithm,scalable nearest neighbor algorithms,search problems,trees (mathematics)},
mendeley-tags = {approximate,approximate{\_}nearest{\_}neighbors,nearest{\_}neighbors},
month = {nov},
number = {11},
pages = {2227--2240},
shorttitle = {Pattern Analysis and Machine Intelligence, IEEE Tr},
title = {Scalable Nearest Neighbor Algorithms for High Dimensional Data},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6809191},
volume = {36},
year = {2014}
}

@article{BisaniAndNey2008,
abstract = {Grapheme-to-phoneme conversion is the task of finding the pronunciation of a word given its written form. It has important applications in text-to-speech and speech recognition. Joint-sequence models are a simple and theoretically stringent probabilistic framework that is applicable to this problem. This article provides a self-contained and detailed description of this method. We present a novel estimation algorithm and demonstrate high accuracy on a variety of databases. Moreover, we study the impact of the maximum approximation in training and transcription, the interaction of model size parameters, n-best list generation, confidence measures, and phoneme-to-grapheme conversion. Our software implementation of the method proposed in this work is available under an Open Source license.},
author = {Bisani, Maximilian and Ney, Hermann},
doi = {10.1016/j.specom.2008.01.002},
issn = {01676393},
journal = spcomm,
keywords = {Grapheme-to-phoneme,Joint-sequence model,Letter-to-sound,Phonemic transcription,Pronunciation modeling},
month = {may},
number = {5},
pages = {434--451},
title = {Joint-sequence models for grapheme-to-phoneme conversion},
url = {http://www.sciencedirect.com/science/article/pii/S0167639308000046},
volume = {50},
year = {2008}
}

@article{HuAndLoizou2008,
abstract = {In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy speech enhanced by noise suppression algorithms. The objective measures considered a wide range of distortions introduced by four types of real-world noise at two signal-to-noise ratio levels by four classes of speech enhancement algorithms: spectral subtractive, subspace, statistical-model based, and Wiener algorithms. The subjective quality ratings were obtained using the ITU-T P.835 methodology designed to evaluate the quality of enhanced speech along three dimensions: signal distortion, noise distortion, and overall quality. This paper reports on the evaluation of correlations of several objective measures with these three subjective rating scales. Several new composite objective measures are also proposed by combining the individual objective measures using nonparametric and parametric regression analysis techniques.},
author = {Hu, Yi and Loizou, Philipos C},
doi = {doi: 10.1109/tasl.2007.911054},
issn = {1558-7916},
journal = taslp,
keywords = {enhancement,evaluation,pesq,speech},
month = {jan},
number = {1},
pages = {229--238},
publisher = {IEEE},
title = {Evaluation of Objective Quality Measures for Speech Enhancement},
url = {citeulike-article-id:5219302 http://dx.doi.org/10.1109/tasl.2007.911054},
volume = {16},
year = {2008}
}

@article{HuAndLoizou2007,
abstract = {Making meaningful comparisons between the performance of the various speech enhancement algorithms proposed over the years, has been elusive due to lack of a common speech database, differences in the types of noise used and differences in the testing methodology. To facilitate such comparisons, we report on the development of a noisy speech corpus suitable for evaluation of speech enhancement algorithms. This corpus is subsequently used for the subjective evaluation of 13 speech enhancement methods encompassing four classes of algorithms: spectral subtractive, subspace, statistical-model based and Wiener-type algorithms. The subjective evaluation was performed by Dynastat, Inc. using the ITU-T P.835 methodology designed to evaluate the speech quality along three dimensions: signal distortion, noise distortion and overall quality. This paper reports the results of the subjective tests.},
author = {Hu, Yi and Loizou, Philipos C},
doi = {10.1016/j.specom.2006.12.006},
file = {::},
issn = {0167-6393},
journal = spcomm,
keywords = {enhancement,hitMinusFa,listening{\_}test,p835,quality,stationary{\_}noise,subjective},
mendeley-tags = {enhancement,hitMinusFa,listening{\_}test,p835,quality,stationary{\_}noise,subjective},
month = {jul},
number = {7},
pages = {588--601},
pmid = {18046463},
title = {Subjective comparison and evaluation of speech enhancement algorithms.},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2098693{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {49},
year = {2007}
}

@inproceedings{HuntAndBlack1996,
abstract = {One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning},
author = {Hunt, A J and Black, Alan},
booktitle = icassp,
doi = {doi: 10.1109/icassp.1996.541110},
isbn = {0-7803-3192-3},
keywords = {6820,hmm,speech,synthesis},
month = {may},
pages = {373--376},
publisher = {IEEE},
title = {Unit selection in a concatenative speech synthesis system using a large speech database},
url = {citeulike-article-id:3934924 http://dx.doi.org/10.1109/icassp.1996.541110},
volume = {1},
year = {1996}
}

@inproceedings{SmaragdisEtAl2009,
author = {Smaragdis, Paris and Shashanka, Madhusudana and Raj, Bhiksha},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
keywords = {cochannel,exemplar,monaural,nmf,plca,separation,sparsity,speech},
mendeley-tags = {cochannel,exemplar,monaural,nmf,plca,separation,sparsity,speech},
pages = {1705--1713},
title = {A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds},
url = {http://papers.nips.cc/paper/3668-a-sparse-non-parametric-approach-for-single-channel-separation-of-known-sounds},
year = {2009}
}

@incollection{CichockiEtAl2006,
author = {Cichocki, Andrzej and Zdunek, Rafal and Amari, Shun-ichi},
booktitle = {Independent Component Analysis and Blind Signal Separation SE  - 5},
doi = {10.1007/11679363{\_}5},
editor = {Rosca, Justinian and Erdogmus, Deniz and Pr{\'{\i}}ncipe, Jos{\'{e}}C. and Haykin, Simon},
isbn = {978-3-540-32630-4},
language = {English},
pages = {32--39},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {Csisz{\'{a}}r’s Divergences for Non-negative Matrix Factorization: Family of New Algorithms},
url = {http://dx.doi.org/10.1007/11679363{\_}5},
volume = {3889},
year = {2006}
}

@TechReport{HuangEtAl2007,
  author =       {Gary B. Huang and Manu Ramesh and Tamara Berg and 
                  Erik Learned-Miller},
  title =        {Labeled Faces in the Wild: A Database for Studying 
                  Face Recognition in Unconstrained Environments},
  institution =  {U.~Mass., Amherst},
  year =         2007,
  number =       {07-49},
  month =        oct,
}

@article{BarkerEtAl2013,
    abstract = {Distant microphone speech recognition systems that operate with human-like robustness remain a distant goal. The key difficulty is that operating in everyday listening conditions entails processing a speech signal that is reverberantly mixed into a noise background composed of multiple competing sound sources. This paper describes a recent speech recognition evaluation that was designed to bring together researchers from multiple communities in order to foster novel approaches to this problem. The task was to identify keywords from sentences reverberantly mixed into audio backgrounds binaurally recorded in a busy domestic environment. The challenge was designed to model the essential difficulties of the multisource environment problem while remaining on a scale that would make it accessible to a wide audience. Compared to previous {ASR} evaluations a particular novelty of the task is that the utterances to be recognised were provided in a continuous audio background rather than as pre-segmented utterances thus allowing a range of background modelling techniques to be employed. The challenge attracted thirteen submissions. This paper describes the challenge problem, provides an overview of the systems that were entered and provides a comparison alongside both a baseline recognition system and human performance. The paper discusses insights gained from the challenge and lessons learnt for the design of future such evaluations. \^{a}º The paper reviews a recent distant microphone speech recognition evaluation that attracted participation from 13 entrants. \^{a}º The paper presents a comparative analysis of the recognition systems that were entered. \^{a}º Results of the automatic systems are present and compared to human performance. Common features of successful systems are identified. \^{a}º The paper concludes with a brief discussion of possible directions for future challenges.},
    author = {Barker, Jon and Vincent, Emmanuel and Ma, Ning and Christensen, Heidi and Green, Phil},
    citeulike-article-id = {11515416},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.csl.2012.10.004},
    doi = {10.1016/j.csl.2012.10.004},
    issn = {08852308},
    journal = {Comp.~Speech \& Lang.},
    keywords = {noise, noisy, speechrecognition, survey},
    month = may,
    number = {3},
    pages = {621--633},
    posted-at = {2013-04-09 19:39:40},
    priority = {0},
    title = {The {PASCAL} {CHiME} speech separation and recognition challenge},
    url = {http://dx.doi.org/10.1016/j.csl.2012.10.004},
    volume = {27},
    year = {2013}
}

@inproceedings{ DeanEtAl2010,
title = {The {QUT-NOISE-TIMIT} corpus for the evaluation of voice activity detection algorithms},
author = {Dean, David B. and Sridharan, Sridha and Vogt, Robert J. and Mason, Michael W.},
year = {2010},
booktitle = {Proc.~Interspeech},
location = {Makuhari},
abstract = {The {QUT-NOISE-TIMIT} corpus consists of 600 hours of noisy speech sequences designed to enable a thorough evaluation of voice activity detection ({VAD}) algorithms across a wide variety of common background noise scenarios. In order to construct the final mixed-speech database, a collection of over 10 hours of background noise was conducted across 10 unique locations covering 5 common noise scenarios, to create the {QUT-NOISE} corpus. This background noise corpus was then mixed with speech events chosen from the {TIMIT} clean speech corpus over a wide variety of noise lengths, signal-to-noise ratios ({SNRs}) and active speech proportions to form the mixed-speech {QUT-NOISE-TIMIT} corpus. The evaluation of five baseline {VAD} systems on the {QUT-NOISE-TIMIT} corpus is conducted to validate the data and show that the variety of noise available will allow for better evaluation of {VAD} systems than existing approaches in the literature.},
}

@inproceedings{ThiemannEtAl2013,
    hal_id = {hal-00796707},
    title = {The Diverse Environments Multi-channel Acoustic Noise Database ({DEMAND}): A database of multichannel environmental noise recordings},
    author = {Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
    abstract = {{Multi-microphone arrays allow for the use of spatial filtering techniques that can greatly improve noise reduction and source separation. However, for speech and audio data, work on noise reduction or separation has focused primarily on one- or two-channel systems. Because of this, databases of multichannel environmental noise are not widely available. DEMAND (Diverse Environments Multi-channel Acoustic Noise Database) addresses this problem by providing a set of 16-channel noise files recorded in a variety of indoor and outdoor settings. The data was recorded using a planar microphone array consisting of four staggered rows, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. DEMAND is freely available under a Creative Commons license to encourage research into algorithms beyond the stereo setup.}},
    booktitle = {Intl.~Cong.~Acous.},
    address = {Montreal},
    year = {2013},
    month = jun,
}

@article{SainathEtAl2012,
    abstract = {Solving real-world classification and recognition problems requires a principled way of modeling the physical phenomena generating the observed data and the uncertainty in it. The uncertainty originates from the fact that many data generation aspects are influenced by nondirectly measurable variables or are too complex to model and hence are treated as random fluctuations. For example, in speech production, uncertainty could arise from vocal tract variations among different people or corruption by noise. The goal of modeling is to establish a generalization from the set of observed data such that accurate inference (classification, decision, recognition) can be made about the data yet to be observed, which we refer to as unseen data.},
    author = {Sainath, T. and Ramabhadran, B. and Nahamoo, D. and Kanevsky, D. and Van Compernolle, D. and Demuynck, K. and Gemmeke, J. and Bellegarda, J. and Sundaram, S.},
    citeulike-article-id = {12252467},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/msp.2012.2208663},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6296529},
    doi = {10.1109/msp.2012.2208663},
    issn = {1053-5888},
    journal = spm,
    keywords = {exemplar, speechrecognition},
    month = nov,
    number = {6},
    pages = {98--113},
    posted-at = {2013-04-09 20:43:44},
    priority = {4},
    publisher = {IEEE},
    title = {{Exemplar-Based} Processing for Speech Recognition: An Overview},
    url = {http://dx.doi.org/10.1109/msp.2012.2208663},
    volume = {29},
    year = {2012}
}

@article{LeeAndSeung1999,
    abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2, 3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4, 5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
    author = {Lee, Daniel D. and Seung, H. Sebastian},
    citeulike-article-id = {83540},
    citeulike-linkout-0 = {http://dx.doi.org/10.1038/44565},
    citeulike-linkout-1 = {http://dx.doi.org/10.1038/401788a0},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/10548103},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=10548103},
    day = {21},
    doi = {10.1038/44565},
    issn = {0028-0836},
    journal = {Nature},
    keywords = {image, nmf},
    month = oct,
    number = {6755},
    pages = {788--791},
    pmid = {10548103},
    posted-at = {2013-04-10 18:28:16},
    priority = {2},
    publisher = {Nature Publishing Group},
    title = {Learning the parts of objects by non-negative matrix factorization},
    url = {http://dx.doi.org/10.1038/44565},
    volume = {401},
    year = {1999}
}

@misc{ timit,
  author = {J. S. Garofolo and L. F. Lamel and W. M. Fisher and J. G. Fiscus and D. S. Pallett and N. L. Dahlgren},
  title  = {{DARPA} {TIMIT} Acoustic Phonetic Continuous Speech Corpus {CDROM}},
  publisher = {NIST}, 
  year   = {1993}, 
  Url    = {http://www.ldc.upenn.edu/Catalog/LDC93S1.html},
}

@inproceedings{parihar2003performance,
  title={Performance analysis of the Aurora large vocabulary baseline system},
  author={Parihar, N and Picone, J and Pearce, D and Hirsch, HG},
  booktitle={Proc.~Eurospeech},
  pages={337--340},
  year={2003},
}

@inproceedings{smaragdis2009sparse,
  title={A sparse non-parametric approach for single channel separation of known sounds},
  author={Smaragdis, Paris and Shashanka, Madhusudana and Raj, Bhiksha},
  booktitle=nips,
  pages={1705--1713},
  year={2009}
}

@article{CandesEtAl2011,
    abstract = {This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
    address = {New York, NY, USA},
    author = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
    citeulike-article-id = {9383116},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1970395},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1970392.1970395},
    doi = {10.1145/1970392.1970395},
    issn = {0004-5411},
    journal = {J.~{ACM}},
    keywords = {factorization, lowrank, matrix, nmf, sparse},
    month = jun,
    number = {3},
    posted-at = {2013-04-10 18:54:32},
    priority = {2},
    publisher = {ACM},
    title = {Robust principal component analysis?},
    url = {http://dx.doi.org/10.1145/1970392.1970395},
    volume = {58},
    year = {2011}
}
@article{McDermottEtAl2011,
    abstract = {Cocktail parties and other natural auditory environments present organisms with mixtures of sounds. Segregating individual sound sources is thought to require prior knowledge of source properties, yet these presumably cannot be learned unless the sources are segregated first. Here we show that the auditory system can bootstrap its way around this problem by identifying sound sources as repeating patterns embedded in the acoustic input. Due to the presence of competing sounds, source repetition is not explicit in the input to the ear, but it produces temporal regularities that listeners detect and use for segregation. We used a simple generative model to synthesize novel sounds with naturalistic properties. We found that such sounds could be segregated and identified if they occurred more than once across different mixtures, even when the same sounds were impossible to segregate in single mixtures. Sensitivity to the repetition of sound sources can permit their recovery in the absence of other segregation cues or prior knowledge of sounds, and could help solve the cocktail party problem.},
    author = {McDermott, Josh H. and Wrobleski, David and Oxenham, Andrew J.},
    citeulike-article-id = {11682386},
    citeulike-linkout-0 = {http://dx.doi.org/10.1073/pnas.1004765108},
    citeulike-linkout-1 = {http://www.pnas.org/content/108/3/1188.abstract},
    citeulike-linkout-2 = {http://www.pnas.org/content/108/3/1188.full.pdf},
    citeulike-linkout-3 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3024660/},
    citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/21199948},
    citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=21199948},
    day = {18},
    doi = {10.1073/pnas.1004765108},
    issn = {1091-6490},
    journal = pnas,
    keywords = {mixtures, psychoacoustics},
    month = jan,
    number = {3},
    pages = {1188--1193},
    pmcid = {PMC3024660},
    pmid = {21199948},
    posted-at = {2013-04-10 19:02:34},
    priority = {2},
    publisher = {National Academy of Sciences},
    title = {Recovering sound sources from embedded repetition},
    url = {http://dx.doi.org/10.1073/pnas.1004765108},
    volume = {108},
    year = {2011}
}
@article{RafiiAndPardo2013,
    abstract = {Repetition is a core principle in music. Many musical pieces are characterized by an underlying repeating structure over which varying elements are superimposed. This is especially true for pop songs where a singer often overlays varying vocals on a repeating accompaniment. On this basis, we present the {REpeating} Pattern Extraction Technique ({REPET}), a novel and simple approach for separating the repeating  ” background” from the non-repeating  ” foreground” in a mixture. The basic idea is to identify the periodically repeating segments in the audio, compare them to a repeating segment model derived from them, and extract the repeating patterns via time-frequency masking. Experiments on data sets of 1,000 song clips and 14 full-track real-world songs showed that this method can be successfully applied for music/voice separation, competing with two recent state-of-the-art approaches. Further experiments showed that {REPET} can also be used as a preprocessor to pitch detection algorithms to improve melody extraction.},
    author = {Rafii, Z. and Pardo, B.},
    citeulike-article-id = {12256791},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tasl.2012.2213249},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6269059},
    doi = {10.1109/tasl.2012.2213249},
    issn = {1558-7916},
    journal = taslp,
    keywords = {repetition, separation},
    month = jan,
    number = {1},
    pages = {73--84},
    posted-at = {2013-04-10 19:04:21},
    priority = {2},
    publisher = {IEEE},
    title = {{REpeating} Pattern Extraction Technique ({REPET}): A Simple Method for {Music/Voice} Separation},
    url = {http://dx.doi.org/10.1109/tasl.2012.2213249},
    volume = {21},
    year = {2013}
}

@inproceedings{BurgesEtAl2005,
    abstract = {We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce {RankNet}, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.},
    author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
    booktitle = icml,
    citeulike-article-id = {1714577},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1102351.1102363},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1102351.1102363},
    doi = {10.1145/1102351.1102363},
    isbn = {1-59593-180-5},
    keywords = {gradient, neuralnet, ranking},
    pages = {89--96},
    posted-at = {2014-04-16 20:21:06},
    priority = {0},
    title = {Learning to Rank Using Gradient Descent},
    url = {http://dx.doi.org/10.1145/1102351.1102363},
    year = {2005}
}

@article{MingEtAl2013,
    abstract = {This paper studies single-channel speech separation, assuming unknown, arbitrary temporal dynamics for the speech signals to be separated. A data-driven approach is described, which matches each mixed speech segment against a composite training segment to separate the underlying clean speech segments. To advance the separation accuracy, the new approach seeks and separates the longest mixed speech segments with matching composite training segments. Lengthening the mixed speech segments to match reduces the uncertainty of the constituent training segments, and hence the error of separation. For convenience, we call the new approach Composition of Longest Segments, or {CLOSE}. The {CLOSE} method includes a data-driven approach to model long-range temporal dynamics of speech signals, and a statistical approach to identify the longest mixed speech segments with matching composite training segments. Experiments are conducted on the Wall Street Journal database, for separating mixtures of two simultaneous large-vocabulary speech utterances spoken by two different speakers. The results are evaluated using various objective and subjective measures, including the challenge of large-vocabulary continuous speech recognition. It is shown that the new separation approach leads to significant improvement in all these measures.},
    author = {Ming, Ji and Srinivasan, R. and Crookes, D. and Jafari, A.},
    citeulike-article-id = {12274606},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tasl.2013.2250959},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6473839},
    doi = {10.1109/tasl.2013.2250959},
    issn = {1558-7916},
    journal = taslp,
    keywords = {datadriven, monaural, nonparametric, separation},
    month = jul,
    number = {7},
    pages = {1355--1368},
    posted-at = {2013-04-16 14:35:54},
    priority = {0},
    publisher = {IEEE},
    title = {{CLOSE}-A {Data-Driven} Approach to Speech Separation},
    url = {http://dx.doi.org/10.1109/tasl.2013.2250959},
    volume = {21},
    year = {2013}
}

@inproceedings{ OgawaEtAl2014,
 title = {Fast segment search for corpus-based speech enhancement based on speech recognition technology},
 author = {Atsunori Ogawa and Keisuke Kinoshita and Takaaki Hori and Tomohiro Nakatani and Atsushi Nakamura},
  booktitle = icassp,
  year = 2014,
}

@article{NickelEtAl2013,
    abstract = {We present a new approach for corpus-based speech enhancement that significantly improves over a method published by Xiao and Nickel in 2010. Corpus-based enhancement systems do not merely filter an incoming noisy signal, but resynthesize its speech content via an inventory of pre-recorded clean signals. The goal of the procedure is to perceptually improve the sound of speech signals in background noise. The proposed new method modifies Xiao's method in four significant ways. Firstly, it employs a Gaussian mixture model ({GMM}) instead of a vector quantizer in the phoneme recognition front-end. Secondly, the state decoding of the recognition stage is supported with an uncertainty modeling technique. With the {GMM} and the uncertainty modeling it is possible to eliminate the need for noise dependent system training. Thirdly, the post-processing of the original method via sinusoidal modeling is replaced with a powerful cepstral smoothing operation. And lastly, due to the improvements of these modifications, it is possible to extend the operational bandwidth of the procedure from 4 {kHz} to 8 {kHz}. The performance of the proposed method was evaluated across different noise types and different signal-to-noise ratios. The new method was able to significantly outperform traditional methods, including the one by Xiao and Nickel, in terms of {PESQ} scores and other objective quality measures. Results of subjective {CMOS} tests over a smaller set of test samples support our claims.},
    author = {Nickel, R. M. and Astudillo, R. F. and Kolossa, D. and Martin, R.},
    citeulike-article-id = {13170514},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tasl.2013.2243434},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6423260},
    doi = {10.1109/tasl.2013.2243434},
    issn = {1558-7916},
    journal = taslp,
    keywords = {enhancement, exemplar, separation, speech},
    month = may,
    number = {5},
    pages = {983--997},
    posted-at = {2014-05-14 14:41:06},
    priority = {4},
    publisher = {IEEE},
    title = {{Corpus-Based} Speech Enhancement With Uncertainty Modeling and Cepstral Smoothing},
    url = {http://dx.doi.org/10.1109/tasl.2013.2243434},
    volume = {21},
    year = {2013}
}

@article{MingEtAl2011,
    abstract = {Temporal dynamics and speaker characteristics are two important features of speech that distinguish speech from noise. In this paper, we propose a method to maximally extract these two features of speech for speech enhancement. We demonstrate that this can reduce the requirement for prior information about the noise, which can be difficult to estimate for fast-varying noise. Given noisy speech, the new approach estimates clean speech by recognizing long segments of the clean speech as whole units. In the recognition, clean speech sentences, taken from a speech corpus, are used as examples. Matching segments are identified between the noisy sentence and the corpus sentences. The estimate is formed by using the longest matching segments found in the corpus sentences. Longer speech segments as whole units contain more distinct dynamics and richer speaker characteristics, and can be identified more accurately from noise than shorter speech segments. Therefore, estimation based on the longest recognized segments increases the noise immunity and hence the estimation accuracy. The new approach consists of a statistical model to represent up to sentence-long temporal dynamics in the corpus speech, and an algorithm to identify the longest matching segments between the noisy sentence and the corpus sentences. The algorithm is made more robust to noise uncertainty by introducing missing-feature based noise compensation into the corpus sentences. Experiments have been conducted on the {TIMIT} database for speech enhancement from various types of nonstationary noise including song, music, and crosstalk speech. The new approach has shown improved performance over conventional enhancement algorithms in both objective and subjective evaluations.},
    author = {Ming, Ji and Srinivasan, R. and Crookes, D.},
    citeulike-article-id = {13169627},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tasl.2010.2064312},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5545372},
    doi = {10.1109/tasl.2010.2064312},
    issn = {1558-7916},
    journal = taslp,
    keywords = {enhancement, exemplar, separation, speech},
    month = may,
    number = {4},
    pages = {822--836},
    posted-at = {2014-05-14 12:44:29},
    priority = {0},
    publisher = {IEEE},
    title = {A {Corpus-Based} Approach to Speech Enhancement From Nonstationary Noise},
    url = {http://dx.doi.org/10.1109/tasl.2010.2064312},
    volume = {19},
    year = {2011}
}

@article{XiaoAndNickel2010,
    abstract = {We present a new method for the enhancement of speech. The method is designed for scenarios in which targeted speaker enrollment as well as system training within the typical noise environment are feasible. The proposed procedure is fundamentally different from most conventional and state-of-the-art denoising approaches. Instead of filtering a distorted signal we are resynthesizing a new  ” clean” signal based on its likely characteristics. These characteristics are estimated from the distorted signal. A successful implementation of the proposed method is presented. Experiments were performed in a scenario with roughly one hour of clean speech training data. Our results show that the proposed method compares very favorably to other state-of-the-art systems in both objective and subjective speech quality assessments. Potential applications for the proposed method include jet cockpit communication systems and offline methods for the restoration of audio recordings.},
    author = {Xiao, X. and Nickel, R. M.},
    citeulike-article-id = {13169375},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tasl.2009.2031793},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5233872},
    doi = {10.1109/tasl.2009.2031793},
    issn = {1558-7916},
    journal = taslp,
    keywords = {concatenative, exemplar, nonparametric, separation, speech, synthesis},
    month = aug,
    number = {6},
    pages = {1243--1257},
    posted-at = {2014-05-14 10:26:13},
    priority = {0},
    publisher = {IEEE},
    title = {Speech Enhancement With Inventory Style Speech Resynthesis},
    url = {http://dx.doi.org/10.1109/tasl.2009.2031793},
    volume = {18},
    year = {2010}
}

@inproceedings{delcroix2011speech,
  title={Speech recognition in the presence of highly non-stationary noise based on spatial, spectral and temporal speech/noise modeling combined with dynamic variance adaptation},
  author={Delcroix, Marc and Kinoshita, Keisuke and Nakatani, Tomohiro and Araki, Shoko and Ogawa, Atsunori and Hori, Takaaki and Watanabe, Shinji and Fujimoto, Masakiyo and Yoshioka, Takuya and Oba, Takanobu and others},
  booktitle={Proc. 1st Intl. Workshop on Machine Listening in Multisource Environments (CHiME)},
  pages={12--17},
  year={2011}
}

@inproceedings{salakhutdinov2007learning,
  title={Learning a nonlinear embedding by preserving class neighbourhood structure},
  author={Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  booktitle={Proc.~AISTATS},
  pages={412--419},
  year={2007}
}

@techreport{ Burges2010,
  title = {From {RankNet} to {LambdaRank} to {LambdaMART}: An Overview},
  author = {Christopher J.C. Burges},
  institution = {Microsoft Research},
  number = {MSR-TR-2010-82},
  year = {2010},
}

@inproceedings{mcfee2010metric,
  title={Metric learning to rank},
  author={McFee, Brian and Lanckriet, Gert R},
  booktitle=icml,
  pages={775--782},
  year={2010}
}

@article{ JainEtAl2012,
  author = {P. Jain and B. Kulis and J.V. Davis and I.S. Dhillon},
  title = {Metric and kernel learning using a linear transformation},
  journal = jmlr,
volume = {13},
pages = {519--547},
year = {2012},
}

@inproceedings{ TorresaniAndLee2007,
  author = {L. Torresani and K. Lee},
  title = {Large margin component analysis},
  booktitle = nips,
  year = 2007,
  pages = {1385--1392},
}

@inproceedings{ WeinbergerAndSaul2008,
  author = {K.Q. Weinberger and L.K. Saul},
  title = {Fast solvers and efficient implementations for distance metric learning},
  booktitle = icml,
  pages = {1160--1167},
  year = 2008,
}

@inproceedings{ LimAndLanckriet2014,
  author = {Lim, D. and Lanckriet, G.R.G.},
  year = {2014},
  title = {Efficient Learning of Mahalanobis Metrics for Ranking},
  booktitle = icml,
}

@ARTICLE{ZekveldEtAl2011,
  title       = "Cognitive load during speech perception in noise: the
                 influence of age, hearing loss, and cognition on the pupil
                 response",
  author      = "Zekveld, Adriana A and Kramer, Sophia E and Festen, Joost M",
  affiliation = "ENT/Audiology and the EMGO+ Institute for Health and Care
                 Research, VU University Medical Center, Amsterdam, The
                 Netherlands. aa.zekveld@vumc.nl",
  abstract    = "OBJECTIVES: The aim of the present study was to evaluate the
                 influence of age, hearing loss, and cognitive ability on the
                 cognitive processing load during listening to speech presented
                 in noise. Cognitive load was assessed by means of pupillometry
                 (i.e., examination of pupil dilation), supplemented with
                 subjective ratings. DESIGN: Two groups of subjects
                 participated: 38 middle-aged participants (mean age = 55 yrs)
                 with normal hearing and 36 middle-aged participants (mean age
                 = 61 yrs) with hearing loss. Using three Speech Reception
                 Threshold (SRT) in stationary noise tests, we estimated the
                 speech-to-noise ratios (SNRs) required for the correct
                 repetition of 50\%, 71\%, or 84\% of the sentences (SRT50\%,
                 SRT71\%, and SRT84\%, respectively). We examined the pupil
                 response during listening: the peak amplitude, the peak
                 latency, the mean dilation, and the pupil response duration.
                 For each condition, participants rated the experienced
                 listening effort and estimated their performance level.
                 Participants also performed the Text Reception Threshold (TRT)
                 test, a test of processing speed, and a word vocabulary test.
                 Data were compared with previously published data from young
                 participants with normal hearing. RESULTS: Hearing loss was
                 related to relatively poor SRTs, and higher speech
                 intelligibility was associated with lower effort and higher
                 performance ratings. For listeners with normal hearing,
                 increasing age was associated with poorer TRTs and slower
                 processing speed but with larger word vocabulary. A
                 multivariate repeated-measures analysis of variance indicated
                 main effects of group and SNR and an interaction effect
                 between these factors on the pupil response. The peak latency
                 was relatively short and the mean dilation was relatively
                 small at low intelligibility levels for the middle-aged
                 groups, whereas the reverse was observed for high
                 intelligibility levels. The decrease in the pupil response as
                 a function of increasing SNR was relatively small for the
                 listeners with hearing loss. Spearman correlation coefficients
                 indicated that the cognitive load was larger in listeners with
                 better TRT performances as reflected by a longer peak latency
                 (normal-hearing participants, SRT50\% condition) and a larger
                 peak amplitude and longer response duration (hearing-impaired
                 participants, SRT50\% and SRT84\% conditions). Also, a larger
                 word vocabulary was related to longer response duration in the
                 SRT84\% condition for the participants with normal hearing.
                 CONCLUSIONS: The pupil response systematically increased with
                 decreasing speech intelligibility. Ageing and hearing loss
                 were related to less release from effort when increasing the
                 intelligibility of speech in noise. In difficult listening
                 conditions, these factors may induce cognitive overload
                 relatively early or they may be associated with relatively
                 shallow speech processing. More research is needed to
                 elucidate the underlying mechanisms explaining these results.
                 Better TRTs and larger word vocabulary were related to higher
                 mental processing load across speech intelligibility levels.
                 This indicates that utilizing linguistic ability to improve
                 speech perception is associated with increased listening load.",
  journal     = "Ear Hear.",
  publisher   = "journals.lww.com",
  volume      =  32,
  number      =  4,
  pages       = "498--510",
  month       =  jul,
  year        =  2011
}

@article{SainathEtAl2012,
    abstract = {Solving real-world classification and recognition problems requires a principled way of modeling the physical phenomena generating the observed data and the uncertainty in it. The uncertainty originates from the fact that many data generation aspects are influenced by nondirectly measurable variables or are too complex to model and hence are treated as random fluctuations. For example, in speech production, uncertainty could arise from vocal tract variations among different people or corruption by noise. The goal of modeling is to establish a generalization from the set of observed data such that accurate inference (classification, decision, recognition) can be made about the data yet to be observed, which we refer to as unseen data.},
    author = {Sainath, T. N. and Ramabhadran, B. and Nahamoo, D. and Kanevsky, D. and Van Compernolle, D. and Demuynck, K. and Gemmeke, J. F. and Bellegarda, J. R. and Sundaram, S.},
    citeulike-article-id = {12252467},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/msp.2012.2208663},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6296529},
    doi = {10.1109/msp.2012.2208663},
    issn = {1053-5888},
    journal = spm,
    keywords = {exemplar, speechrecognition},
    month = nov,
    number = {6},
    pages = {98--113},
    posted-at = {2013-04-09 20:43:44},
    priority = {4},
    publisher = {IEEE},
    title = {{Exemplar-Based} Processing for Speech Recognition: An Overview},
    url = {http://dx.doi.org/10.1109/msp.2012.2208663},
    volume = {29},
    year = {2012}
}

@article{PlumbleyEtAl2010,
    abstract = {Sparse representations have proved a powerful tool in the analysis and processing of audio signals and already lie at the heart of popular coding standards such as {MP3} and Dolby {AAC}. In this paper we give an overview of a number of current and emerging applications of sparse representations in areas from audio coding, audio enhancement and music transcription to blind source separation solutions that can solve the ??cocktail party problem.?? In each case we will show how the prior assumption that the audio signals are approximately sparse in some time-frequency representation allows us to address the associated signal processing task.},
    author = {Plumbley, M. D. and Blumensath, T. and Daudet, L. and Gribonval, R. and Davies, M. E.},
    citeulike-article-id = {12096875},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/jproc.2009.2030345},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5332363},
    doi = {10.1109/jproc.2009.2030345},
    issn = {0018-9219},
    journal = pieee,
    keywords = {review, sparse},
    month = jun,
    number = {6},
    pages = {995--1005},
    posted-at = {2013-03-01 16:56:46},
    priority = {4},
    publisher = {IEEE},
    title = {Sparse Representations in Audio and Music: From Coding to Source Separation},
    url = {http://dx.doi.org/10.1109/jproc.2009.2030345},
    volume = {98},
    year = {2010}
}

@article{WilliamsonEtAl2014,
abstract = {This study proposes an approach to improve the perceptual quality of speech separated by binary masking through the use of reconstruction in the time-frequency domain. Non-negative matrix factorization and sparse reconstruction approaches are investigated, both using a linear combination of basis vectors to represent a signal. In this approach, the short-time Fourier transform (STFT) of separated speech is represented as a linear combination of STFTs from a clean speech dictionary. Binary masking for separation is performed using deep neural networks or Bayesian classifiers. The perceptual evaluation of speech quality, which is a standard objective speech quality measure, is used to evaluate the performance of the proposed approach. The results show that the proposed techniques improve the perceptual quality of binary masked speech, and outperform traditional time-frequency reconstruction approaches.},
author = {Williamson, Donald S and Wang, Yuxuan and Wang, DeLiang},
doi = {10.1121/1.4884759},
issn = {1520-8524},
journal = jasa,
keywords = {Acoustic Stimulation,Algorithms,Audiometry, Speech,Bayes Theorem,Fourier Analysis,Humans,Linear Models,Neural Networks (Computer),Noise,Noise: adverse effects,Perceptual Masking,Signal Processing, Computer-Assisted,Speech Acoustics,Speech Intelligibility,Speech Perception,Speech Production Measurement,Time Factors,Voice Quality,clean{\_}only{\_}model,enhancement,nmf,quality,speech},
mendeley-tags = {clean{\_}only{\_}model,enhancement,nmf,quality,speech},
month = {aug},
number = {2},
pages = {892--902},
pmid = {25096123},
publisher = {Acoustical Society of America},
title = {Reconstruction techniques for improving the perceptual quality of binary masked speech.},
url = {http://scitation.aip.org/content/asa/journal/jasa/136/2/10.1121/1.4884759},
volume = {136},
year = {2014}
}


@article{EmiyaEtAl2011,
    author = {Emiya, V. and Vincent, E. and Harlander, N. and Hohmann, V.},
    citeulike-article-id = {13248426},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tasl.2011.2109381},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5704564},
    doi = {10.1109/tasl.2011.2109381},
    issn = {1558-7916},
    journal = taslp,
    keywords = {objectivemeasure, quality, speech, subjective},
    number = {7},
    pages = {2046--2057},
    posted-at = {2014-07-02 19:48:06},
    priority = {0},
    publisher = {IEEE},
    title = {Subjective and Objective Quality Assessment of Audio Source Separation},
    url = {http://dx.doi.org/10.1109/tasl.2011.2109381},
    volume = {19},
    year = {2011}
}

@article{PlumbleyEtAl2010,
    abstract = {Sparse representations have proved a powerful tool in the analysis and processing of audio signals and already lie at the heart of popular coding standards such as {MP3} and Dolby {AAC}. In this paper we give an overview of a number of current and emerging applications of sparse representations in areas from audio coding, audio enhancement and music transcription to blind source separation solutions that can solve the ??cocktail party problem.?? In each case we will show how the prior assumption that the audio signals are approximately sparse in some time-frequency representation allows us to address the associated signal processing task.},
    author = {Plumbley, M. D. and Blumensath, T. and Daudet, L. and Gribonval, R. and Davies, M. E.},
    citeulike-article-id = {12096875},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/jproc.2009.2030345},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5332363},
    doi = {10.1109/jproc.2009.2030345},
    issn = {0018-9219},
    journal = {Proc.~IEEE},
    keywords = {review, sparse},
    month = jun,
    number = {6},
    pages = {995--1005},
    posted-at = {2013-03-01 16:56:46},
    priority = {4},
    publisher = {IEEE},
    title = {Sparse Representations in Audio and Music: From Coding to Source Separation},
    url = {http://dx.doi.org/10.1109/jproc.2009.2030345},
    volume = {98},
    year = {2010}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle=icml,
  pages={807--814},
  year={2010}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal=jmlr,
  volume={12},
  pages={2121--2159},
  year={2011},
}

@inproceedings{Imai83,
    author = {Imai, S.},
    booktitle = icassp,
    citeulike-article-id = {13248469},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icassp.1983.1172250},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1172250},
    doi = {10.1109/icassp.1983.1172250},
    keywords = {coding, mel, speech},
    month = apr,
    pages = {93--96},
    posted-at = {2014-07-02 20:20:24},
    priority = {2},
    publisher = {IEEE},
    title = {Cepstral analysis synthesis on the mel frequency scale},
    url = {http://dx.doi.org/10.1109/icassp.1983.1172250},
    volume = {8},
    year = {1983}
}
@article{Srinivasan2006Binary,
    abstract = {A time-varying Wiener filter specifies the ratio of a target signal and a noisy mixture in a local time-frequency unit. We estimate this ratio using a binaural processor and derive a ratio time-frequency mask. This mask is used to extract the speech signal, which is then fed to a conventional speech recognizer operating in the cepstral domain. We compare the performance of this system with a missing-data recognizer that operates in the spectral domain using the time-frequency units that are dominated by speech. To apply the missing-data recognizer, the same binaural processor is used to estimate an ideal binary time-frequency mask, which selects a local time-frequency unit if the speech signal within the unit is stronger than the interference. We find that the performance of the missing data recognizer is better on a small vocabulary recognition task but the performance of the conventional recognizer is substantially better when the vocabulary size is increased.},
    author = {Srinivasan, Soundararajan and Roman, Nicoleta and Wang, Deliang},
    booktitle = {Robustness Issues for Conversational Interaction},
    citeulike-article-id = {2216007},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.specom.2006.09.003},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6V1C-4KYY4G4-1/2/5a5887544a15ae0d7df3f51e82a442e2},
    doi = {10.1016/j.specom.2006.09.003},
    issn = {01676393},
    journal = {Speech Comm.},
    keywords = {masking, model, separation, speechrecognition},
    month = nov,
    number = {11},
    pages = {1486--1501},
    posted-at = {2008-01-10 19:58:05},
    priority = {3},
    title = {Binary and ratio time-frequency masks for robust speech recognition},
    url = {http://dx.doi.org/10.1016/j.specom.2006.09.003},
    volume = {48},
    year = {2006}
}

@misc{Ellis05-rastamat,
      Author = {Daniel P. W. Ellis},
      Year = {2005},
      Title = {{PLP} and {RASTA} (and {MFCC}, and inversion) in {M}atlab},
      Url = {http://www.ee.columbia.edu/~dpwe/resources/matlab/rastamat/},
}

@misc{wang07cochleagram,
  author = {DeLiang Wang},
  title = {Matlab Toolbox for Cochleagram Analysis and Synthesis},
  year = {2007},
  url = {http://web.cse.ohio-state.edu/pnl/shareware/cochleagram/},
}

@inproceedings{wang2015joint,
  title={Joint training of speech separation, filterbank and acoustic model for robust automatic speech recognition},
  author={Wang, Z and Wang, D},
  booktitle=interspeech,
  year={2015}
}
@inproceedings{HeAndFoslerLussier2015,
  title = {Segmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition},
  author = {He, Yanzhang and Fosler-Lussier, Eric},
  booktitle = interspeech,
  year = {2015},
}

@inproceedings{TuEtAl2014,
author = {Tu, Yanhui and Du, Jun and Xu, Yong and Dai, Lirong and Lee, Chin-Hui},
booktitle = {2014 12th International Conference on Signal Processing (ICSP)},
doi = {10.1109/ICOSP.2014.7015061},
isbn = {978-1-4799-2186-7},
issn = {2164-5221},
keywords = {DNN architecture,Hidden Markov models,Neural networks,Robustness,Speech,Speech processing,Speech recognition,Training,baseline system,cochannel,deep neural network based speech separation,deep neural networks,dnn,enhancement,feature extraction,interfering speaker,monaural speech separation,neural nets,recognition challenge task,robust speech recognition,semi-supervised mode,semisupervised mode,separation,single-channel speech separation,speech features,speech recognition,target speaker,training data,training stage},
language = {English},
mendeley-tags = {cochannel,dnn,enhancement,separation},
month = {oct},
pages = {532--536},
publisher = {IEEE},
title = {Deep neural network based speech separation for robust speech recognition},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7015061},
year = {2014}
}

@article{WengEtAl2015,
abstract = {We investigate techniques based on deep neural networks (DNNs) for attacking the single-channel multi-talker speech recognition problem. Our proposed approach contains five key ingredients: a multi-style training strategy on artificially mixed speech data, a separate DNN to estimate senone posterior probabilities of the louder and softer speakers at each frame, a weighted finite-state transducer (WFST)-based two-talker decoder to jointly estimate and correlate the speaker and speech, a speaker switching penalty estimated from the energy pattern change in the mixed-speech, and a confidence based system combination strategy. Experiments on the 2006 speech separation and recognition challenge task demonstrate that our proposed DNN-based system has remarkable noise robustness to the interference of a competing speaker. The best setup of our proposed systems achieves an average word error rate (WER) of 18.8{\%} across different SNRs and outperforms the state-of-the-art IBM superhuman system by 2.8{\%} absolute with fewer assumptions.},
author = {Weng, Chao and Yu, Dong and Seltzer, Michael L. and Droppo, Jasha},
doi = {10.1109/TASLP.2015.2444659},
issn = {2329-9290},
journal = taslp,
keywords = {Acoustics,DNN,Decoding,Deep neural network (DNN),Hidden Markov models,Joints,Speech,Speech recognition,Training,WER,WFST-based two-talker decoder,artificially mixed speech data,asr,cochannel,confidence based system combination strategy,decoding,deep neural networks,dnn,energy pattern change,finite state machines,joint decoding,monaural,multi-style training strategy,multi-talker automatic speech recognition (ASR),neural nets,noise robustness,probability,selection{\_}problem,senone posterior probabilities,single-channel,single-channel multi-talker speech recognition pro,speaker recognition,speaker switching penalty,speech separation,weighted finite-state transducer (WFST),weighted finite-state transducer-based two-talker,word error rate},
mendeley-tags = {asr,cochannel,dnn,monaural,selection{\_}problem},
month = {oct},
number = {10},
pages = {1670--1679},
title = {Deep Neural Networks for Single-Channel Multi-Talker Speech Recognition},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7122291},
volume = {23},
year = {2015}
}

@article{HersheyEtAl2010,
abstract = {We present a system that can separate and recognize the simultaneous speech of two people recorded in a single channel. Applied to the monaural speech separation and recognition challenge, the system out-performed all other participants -including human listeners - with an overall recognition error rate of 21.6{\%}, compared to the human error rate of 22.3{\%}. The system consists of a speaker recognizer, a model-based speech separation module, and a speech recognizer. For the separation models we explored a range of speech models that incorporate different levels of constraints on temporal dynamics to help infer the source speech signals. The system achieves its best performance when the model of temporal dynamics closely captures the grammatical constraints of the task. For inference, we compare a 2-D Viterbi algorithm and two loopy belief-propagation algorithms. We show how belief-propagation reduces the complexity of temporal inference from exponential to linear in the number of sources and the size of the language model. The best belief-propagation method results in nearly the same recognition error rate as exact inference.},
address = {London, UK},
author = {Hershey, John R. and Rennie, Steven J. and Olsen, Peder A. and Kristjansson, Trausti T.},
doi = {doi: 10.1016/j.csl.2008.11.001},
issn = {0885-2308},
journal = {Comput. Speech Lang.},
keywords = {ASR,Algonquin,Factorial hidden Markov model,Multiple talker speaker identification,Speaker-dependent labeling,Speech separation,beliefPropagation,factorialHmm,graphicalmodels,monaural,separation,sourceSeparation,sourcemodel,speechrecognition},
mendeley-tags = {ASR,beliefPropagation,factorialHmm,sourceSeparation},
month = {jan},
number = {1},
pages = {45--66},
publisher = {Academic Press Ltd.},
title = {Super-human multi-talker speech recognition: A graphical modeling approach},
url = {http://www.sciencedirect.com/science/article/pii/S0885230808000557 citeulike-article-id:4615564 http://dx.doi.org/10.1016/j.csl.2008.11.001},
volume = {24},
year = {2010}
}

@inproceedings{BlackAndTokuda2005,
abstract = {In order to better understand different speech synthesis techniques on a common dataset, we devised a challenge that will help us better compare research techniques in building corpusbased speech synthesizers. In 2004, we released the first two 1200-utterance single-speaker databases from the CMU ARC-TIC speech databases, and challenged current groups working in speech synthesis around the world to build their best voices from these databases. In January of 2005, we released two further databases and a set of 50 utterance texts from each of five genres and asked the participants to synthesize these utterances. Their resulting synthesized utterances were then presented to three groups of listeners: speech experts, volunteers, and US English-speaking undergraduates. This paper summarizes the purpose, design, and whole process of the challenge.},
author = {Alan W Black and Keiichi Tokuda},
booktitle = interspeech,
keywords = {challenge,speech,speech{\_}synthesis,synthesis},
mendeley-tags = {challenge,speech,speech{\_}synthesis,synthesis},
pages = {77--80},
title = {The Blizzard Challenge 2005: Evaluating corpus-based speech synthesis on common datasets},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.67.5298},
year = {2005}
}

@article{King2014,
abstract = {The Blizzard Challenge offers a unique insight into progress in text-to-speech synthesis over the last decade. By using a very large listening test to compare the performance of a wide range of systems that have been constructed using a common corpus of speech recordings, it is possible to make some direct comparisons between competing techniques. By reviewing over a hundred papers describing all entries to the Challenge since 2005, we can make a useful summary of the most successful techniques adopted by participating teams, as well as drawing some conclusions about where the Blizzard Challenge has succeeded, and where there are still open problems in cross-system comparisons of text-to-speech synthesisers.},
author = {King, Simon},
doi = {10.3989/loquens.2014.006},
issn = {2386-2637},
journal = {Loquens},
keywords = {The Blizzard Challenge,challenge,evaluation,review,speech,synthesis,text-to-speech synthesis,tts},
language = {en},
mendeley-tags = {challenge,review,speech,synthesis,tts},
month = {jun},
number = {1},
pages = {e006},
title = {Measuring a decade of progress in Text-to-Speech},
url = {http://loquens.revistas.csic.es/index.php/loquens/article/view/6/14},
volume = {1},
year = {2014}
}

@inproceedings{Virtanen2003,
  title={Sound source separation using sparse coding with temporal continuity objective},
  author={Virtanen, Tuomas},
  booktitle = {International computer music conference},
  year = 2003,
  pages = {231--234},
}

@inproceedings{SmaragdisEtAl2007,
abstract = {In this paper we describe a methodology for model-based single channel separation of sounds. We present a sparse latent variable model that can learn sounds based on their distribution of time/ frequency energy. This model can then be used to extract known types of sounds from mixtures in two scenarios. One being the case where all sound types in the mixture are known, and the other being being the case where only the target or the interference models are known. The model we propose has close ties to non-negative decompositions and latent variable models commonly used for semantic analysis.},
address = {Berlin, Heidelberg},
author = {Smaragdis, Paris and Raj, Bhiksha and Shashanka, Madhusudana},
booktitle = {Independent Component Analysis and Signal Separation},
doi = {10.1007/978-3-540-74494-8},
editor = {Davies, Mike E. and James, Christopher J. and Abdallah, Samer A. and Plumbley, Mark D},
isbn = {978-3-540-74493-1},
keywords = {monaural,nmf,semisupervised,separation,supervised},
mendeley-tags = {monaural,nmf,semisupervised,separation,supervised},
pages = {414--421},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {Supervised and Semi-supervised Separation of Sounds from Single-Channel Mixtures},
url = {http://www.springerlink.com/index/10.1007/978-3-540-74494-8},
volume = {4666},
year = {2007}
}
@inproceedings{RajAndSmaragdis2005,
author = {Raj, B. and Smaragdis, P.},
booktitle = waspaa,
doi = {10.1109/ASPAA.2005.1540157},
isbn = {0-7803-9154-3},
keywords = {Acoustic applications,Acoustic signal processing,Conferences,Fourier transforms,Frequency,Histograms,Laboratories,Loudspeakers,Probability distribution,Spectrogram,Speech,discrete random process,frequency bin indices,frequency distribution,latent variable decomposition,magnitude spectral vectors,mixed single-channel recordings,multinomial distributions,nmf,plca,separation,short-time Fourier transform,single channel speaker separation,source separation,speaker recognition,speech,speech processing,speech spectrogram},
language = {English},
mendeley-tags = {nmf,plca,separation,speech},
pages = {17--20},
title = {Latent variable decomposition of spectrograms for single channel speaker separation},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1540157},
year = 2005,
}

@article{Mysore2015,
abstract = {The goal of speech enhancement is typically to recover clean speech from noisy, reverberant, and often bandlimited speech in order to yield improved intelligibility, clarity, or automatic speech recognition performance. However, the acoustic goal for a great deal of speech content such as voice overs, podcasts, demo videos, lecture videos, and audio stories is often not merely clean speech, but speech that is aesthetically pleasing. This is achieved in professional recording studios by having a skilled sound engineer record clean speech in an acoustically treated room and then edit and process it with audio effects (which we refer to as production). A growing amount of speech content is being recorded on common consumer devices such as tablets, smartphones, and laptops. Moreover, it is typically recorded in common but non-acoustically treated environments such as homes and offices. We argue that the goal of enhancing such recordings should not only be to make it sound cleaner as would be done using traditional speech enhancement techniques, but to make it sound like it was recorded and produced in a professional recording studio. In this paper, we show why this can be beneficial, describe a new data set (a great deal of which was recorded in a professional recording studio) that we prepared to help in developing algorithms for this purpose, and discuss some insights and challenges associated with this problem.},
author = {Mysore, Gautham J.},
doi = {10.1109/LSP.2014.2379648},
issn = {1070-9908},
journal = spl,
keywords = {Acoustics,Automatic production,Performance evaluation,Production,Signal processing algorithms,Speech,Speech enhancement,audio effects,audio recording,clean speech,consumer{\_}devices,professional recording studios,professional{\_}recordings,speech content,speech enhancement,speech recognition,speech{\_}mapping,voiceover},
mendeley-tags = {consumer{\_}devices,professional{\_}recordings,speech{\_}mapping,voiceover},
month = {aug},
number = {8},
pages = {1006--1010},
title = {Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6981922},
volume = {22},
year = {2015}
}

@inproceedings{SakEtAl2014,
abstract = {Long Short-Term Memory (LSTM) is a specific recurrent neu- ral network (RNN) architecture that was designed to model tem- poral sequences and their long-range dependencies more accu- rately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic mod- eling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a lin- ear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effec- tive use of model parameters than the others considered, con- verges quickly, and outperforms a deep feed forward neural net- work having an order of magnitude more parameters.},
author = {Sak, Hasim and Senior, Andrew and Beaufays, Francoise},
booktitle = interspeech,
keywords = {acoustic{\_}model,asr,lstm},
mendeley-tags = {acoustic{\_}model,asr,lstm},
pages = {338--342},
title = {Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling},
url = {http://193.6.4.39/{~}czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS141304.PDF},
year = {2014}
}

@article{QianEtAl2013,
author = {Qian, Yao and Soong, F.K. and Yan, Zhi-Jie},
doi = {10.1109/TASL.2012.2221460},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Cross-lingual,Hidden Markov models,Rendering (computer graphics),Speech,Speech processing,Training data,Trajectory,cross-lingual voice transformation,frictionless interactions,high quality speech rendering,hybrid{\_}synthesis,machine talk,natural speech,pre-recorded speech database,processed speech,speech parameter trajectory,speech processing,speech synthesis,synthesis,synthesized speech,text-to-speech synthesis,trajectory tiling,trajectory{\_}tiling,unified trajectory tiling,voice transformation,waveform tiles},
language = {English},
mendeley-tags = {hybrid{\_}synthesis,synthesis,trajectory{\_}tiling},
month = {feb},
number = {2},
pages = {280--290},
publisher = {IEEE},
title = {A Unified Trajectory Tiling Approach to High Quality Speech Rendering},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6317143},
volume = {21},
year = {2013}
}

@inproceedings{kominek2006blizzard,
  title={The Blizzard Challenge 2006 CMU entry introducing hybrid trajectory-selection synthesis},
  author={Kominek, John and Black, A},
  booktitle={Blizzard Challenge Workshop},
  year={2006}
}

@inproceedings{BeroutiEtAl1979,
author = {Berouti, M. and Schwartz, R. and Makhoul, J.},
booktitle = icassp,
doi = {10.1109/ICASSP.1979.1170788},
keywords = {Acoustic noise,Background noise,Noise level,Noise reduction,Performance evaluation,Phase estimation,Phase noise,Signal to noise ratio,Speech enhancement,Testing,enhancement,noise,quality,spectral{\_}subtraction},
language = {English},
mendeley-tags = {enhancement,noise,quality,spectral{\_}subtraction},
month = {apr},
pages = {208--211},
publisher = {Institute of Electrical and Electronics Engineers},
title = {{Enhancement of speech corrupted by acoustic noise}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1170788},
volume = {4},
year = {1979}
}

@inproceedings{OnoEtAl2013,
author = {Ono, Nobutaka and Koldovsky, Zbynek and Miyabe, Shigeki and Ito, Nobutaka},
booktitle = {2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
doi = {10.1109/MLSP.2013.6661988},
isbn = {978-1-4799-1180-6},
issn = {1551-2541},
keywords = {2013 community-based signal separation evaluation,Estimation,Microphones,Microwave integrated circuits,Noise,Noise measurement,SiSEC 2013,Source separation,Speech,asynchronous recordings,bakeoff,campaign specifications,competition,enhancement,evaluation,moving speaker,music,objective performance criteria,organization strategies,separation,source separation,speech,speech mixtures,speech processing,two-channel noisy recordings},
language = {English},
mendeley-tags = {bakeoff,competition,enhancement,evaluation,music,separation,speech},
month = {sep},
pages = {1--6},
publisher = {IEEE},
title = {The 2013 Signal Separation Evaluation Campaign},
url = {http://ieeexplore.ieee.org.proxy.lib.ohio-state.edu/articleDetails.jsp?arnumber=6661988},
year = {2013}
}

@book{VirtanenEtAl2012,
booktitle = {Techniques for Noise Robustness in Automatic Speech Recognition},
doi = {10.1002/9781118392683},
editor = {Virtanen, Tuomas and Raj, Bhiksha and Singh, Rita},
isbn = {9781118392683},
publisher = {John Wiley {\&} Sons, Ltd},
title = {Techniques for Noise Robustness in Automatic Speech Recognition},
year = {2012}
}

@article{StickneyEtAl2004,
abstract = {Speech recognition performance was measured in normal-hearing and cochlear-implant listeners with maskers consisting of either steady-state speech-spectrum-shaped noise or a competing sentence. Target sentences from a male talker were presented in the presence of one of three competing talkers (same male, different male, or female) or speech-spectrum-shaped noise generated from this talker at several target-to-masker ratios. For the normal-hearing listeners, target-masker combinations were processed through a noise-excited vocoder designed to simulate a cochlear implant. With unprocessed stimuli, a normal-hearing control group maintained high levels of intelligibility down to target-to-masker ratios as low as 0 dB and showed a release from masking, producing better performance with single-talker maskers than with steady-state noise. In contrast, no masking release was observed in either implant or normal-hearing subjects listening through an implant simulation. The performance of the simulation and implant groups did not improve when the single-talker masker was a different talker compared to the same talker as the target speech, as was found in the normal-hearing control. These results are interpreted as evidence for a significant role of informational masking and modulation interference in cochlear implant speech recognition with fluctuating maskers. This informational masking may originate from increased target-masker similarity when spectral resolution is reduced.},
author = {Stickney, Ginger S. and Zeng, Fan-Gang and Litovsky, Ruth and Assmann, Peter},
doi = {10.1121/1.1772399},
issn = {00014966},
journal = jasa,
keywords = {cochannel,cochlear{\_}implant,noise,speech},
mendeley-tags = {cochannel,cochlear{\_}implant,noise,speech},
month = {aug},
number = {2},
pages = {1081},
publisher = {Acoustical Society of America},
title = {Cochlear implant speech recognition with speech maskers},
url = {http://scitation.aip.org.proxy.lib.ohio-state.edu/content/asa/journal/jasa/116/2/10.1121/1.1772399},
volume = {116},
year = {2004}
}
@article{FriesenEtAl2001,
abstract = {Speech recognition was measured as a function of spectral resolution (number of spectral channels) and speech-to-noise ratio in normal-hearing (NH) and cochlear-implant(CI) listeners. Vowel, consonant, word, and sentence recognition were measured in five normal-hearing listeners, ten listeners with the Nucleus-22 cochlear implant, and nine listeners with the Advanced Bionics Clarion cochlear implant. Recognition was measured as a function of the number of spectral channels (noise bands or electrodes) at signal-to-noise ratios of +15, +10, +5, 0 dB, and in quiet. Performance with three different speech processing strategies (SPEAK, CIS, and SAS) was similar across all conditions, and improved as the number of electrodes increased (up to seven or eight) for all conditions. For all noise levels, vowel and consonant recognition with the SPEAK speech processor did not improve with more than seven electrodes, while for normal-hearing listeners, performance continued to increase up to at least 20 channels. Speech recognition on more difficult speech materials (word and sentence recognition) showed a marginally significant increase in Nucleus-22 listeners from seven to ten electrodes. The average implant score on all processing strategies was poorer than scores of NH listeners with similar processing. However, the best CI scores were similar to the normal-hearing scores for that condition (up to seven channels). CI listeners with the highest performance level increased in performance as the number of electrodes increased up to seven, while CI listeners with low levels of speech recognition did not increase in performance as the number of electrodes was increased beyond four. These results quantify the effect of number of spectral channels on speech recognition in noise and demonstrate that most CI subjects are not able to fully utilize the spectral information provided by the number of electrodes used in their implant.},
author = {Friesen, Lendra M. and Shannon, Robert V. and Baskent, Deniz and Wang, Xiaosong},
doi = {10.1121/1.1381538},
issn = {00014966},
journal = jasa,
keywords = {cochlear{\_}implant,noise,speech{\_}recognition},
mendeley-tags = {cochlear{\_}implant,noise,speech{\_}recognition},
month = {aug},
number = {2},
pages = {1150},
publisher = {Acoustical Society of America},
title = {Speech recognition in noise as a function of the number of spectral channels: Comparison of acoustic hearing and cochlear implants},
url = {http://scitation.aip.org.proxy.lib.ohio-state.edu/content/asa/journal/jasa/110/2/10.1121/1.1381538},
volume = {110},
year = {2001}
}
@article{TurnerEtAl2004,
abstract = {The purpose of this study was to explore the potential advantages, both theoretical and applied, of preserving low-frequency acoustic hearing in cochlear implant patients. Several hypotheses are presented that predict that residual low-frequency acoustic hearing along with electric stimulation for high frequencies will provide an advantage over traditional long-electrode cochlear implants for the recognition of speech in competing backgrounds. A simulation experiment in normal-hearing subjects demonstrated a clear advantage for preserving low-frequency residual acoustic hearing for speech recognition in a background of other talkers, but not in steady noise. Three subjects with an implanted “short-electrode” cochlear implant and preserved low-frequency acoustic hearing were also tested on speech recognition in the same competing backgrounds and compared to a larger group of traditional cochlear implant users. Each of the three short-electrode subjects performed better than any of the traditional long-electrode implant subjects for speech recognition in a background of other talkers, but not in steady noise, in general agreement with the simulation studies. When compared to a subgroup of traditional implant users matched according to speech recognition ability in quiet, the short-electrode patients showed a 9-dB advantage in the multitalker background. These experiments provide strong preliminary support for retaining residual low-frequency acoustic hearing in cochlear implant patients. The results are consistent with the idea that better perception of voice pitch, which can aid in separating voices in a background of other talkers, was responsible for this advantage.},
author = {Turner, Christopher W. and Gantz, Bruce J. and Vidal, Corina and Behrens, Amy and Henry, Belinda A.},
doi = {10.1121/1.1687425},
issn = {00014966},
journal = jasa,
keywords = {acoustic{\_}electric,cochlear{\_}implant,intelligibility,noise,short{\_}electrode,speech},
mendeley-tags = {acoustic{\_}electric,cochlear{\_}implant,intelligibility,noise,short{\_}electrode,speech},
month = {mar},
number = {4},
pages = {1729},
publisher = {Acoustical Society of America},
title = {Speech recognition in noise for cochlear implant listeners: Benefits of residual acoustic hearing},
url = {http://scitation.aip.org.proxy.lib.ohio-state.edu/content/asa/journal/jasa/115/4/10.1121/1.1687425},
volume = {115},
year = {2004}
}

@incollection{Sproat2008,
author = {Sproat, Richard},
booktitle = {Springer Handbook of Speech Processing SE  - 22},
doi = {10.1007/978-3-540-49127-9{\_}22},
editor = {Benesty, Jacob and Sondhi, M.Mohan and Huang, Yiteng(Arden)},
isbn = {978-3-540-49125-5},
language = {English},
pages = {457--470},
publisher = {Springer Berlin Heidelberg},
title = {Linguistic Processing for Speech Synthesis},
url = {http://dx.doi.org/10.1007/978-3-540-49127-9{\_}22},
year = {2008}
}

@book{Loizou2013,
  title={Speech enhancement: theory and practice},
  author={Loizou, Philipos C},
  year={2013},
  publisher={CRC press},
  edition = {Second},
}

@article{ChiEtAl2005,
abstract = {A computational model of auditory analysis is described that is inspired by psychoacoustical and neurophysiological findings in early and central stages of the auditory system. The model provides a unified multiresolution representation of the spectral and temporal features likely critical in the perception of sound. Simplified, more specifically tailored versions of this model have already been validated by successful application in the assessment of speech intelligibility [Elhilali et al., Speech Commun. 41(2-3), 331{\&}{\#}150;348 (2003); Chi et al., J. Acoust. Soc. Am. 106, 2719{\&}{\#}150;2732 (1999)] and in explaining the perception of monaural phase sensitivity [R. Carlyon and S. Shamma, J. Acoust. Soc. Am. 114, 333{\&}{\#}150;348 (2003)]. Here we provide a more complete mathematical formulation of the model, illustrating how complex signals are transformed through various stages of the model, and relating it to comparable existing models of auditory processing. Furthermore, we outline several reconstruction algorithms to resynthesize the sound from the model output so as to evaluate the fidelity of the representation and contribution of different features and cues to the sound percept.},
author = {Chi, Taishih and Ru, Powen and Shamma, Shihab},
doi = {doi: 10.1121/1.1945807},
journal = {Journal of the Acoustical Society of America},
keywords = {model,timefrequency},
number = {2},
pages = {887--906},
publisher = {ASA},
title = {Multiresolution spectrotemporal analysis of complex sounds},
url = {citeulike-article-id:2586445 http://dx.doi.org/10.1121/1.1945807},
volume = {118},
year = {2005}
}

@inproceedings{ KingAndKaraiskos2013,
  title = {The Blizzard Challenge 2013},
  author = {Simon King and Vasilis Karaiskos},
  year = 2013,
  booktitle = {Blizzard Challenge Workshop},
}

@inproceedings{PaulAndBaker1992,
address = {Morristown, NJ, USA},
author = {Paul, Douglas B. and Baker, Janet M.},
booktitle = {Proceedings of the workshop on Speech and Natural Language (HLT)},
doi = {10.3115/1075527.1075614},
isbn = {1558602720},
keywords = {asr,corpus,dataset,speech,text},
mendeley-tags = {asr,corpus,dataset,speech,text},
month = {feb},
pages = {357},
publisher = {Association for Computational Linguistics},
title = {The design for the wall street journal-based {CSR} corpus},
url = {http://dl.acm.org/citation.cfm?id=1075527.1075614},
year = {1992}
}

@article{GemmekeEtAl2011,
author = {Gemmeke, Jort F. and Virtanen, Tuomas and Hurmalainen, Antti},
doi = {10.1109/TASL.2011.2112350},
issn = {1558-7916},
journal = taslp,
keywords = {AURORA-2 database,Dictionaries,Exemplar-based,Hidden Markov models,Noise,Noise measurement,SNR,Spectrogram,Speech,Speech recognition,additive noise,asr,connected digit recognition,exemplar,exemplar based feature enhancement,exemplar-based sparse representation,feature extraction,hidden Markov models,hybrid exemplar-based HMM recognition,missing data mask estimation,nmf,noise robust automatic speech recognition,noise robustness,non-negative matrix factorization,phonetic information,recognition,signal denoising,signal-to-noise ratio,source separation,sparse representations,speech enhancement,speech exemplar dictionary,speech recognition,time-frequency analysis,time-frequency patch},
language = {English},
mendeley-tags = {asr,exemplar,nmf,recognition},
month = {sep},
number = {7},
pages = {2067--2080},
publisher = {IEEE},
title = {Exemplar-Based Sparse Representations for Noise Robust Automatic Speech Recognition},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5710402},
volume = {19},
year = {2011}
}

@inproceedings{BenaroyaEtAl2003,
abstract = {We propose a new method to perform the separation of two sound sources from a single sensor. This method generalizes Wiener filtering with locally stationary, non-Gaussian, parametric source models. The method involves a learning phase for which we propose three different algorithm. In the separation phase, we use a sparse non negative decomposition algorithm of our own. The algorithms are evaluated on the separation of real audio data.},
author = {Benaroya, L. and Donagh, L.M. and Bimbot, F. and Gribonval, R.},
booktitle = icassp,
doi = {10.1109/ICASSP.2003.1201756},
isbn = {0-7803-7663-3},
issn = {1520-6149},
keywords = {Acoustic sensors,Fourier transforms,Frequency estimation,Gaussian processes,Parameter estimation,Parametric statistics,Phase estimation,Source separation,Spectral shape,Wiener filter,Wiener filtering,Wiener filters,audio signal processing,learning (artificial intelligence),learning phase,monaural,nmf,nonGaussian source models,nonnegative decomposition algorithm,nonnegative sparse representation,optimal estimates,parameter estimation,parametric source models,real audio data,separation phase,signal representation,sound source separation,source separation,sparse,sparse decomposition algorithm,stationary source models},
mendeley-tags = {monaural,nmf,sparse},
pages = {VI--613--16},
publisher = {IEEE},
shorttitle = {Acoustics, Speech, and Signal Processing, 2003. Pr},
title = {Non negative sparse representation for Wiener based source separation with a single sensor},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1201756},
volume = {1},
year = {2003}
}

@inproceedings{SchmidtAndOlsson2006,
author = {Mikkel N. Schmidt and Rasmus K. Olsson},
keywords = {monaural,nmf,sparse,speech},
mendeley-tags = {monaural,nmf,sparse,speech},
title = {Single-channel speech separation using sparse non-negative matrix factorization},
year = 2006,
booktitle = interspeech,
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.6011}
}

@inproceedings{ ClarkEtAl2006,
  author = {R. Clark and K. Richmond and V. Strom and S. King},
  title = {Multisyn voices for the Blizzard Challenge 2006},
  booktitle = {Proc. Blizzard Challenge Workshop (Interspeech Satellite)},
  year = {2006},
  location = {Pittsburgh, USA},
}

@phdthesis{lee2012noise,
  title={Noise robust pitch tracking by subband autocorrelation classification},
  author={Lee, Byung Suk},
  year={2012},
  school={Columbia University}
}

@article{MingEtAl2007,
author = {Ming, Ji and Hazen, Timothy J. and Glass, James R. and Reynolds, Douglas A.},
doi = {10.1109/TASL.2007.899278},
issn = {1558-7916},
journal = taslp,
keywords = {Databases,Handheld computers,Missing-feature theory,Noise reduction,Noise robustness,Signal processing,Speaker recognition,Speech enhancement,Testing,Training data,Working environment noise,acoustic noise,missing-feature theory,multicondition model training,multicondition training,noise,noise compensation,noise modeling,noisy conditions,robust speaker recognition,speaker identification,speaker recognition,speaker verification,speaker{\_}id,speech signals,temporal-spectral characteristics},
language = {English},
mendeley-tags = {noise,speaker{\_}id},
month = {jul},
number = {5},
pages = {1711--1723},
publisher = {IEEE},
title = {Robust Speaker Recognition in Noisy Conditions},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4244529},
volume = {15},
year = {2007}
}



@article{pollack1957cocktail,
  title={Cocktail party effect},
  author={Pollack, Irwin and Pickett, James M},
  journal={The Journal of the Acoustical Society of America},
  volume={29},
  number={11},
  pages={1262--1262},
  year={1957},
  publisher={ASA}
}

@article{hawley2004benefit,
  title={The benefit of binaural hearing in a cocktail party: Effect of location and type of interferer},
  author={Hawley, Monica L and Litovsky, Ruth Y and Culling, John F},
  journal={The Journal of the Acoustical Society of America},
  volume={115},
  number={2},
  pages={833--843},
  year={2004},
  publisher={ASA}
}

@article{arons1992review,
  title={A review of the cocktail party effect},
  author={Arons, Barry},
  journal={Journal of the American Voice I/O Society},
  volume={12},
  number={7},
  pages={35--50},
  year={1992}
}

@article{bronkhorst2000cocktail,
  title={The cocktail party phenomenon: A review of research on speech intelligibility in multiple-talker conditions},
  author={Bronkhorst, Adelbert W},
  journal={Acta Acustica united with Acustica},
  volume={86},
  number={1},
  pages={117--128},
  year={2000},
  publisher={S. Hirzel Verlag}
}

